{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmz0SZU8BnwAus4AM3HQh3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YH5Y-hogcibh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import json\n","import math\n","import glob\n","import time\n","from datetime import datetime\n","from typing import List, Tuple, Dict, Any\n","\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"],"metadata":{"id":"xQxEmQnwckoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/yolo/Bee.zip"],"metadata":{"id":"8n6x-G9ocngc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Config\n","# =========================\n","class Config:\n","    DATA_ROOT = \"/content/data\"\n","    IMAGES_DIR = os.path.join(DATA_ROOT, \"images\")\n","    JSONS_DIR = os.path.join(DATA_ROOT, \"jsons\")\n","\n","    OUTPUT_DIR = \"/content/drive/MyDrive/yolo/output\"\n","    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n","    RESULTS_DIR = os.path.join(OUTPUT_DIR, \"results\")\n","    CROPS_DIR = os.path.join(OUTPUT_DIR, \"crops\")\n","\n","    INPUT_SIZE = 416\n","    GRID_SIZE = 13\n","    ANCHORS = [\n","        (10, 13), (16, 30), (33, 23), (30, 61), (62, 45),\n","        (59, 119), (116, 90), (156, 198), (373, 326)\n","    ]\n","    NUM_CLASSES = 1\n","    NUM_ANCHORS = 5\n","\n","    CLASSIFICATION_INPUT_SIZE = 224\n","    CLASSIFICATION_NUM_CLASSES = 2\n","    CLASSIFICATION_CLASSES = ['non_bee', 'bee']\n","\n","    BATCH_SIZE_GPU = 8\n","    BATCH_SIZE_CPU = 64\n","    BASE_LR = 4e-5       # warmup에 사용할 기본 LR\n","    WARMUP_EPOCHS = 5    # warmup epoch 수\n","    NUM_EPOCHS = 100\n","    MOMENTUM = 0.9\n","    WEIGHT_DECAY = 1e-4\n","\n","    AUGMENTATION = True\n","    AUGMENTATION_PROB = 0.5\n","    HORIZONTAL_FLIP = True\n","    VERTICAL_FLIP = False\n","    ROTATION_RANGE = 15\n","    BRIGHTNESS_RANGE = 0.8\n","    CONTRAST_RANGE = 0.8\n","\n","    DEVICE = \"cuda\"\n","    SAVE_INTERVAL = 10\n","\n","    CONFIDENCE_THRESHOLD = 0.3\n","    NMS_THRESHOLD = 0.45\n","    MIN_BOX_SIZE = 6\n","\n","    CLASSIFICATION_CONFIDENCE_THRESHOLD = 0.7\n","\n","    def __init__(self):\n","        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n","        os.makedirs(self.CHECKPOINT_DIR, exist_ok=True)\n","        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n","        os.makedirs(self.CROPS_DIR, exist_ok=True)\n"],"metadata":{"id":"EN6qyWFhcptT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Utils\n","# =========================\n","def load_json_annotations(json_path: str) -> Dict[str, Any]:\n","    with open(json_path, 'r', encoding='utf-8') as f:\n","        return json.load(f)\n","\n","\n","def convert_to_yolo_format(annotation_data: Dict[str, Any], img_width: int, img_height: int) -> List[List[float]]:\n","    yolo_annotations = []\n","    # Use .get() for safety when accessing 'ANNOTATION_INFO'\n","    for annotation in annotation_data.get('ANNOTATION_INFO', []):\n","        x_tl = annotation['XTL']\n","        y_tl = annotation['YTL']\n","        x_br = annotation['XBR']\n","        y_br = annotation['YBR']\n","        center_x = (x_tl + x_br) / 2.0 / img_width\n","        center_y = (y_tl + y_br) / 2.0 / img_height\n","        width = (x_br - x_tl) / img_width\n","        height = (y_br - y_tl) / img_height # Fixed ytl instead of ybr for height calculation\n","        class_id = 0\n","        yolo_annotations.append([class_id, center_x, center_y, width, height])\n","    return yolo_annotations\n","\n","\n","def preprocess_image(image_path: str, target_size: int = 416) -> Tuple[np.ndarray, float, Tuple[int, int]]:\n","    # Robust load (Korean path safe)\n","    image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n","    if image is None:\n","        raise ValueError(f\"이미지를 로드할 수 없습니다: {image_path}\")\n","\n","    original_height, original_width = image.shape[:2]\n","    scale = min(target_size / original_width, target_size / original_height)\n","    new_width = int(original_width * scale)\n","    new_height = int(original_height * scale)\n","\n","    resized_image = cv2.resize(image, (new_width, new_height))\n","    padded_image = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n","    y_offset = (target_size - new_height) // 2\n","    x_offset = (target_size - new_width) // 2\n","    padded_image[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_image\n","\n","    # BGR->RGB and normalize (ImageNet stats)\n","    padded_image = cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB)\n","    padded_image = padded_image.astype(np.float32) / 255.0\n","    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","    padded_image = (padded_image - mean) / std\n","\n","    return padded_image, scale, (x_offset, y_offset)\n","\n","\n","def calculate_iou(box1, box2):\n","    x1_1, y1_1, x2_1, y2_1 = box1\n","    x1_2, y1_2, x2_2, y2_2 = box2\n","\n","    # 면적 계산 (가로 * 세로)\n","    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n","    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n","\n","    # 교집합 좌표\n","    inter_x1 = max(x1_1, x1_2)\n","    inter_y1 = max(y1_1, y1_2)\n","    inter_x2 = min(x2_1, x2_2)\n","    inter_y2 = min(y2_1, y2_2)\n","\n","    # 교집합 넓이 (음수 방지)\n","    inter_width = max(0, inter_x2 - inter_x1)\n","    inter_height = max(0, inter_y2 - inter_y1)\n","    intersection = inter_width * inter_height\n","\n","    # IOU 계산 (분모가 0인 경우 처리)\n","    union = area1 + area2 - intersection\n","    if union == 0:\n","        return 0\n","\n","    iou = intersection / union\n","    return iou\n","\n","\n","def non_max_suppression(boxes: List[List[float]], iou_threshold: float = 0.5) -> List[List[float]]:\n","    if not boxes:\n","        return []\n","    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n","    keep = []\n","    while boxes:\n","        current_box = boxes.pop(0)\n","        keep.append(current_box)\n","        boxes_to_remove = []\n","        for i, box in enumerate(boxes):\n","            iou = calculate_iou(current_box[:4], box[:4])\n","            if iou > iou_threshold:\n","                boxes_to_remove.append(i)\n","        for i in reversed(boxes_to_remove):\n","            boxes.pop(i)\n","    return keep\n","\n","\n","def save_detection_result(image: np.ndarray, output_path: str):\n","    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    success, encoded_image = cv2.imencode('.jpg', image_bgr)\n","    if success:\n","        encoded_image.tofile(output_path)\n","    else:\n","        cv2.imwrite(output_path, image_bgr)\n","def detect_bees(model: YOLOv2, image_path: str, config: Config,\n","                confidence_threshold: float = 0.3, nms_threshold: float = 0.45) -> List[List[float]]:\n","    processed_image, scale, (x_offset, y_offset) = preprocess_image(image_path, config.INPUT_SIZE)\n","    input_tensor = torch.from_numpy(processed_image).unsqueeze(0).permute(0,3,1,2).float()\n","\n","    device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n","    input_tensor = input_tensor.to(device)\n","    model = model.to(device)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        predictions = model(input_tensor)\n","\n","    predictions = predictions.squeeze(0).cpu().numpy()\n","    boxes = []\n","\n","    for gy in range(config.GRID_SIZE):\n","        for gx in range(config.GRID_SIZE):\n","            for anchor_idx in range(config.NUM_ANCHORS):\n","                pred = predictions[gy, gx, anchor_idx]\n","                confidence = torch.sigmoid(torch.tensor(pred[4])).item()\n","\n","                if confidence > confidence_threshold:\n","                    cx = (pred[0] + gx) / config.GRID_SIZE\n","                    cy = (pred[1] + gy) / config.GRID_SIZE\n","                    w = pred[2]\n","                    h = pred[3]\n","\n","                    class_probs = torch.sigmoid(torch.tensor(pred[5:])).numpy()\n","                    class_id = np.argmax(class_probs)\n","                    class_confidence = class_probs[class_id]\n","\n","                    final_confidence = confidence * class_confidence\n","\n","                    if final_confidence > confidence_threshold:\n","                        abs_cx = cx * config.INPUT_SIZE\n","                        abs_cy = cy * config.INPUT_SIZE\n","                        abs_w = w * config.INPUT_SIZE\n","                        abs_h = h * config.INPUT_SIZE\n","\n","                        x1 = int(abs_cx - abs_w / 2)\n","                        y1 = int(abs_cy - abs_h / 2)\n","                        x2 = int(abs_cx + abs_w / 2)\n","                        y2 = int(abs_cy + abs_h / 2)\n","\n","                        x1 = int((x1 - x_offset) / scale)\n","                        y1 = int((y1 - y_offset) / scale)\n","                        x2 = int((x2 - x_offset) / scale)\n","                        y2 = int((y2 - y_offset) / scale)\n","\n","                        x1 = max(0, x1)\n","                        y1 = max(0, y1)\n","                        x2 = max(0, x2)\n","                        y2 = max(0, y2)\n","\n","                        boxes.append([x1, y1, x2, y2, final_confidence, class_id])\n","\n","    if boxes:\n","        boxes = non_max_suppression(boxes, nms_threshold)\n","\n","    return boxes\n","\n","# ------------------------------\n","# 바운딩 박스 그리기\n","# ------------------------------\n","def draw_bounding_boxes(image: np.ndarray, boxes: list) -> np.ndarray:\n","    \"\"\"\n","    boxes: [x1, y1, x2, y2, confidence, class_id]\n","    \"\"\"\n","    for box in boxes:\n","        x1, y1, x2, y2, conf, class_id = box\n","        color = (0, 255, 0)  # 초록색\n","        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n","        label = f\"{class_id}:{conf:.2f}\"\n","        cv2.putText(image, label, (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n","    return image\n"],"metadata":{"id":"BKAPcaCVcu1s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Dataset\n","# =========================\n","class BeeDataset(Dataset):\n","    def __init__(self, config: Config, transform=None, is_training=True):\n","        self.config = config\n","        self.transform = transform\n","        self.is_training = is_training\n","\n","        # 이미지와 JSON 파일 목록 생성\n","        self.image_files = []\n","        self.json_files = []\n","\n","        for filename in os.listdir(config.IMAGES_DIR):\n","            if filename.endswith(('.jpg', '.jpeg', '.png')):\n","                image_path = os.path.join(config.IMAGES_DIR, filename)\n","                json_filename = filename.replace('.jpg', '.json').replace('.jpeg', '.json').replace('.png', '.json')\n","                json_filename = json_filename.replace('TS_', 'TL_')\n","                json_path = os.path.join(config.JSONS_DIR, json_filename)\n","                if os.path.exists(json_path):\n","                    self.image_files.append(image_path)\n","                    self.json_files.append(json_path)\n","\n","        print(f\"데이터셋 로드 완료: {len(self.image_files)} 개의 이미지\")\n","\n","        # 데이터 증강\n","        if is_training and config.AUGMENTATION:\n","            self.aug_transform = A.Compose([\n","                A.HorizontalFlip(p=0.5),\n","                A.VerticalFlip(p=0.1),\n","                A.RandomBrightnessContrast(p=0.3),\n","                A.HueSaturationValue(p=0.3),\n","                A.RandomGamma(p=0.3),\n","                A.GaussNoise(p=0.2),\n","                A.Blur(p=0.1),\n","                A.Resize(config.INPUT_SIZE, config.INPUT_SIZE),\n","                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                ToTensorV2(),\n","            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n","        else:\n","            self.aug_transform = A.Compose([\n","                A.Resize(config.INPUT_SIZE, config.INPUT_SIZE),\n","                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                ToTensorV2(),\n","            ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_files[idx]\n","        json_path = self.json_files[idx]\n","\n","        # 이미지 로드\n","        try:\n","            image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n","            if image is None:\n","                image = np.zeros((416, 416, 3), dtype=np.uint8)\n","            else:\n","                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        except Exception as e:\n","            print(f\"이미지 로드 오류: {image_path}, {e}\")\n","            image = np.zeros((416, 416, 3), dtype=np.uint8)\n","\n","        # 어노테이션 로드\n","        annotation_data = load_json_annotations(json_path)\n","        img_width = annotation_data['IMAGE']['WIDTH']\n","        img_height = annotation_data['IMAGE']['HEIGHT']\n","\n","        # YOLO 형식 변환\n","        yolo_annotations = convert_to_yolo_format(annotation_data, img_width, img_height)\n","\n","        bboxes, class_labels = [], []\n","        for annotation in yolo_annotations:\n","            class_id, cx, cy, w, h = annotation\n","            # Clip bbox\n","            cx = np.clip(cx, 0.0, 1.0)\n","            cy = np.clip(cy, 0.0, 1.0)\n","            w  = np.clip(w, 0.01, 1.0)\n","            h  = np.clip(h, 0.01, 1.0)\n","            bboxes.append([cx, cy, w, h])\n","            class_labels.append(class_id)\n","\n","        # 데이터 증강\n","        if self.transform:\n","            augmented = self.aug_transform(image=image, bboxes=bboxes, class_labels=class_labels)\n","            image = augmented['image']\n","            bboxes = augmented['bboxes']\n","            class_labels = augmented['class_labels']\n","        else:\n","            image = cv2.resize(image, (self.config.INPUT_SIZE, self.config.INPUT_SIZE))\n","            image = image.astype(np.float32) / 255.0\n","            image = np.transpose(image, (2, 0, 1))\n","            image = torch.from_numpy(image).float()\n","\n","        # dynamic anchor target\n","        target = self._create_dynamic_anchor_target(bboxes, class_labels)\n","        return image, target\n","\n","    def _create_dynamic_anchor_target(self, bboxes, class_labels):\n","        target = torch.zeros((self.config.GRID_SIZE, self.config.GRID_SIZE,\n","                              self.config.NUM_ANCHORS, 5 + self.config.NUM_CLASSES))\n","\n","        for bbox, cls in zip(bboxes, class_labels):\n","            cx, cy, w, h = bbox\n","            grid_x = int(cx * self.config.GRID_SIZE)\n","            grid_y = int(cy * self.config.GRID_SIZE)\n","            grid_cx = cx * self.config.GRID_SIZE - grid_x\n","            grid_cy = cy * self.config.GRID_SIZE - grid_y\n","\n","            # dynamic anchor smoothing\n","            best_anchor = 0\n","            best_iou = 0\n","            for i, (aw, ah) in enumerate(self.config.ANCHORS[:self.config.NUM_ANCHORS]):\n","                aw_n, ah_n = aw / self.config.INPUT_SIZE, ah / self.config.INPUT_SIZE\n","                iou = min(w / aw_n, h / ah_n) * min(aw_n / w, ah_n / h)\n","                iou = np.clip(iou, 0.0, 1.0)  # smoothing\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_anchor = i\n","\n","            if grid_x < self.config.GRID_SIZE and grid_y < self.config.GRID_SIZE:\n","                target[grid_y, grid_x, best_anchor, 0] = grid_cx\n","                target[grid_y, grid_x, best_anchor, 1] = grid_cy\n","                target[grid_y, grid_x, best_anchor, 2] = w\n","                target[grid_y, grid_x, best_anchor, 3] = h\n","                target[grid_y, grid_x, best_anchor, 4] = 1.0  # objectness\n","                target[grid_y, grid_x, best_anchor, 5 + cls] = 1.0\n","\n","        return target\n","\n","\n","\n","def create_data_loaders(config: Config, train_ratio: float = 0.8):\n","    full_dataset = BeeDataset(config)\n","    total_size = len(full_dataset)\n","    train_size = int(total_size * train_ratio)\n","    val_size = total_size - train_size\n","    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n","\n","    pin = torch.cuda.is_available()\n","    train_loader = DataLoader(train_dataset, batch_size=(config.BATCH_SIZE if hasattr(config, 'BATCH_SIZE') else config.BATCH_SIZE_CPU),\n","                              shuffle=True, num_workers=2, pin_memory=pin)\n","    val_loader = DataLoader(val_dataset, batch_size=(config.BATCH_SIZE if hasattr(config, 'BATCH_SIZE') else config.BATCH_SIZE_CPU),\n","                            shuffle=False, num_workers=2, pin_memory=pin)\n","    return train_loader, val_loader\n","\n"],"metadata":{"id":"9taSUk0Jc0WE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Model\n","# =========================\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3,\n","                 stride: int = 1, padding: int = 1, batch_norm: bool = True):\n","        super(ConvBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=not batch_norm)\n","        self.bn = nn.BatchNorm2d(out_channels) if batch_norm else None\n","        self.leaky_relu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.bn is not None:\n","            x = self.bn(x)\n","        x = self.leaky_relu(x)\n","        return x\n","\n","\n","class Darknet19(nn.Module):\n","    def __init__(self):\n","        super(Darknet19, self).__init__()\n","        self.conv1 = ConvBlock(3, 32)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.conv2 = ConvBlock(32, 64)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.conv3 = ConvBlock(64, 128)\n","        self.conv4 = ConvBlock(128, 64, kernel_size=1, padding=0)\n","        self.conv5 = ConvBlock(64, 128)\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        self.conv6 = ConvBlock(128, 256)\n","        self.conv7 = ConvBlock(256, 128, kernel_size=1, padding=0)\n","        self.conv8 = ConvBlock(128, 256)\n","        self.pool4 = nn.MaxPool2d(2, 2)\n","        self.conv9 = ConvBlock(256, 512)\n","        self.conv10 = ConvBlock(512, 256, kernel_size=1, padding=0)\n","        self.conv11 = ConvBlock(256, 512)\n","        self.conv12 = ConvBlock(512, 256, kernel_size=1, padding=0)\n","        self.conv13 = ConvBlock(256, 512)\n","        self.pool5 = nn.MaxPool2d(2, 2)\n","        self.conv14 = ConvBlock(512, 1024)\n","        self.conv15 = ConvBlock(1024, 512, kernel_size=1, padding=0)\n","        self.conv16 = ConvBlock(512, 1024)\n","        self.conv17 = ConvBlock(1024, 512, kernel_size=1, padding=0)\n","        self.conv18 = ConvBlock(512, 1024)\n","\n","    def forward(self, x):\n","        route_1 = self.conv1(x)\n","        route_1 = self.pool1(route_1)\n","        route_1 = self.conv2(route_1)\n","        route_1 = self.pool2(route_1)\n","        route_1 = self.conv3(route_1)\n","        route_1 = self.conv4(route_1)\n","        route_1 = self.conv5(route_1)\n","        route_1 = self.pool3(route_1)\n","        route_1 = self.conv6(route_1)\n","        route_1 = self.conv7(route_1)\n","        route_1 = self.conv8(route_1)\n","        route_1 = self.pool4(route_1)\n","        route_1 = self.conv9(route_1)\n","        route_1 = self.conv10(route_1)\n","        route_1 = self.conv11(route_1)\n","        route_1 = self.conv12(route_1)\n","        route_1 = self.conv13(route_1)\n","\n","        route_2 = self.pool5(route_1)\n","        route_2 = self.conv14(route_2)\n","        route_2 = self.conv15(route_2)\n","        route_2 = self.conv16(route_2)\n","        route_2 = self.conv17(route_2)\n","        route_2 = self.conv18(route_2)\n","\n","        return route_1, route_2\n","\n","\n","class YOLOv2(nn.Module):\n","    def __init__(self, config: Config, num_classes: int = 1, num_anchors: int = 5):\n","        super(YOLOv2, self).__init__()\n","        self.config = config # Store config as an attribute\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.backbone = Darknet19()\n","        self.conv19 = ConvBlock(1024, 1024, kernel_size=3, padding=1)\n","        self.conv20 = ConvBlock(1024, 1024, kernel_size=3, padding=1)\n","        self.route1_resize = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.1)\n","        )\n","        self.conv21 = ConvBlock(512, 64, kernel_size=1, padding=0)\n","        self.conv22 = nn.Conv2d(1024 + 64, num_anchors * (5 + num_classes), kernel_size=1)\n","\n","    def forward(self, x):\n","        route_1, route_2 = self.backbone(x)\n","        x = self.conv19(route_2); x = self.conv20(x)\n","        route_1_resized = self.route1_resize(route_1)\n","        route_1_processed = self.conv21(route_1_resized)\n","        x = torch.cat([x, route_1_processed], dim=1)\n","\n","        # 반드시 head 적용\n","        x = self.conv22(x)  # (B, A*(5+C), H, W)\n","\n","        b, c, h, w = x.shape\n","        x = x.view(b, self.num_anchors, 5 + self.num_classes, h, w)\n","        x = x.permute(0, 3, 4, 1, 2).contiguous()  # (B, H, W, A, 5+C)\n","        return x\n","\n","class YOLOLoss(nn.Module):\n","    def __init__(self, num_classes: int = 1, num_anchors: int = 5,\n","                 anchors: Tuple[Tuple[int, int], ...] = None, input_size: int = 416,\n","                 lambda_coord: float = 10.0, lambda_noobj: float = 1.0, lambda_class: float = 2.0):\n","        super(YOLOLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_anchors = num_anchors\n","        self.input_size = input_size\n","        if anchors is None:\n","            anchors = tuple((10, 13), (16, 30), (33, 23), (30, 61), (62, 45))\n","        self.register_buffer('anchor_ws', torch.tensor([a[0] for a in anchors[:num_anchors]], dtype=torch.float32).view(1, 1, 1, num_anchors))\n","        self.register_buffer('anchor_hs', torch.tensor([a[1] for a in anchors[:num_anchors]], dtype=torch.float32).view(1, 1, 1, num_anchors))\n","        self.lambda_coord = lambda_coord\n","        self.lambda_noobj = lambda_noobj\n","        self.lambda_class = lambda_class\n","        self.mse_loss = nn.MSELoss(reduction='sum')\n","        self.bce_logits = nn.BCEWithLogitsLoss(reduction='sum')\n","\n","    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n","        pred_x = predictions[..., 0]\n","        pred_y = predictions[..., 1]\n","        pred_w = predictions[..., 2]\n","        pred_h = predictions[..., 3]\n","        pred_conf = predictions[..., 4]\n","        pred_cls = predictions[..., 5:]\n","\n","        target_x = targets[..., 0]\n","        target_y = targets[..., 1]\n","        target_w = targets[..., 2]\n","        target_h = targets[..., 3]\n","        target_conf = targets[..., 4]\n","        target_cls = targets[..., 5:]\n","\n","        obj_mask = target_conf > 0\n","        noobj_mask = target_conf == 0\n","\n","        pred_x_sig = torch.sigmoid(pred_x)\n","        pred_y_sig = torch.sigmoid(pred_y)\n","        pred_w_clamped = torch.clamp(pred_w, min=-6.0, max=6.0)\n","        pred_h_clamped = torch.clamp(pred_h, min=-6.0, max=6.0)\n","\n","        anchor_ws = self.anchor_ws.to(predictions.device)\n","        anchor_hs = self.anchor_hs.to(predictions.device)\n","        pred_w_dec = torch.exp(pred_w_clamped) * (anchor_ws / float(self.input_size))\n","        pred_h_dec = torch.exp(pred_h_clamped) * (anchor_hs / float(self.input_size))\n","\n","        eps = 1.0 / float(self.input_size)\n","        target_w_safe = torch.clamp(target_w, min=eps, max=1.0)\n","        target_h_safe = torch.clamp(target_h, min=eps, max=1.0)\n","\n","        coord_loss = self.lambda_coord * (\n","            self.mse_loss(pred_x_sig[obj_mask], target_x[obj_mask]) +\n","            self.mse_loss(pred_y_sig[obj_mask], target_y[obj_mask]) +\n","            self.mse_loss(pred_w_dec[obj_mask], target_w_safe[obj_mask]) +\n","            self.mse_loss(pred_h_dec[obj_mask], target_h_safe[obj_mask])\n","        )\n","\n","        conf_loss_obj = self.bce_logits(pred_conf[obj_mask], target_conf[obj_mask])\n","        conf_loss_noobj = self.lambda_noobj * self.bce_logits(pred_conf[noobj_mask], target_conf[noobj_mask])\n","        class_loss = self.lambda_class * self.bce_logits(pred_cls[obj_mask], target_cls[obj_mask])\n","\n","        total_loss = coord_loss + conf_loss_obj + conf_loss_noobj + class_loss\n","        batch_size = predictions.size(0)\n","        return total_loss / max(1, batch_size)"],"metadata":{"id":"-TXB-NIsc4ns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# Training\n","# =========================\n","def train_model(config: Config, resume_path: str = None):\n","    print(\"YOLOv2 벌 감지 모델 학습을 시작합니다...\")\n","    device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n","    print(f\"사용 디바이스: {device}\")\n","\n","    if torch.cuda.is_available():\n","        config.BATCH_SIZE = config.BATCH_SIZE_GPU\n","    else:\n","        config.BATCH_SIZE = config.BATCH_SIZE_CPU\n","    print(f\"배치 크기: {config.BATCH_SIZE}\")\n","\n","    train_loader, val_loader = create_data_loaders(config)\n","\n","    model = YOLOv2(config=config, num_classes=config.NUM_CLASSES, num_anchors=config.NUM_ANCHORS).to(device)\n","\n","    criterion = YOLOLoss(\n","        num_classes=config.NUM_CLASSES,\n","        num_anchors=config.NUM_ANCHORS,\n","        anchors=tuple(config.ANCHORS[:config.NUM_ANCHORS]),\n","        input_size=config.INPUT_SIZE,\n","        lambda_noobj=0.5\n","    ).to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=config.BASE_LR, weight_decay=config.WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n","\n","    log_dir = os.path.join(config.OUTPUT_DIR, \"logs\", datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n","    writer = SummaryWriter(log_dir)\n","\n","    best_val_loss = float('inf')\n","    start_epoch = 0\n","\n","    # 체크포인트 재개\n","    if resume_path is not None:\n","        if resume_path == 'auto':\n","            resume_candidate = _find_latest_checkpoint(config.CHECKPOINT_DIR)\n","            if resume_candidate is not None:\n","                resume_path = resume_candidate\n","        if resume_path and os.path.exists(resume_path):\n","            print(f\"재개 체크포인트 로드: {resume_path}\")\n","            ckpt = torch.load(resume_path, map_location=device, weights_only=False)\n","            model.load_state_dict(ckpt['model_state_dict'])\n","            if 'optimizer_state_dict' in ckpt:\n","                optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n","            if 'scheduler_state_dict' in ckpt and ckpt['scheduler_state_dict'] is not None:\n","                try:\n","                    scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n","                except Exception:\n","                    pass\n","            if 'loss' in ckpt:\n","                best_val_loss = ckpt['loss']\n","            if 'epoch' in ckpt:\n","                start_epoch = ckpt['epoch'] + 1\n","            print(f\"재개 시작 에포크: {start_epoch}, best_val_loss: {best_val_loss:.4f}\")\n","\n","    train_global_step = 0\n","    for epoch in range(start_epoch, config.NUM_EPOCHS):\n","        # ===== Warm-up Learning Rate 적용 =====\n","        if epoch < config.WARMUP_EPOCHS:\n","            lr = config.BASE_LR * (epoch + 1) / config.WARMUP_EPOCHS\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr\n","        else:\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = scheduler.get_last_lr()[0]\n","\n","        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","        # --- Training ---\n","        model.train()\n","        train_loss = 0.0\n","        train_batches = 0\n","\n","        for images, targets in train_loader:\n","            images = images.to(device)\n","            targets = targets.to(device)\n","\n","            optimizer.zero_grad()\n","            predictions = model(images)\n","            loss = criterion(predictions, targets)\n","            if not torch.isfinite(loss):\n","                print(\"비유한(loss) 발생: 배치 스킵\")\n","                continue\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_batches += 1\n","            writer.add_scalar('Loss/Train_Batch', loss.item(), train_global_step)\n","            train_global_step += 1\n","\n","        avg_train_loss = train_loss / max(1, train_batches)\n","\n","        # --- Validation ---\n","        model.eval()\n","        val_loss = 0.0\n","        val_batches = 0\n","        with torch.no_grad():\n","            for images, targets in val_loader:\n","                images = images.to(device)\n","                targets = targets.to(device)\n","                predictions = model(images)\n","                loss = criterion(predictions, targets)\n","                if not torch.isfinite(loss):\n","                    continue\n","                val_loss += loss.item()\n","                val_batches += 1\n","\n","        avg_val_loss = val_loss / max(1, val_batches)\n","        scheduler.step(avg_val_loss)\n","\n","        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n","        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n","        writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n","        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","        # --- Checkpoint 저장 ---\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'loss': avg_val_loss,\n","            'config': config\n","        }, os.path.join(config.CHECKPOINT_DIR, 'last_checkpoint.pth'))\n","\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scheduler_state_dict': scheduler.state_dict(),\n","                'loss': best_val_loss,\n","                'config': config\n","            }, os.path.join(config.CHECKPOINT_DIR, 'best_model.pth'))\n","            print(f\"새로운 최고 모델 저장: {best_val_loss:.4f}\")\n","\n","        if (epoch + 1) % config.SAVE_INTERVAL == 0:\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scheduler_state_dict': scheduler.state_dict(),\n","                'loss': avg_val_loss,\n","                'config': config\n","            }, os.path.join(config.CHECKPOINT_DIR, f'model_epoch_{epoch+1}.pth'))\n","\n","    writer.close()\n","    print(\"학습 완료!\")\n","    return model\n","\n","\n","def main():\n","    config = Config()\n","    print(\"\\n모델 학습을 시작합니다...\")\n","    train_model(config, resume_path='auto')\n","    print(\"모델 학습이 완료되었습니다!\")\n","    print(f\"결과는 {config.OUTPUT_DIR} 폴더에 저장되었습니다.\")\n","\n","main()"],"metadata":{"id":"04nv8EjTc9IV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Array 기반 preprocess (patch용)\n","# -----------------------------\n","\n","def preprocess_image_from_array(image: np.ndarray, input_size: int):\n","    h, w, _ = image.shape\n","    scale = min(input_size / w, input_size / h)\n","    new_w, new_h = int(w * scale), int(h * scale)\n","    resized = cv2.resize(image, (new_w, new_h))\n","    canvas = np.zeros((input_size, input_size, 3), dtype=np.uint8)\n","    x_offset = (input_size - new_w) // 2\n","    y_offset = (input_size - new_h) // 2\n","    canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w, :] = resized\n","    processed = canvas / 255.0\n","    return processed.astype(np.float32), scale, (x_offset, y_offset)\n","\n","# -----------------------------\n","# Patch 단위 이미지 감지\n","# -----------------------------\n","def detect_bees_patch(model: YOLOv2, image_path: str, config: Config,\n","                      patch_size: int = 416, stride: int = 416,\n","                      confidence_threshold: float = 0.3,\n","                      nms_threshold: float = 0.45) -> List[List[float]]:\n","\n","    image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n","    if image is None:\n","        raise ValueError(f\"이미지를 로드할 수 없습니다: {image_path}\")\n","    orig_h, orig_w = image.shape[:2]\n","\n","    device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    model.eval()\n","\n","    all_boxes = []\n","\n","    for y0 in range(0, orig_h, stride):\n","        for x0 in range(0, orig_w, stride):\n","            y1 = min(y0 + patch_size, orig_h)\n","            x1 = min(x0 + patch_size, orig_w)\n","            patch = image[y0:y1, x0:x1].copy()\n","            ph, pw = patch.shape[:2]\n","\n","            processed_patch, scale, (x_offset, y_offset) = preprocess_image_from_array(patch, config.INPUT_SIZE)\n","            input_tensor = torch.from_numpy(processed_patch).unsqueeze(0).permute(0,3,1,2).float().to(device)\n","\n","            with torch.no_grad():\n","                predictions = model(input_tensor).squeeze(0).cpu().numpy()\n","\n","            for gy in range(config.GRID_SIZE):\n","                for gx in range(config.GRID_SIZE):\n","                    for anchor_idx in range(config.NUM_ANCHORS):\n","                        pred = predictions[gy, gx, anchor_idx]\n","                        confidence = torch.sigmoid(torch.tensor(pred[4])).item()\n","\n","                        if confidence > confidence_threshold:\n","                            cx = (pred[0] + gx) / config.GRID_SIZE\n","                            cy = (pred[1] + gy) / config.GRID_SIZE\n","                            w = pred[2]\n","                            h = pred[3]\n","\n","                            class_probs = torch.sigmoid(torch.tensor(pred[5:])).numpy()\n","                            class_id = np.argmax(class_probs)\n","                            class_confidence = class_probs[class_id]\n","                            final_conf = confidence * class_confidence\n","\n","                            if final_conf > confidence_threshold:\n","                                abs_cx = cx * config.INPUT_SIZE\n","                                abs_cy = cy * config.INPUT_SIZE\n","                                abs_w = w * config.INPUT_SIZE\n","                                abs_h = h * config.INPUT_SIZE\n","\n","                                x1_box = int((abs_cx - abs_w/2 - x_offset)/scale + x0)\n","                                y1_box = int((abs_cy - abs_h/2 - y_offset)/scale + y0)\n","                                x2_box = int((abs_cx + abs_w/2 - x_offset)/scale + x0)\n","                                y2_box = int((abs_cy + abs_h/2 - y_offset)/scale + y0)\n","\n","                                x1_box = max(0, x1_box)\n","                                y1_box = max(0, y1_box)\n","                                x2_box = min(orig_w, x2_box)\n","                                y2_box = min(orig_h, y2_box)\n","\n","                                all_boxes.append([x1_box, y1_box, x2_box, y2_box, final_conf, class_id])\n","\n","    if all_boxes:\n","        all_boxes = non_max_suppression(all_boxes, nms_threshold)\n","\n","    return all_boxes\n","\n","# -----------------------------\n","# 디렉토리 이미지 처리\n","# -----------------------------\n","def process_images(model: YOLOv2, config: Config, input_dir: str, output_dir: str,\n","                   confidence_threshold: float = 0.3, patch_size: int = 416, stride: int = 416):\n","    os.makedirs(output_dir, exist_ok=True)\n","    image_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir)\n","                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n","\n","    print(f\"총 {len(image_files)}장 처리 시작...\")\n","\n","    for i, image_path in enumerate(image_files):\n","        print(f\"[{i+1}/{len(image_files)}] 처리 중: {os.path.basename(image_path)}\")\n","        try:\n","            boxes = detect_bees_patch(model, image_path, config,\n","                                      confidence_threshold=confidence_threshold,\n","                                      patch_size=patch_size, stride=stride)\n","\n","            original_image = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n","            if original_image is None:\n","                print(f\"  이미지를 불러올 수 없음: {image_path}\")\n","                continue\n","            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n","\n","            result_image = draw_bounding_boxes(original_image, boxes) if boxes else original_image\n","            output_path = os.path.join(output_dir, f\"detected_{os.path.basename(image_path)}\")\n","            save_detection_result(result_image, output_path)\n","\n","            print(f\"  감지된 벌 개수: {len(boxes)}\")\n","        except Exception as e:\n","            print(f\"  오류 발생: {e}\")\n","            continue\n","\n","    print(f\"감지 완료! 결과는 {output_dir}에 저장되었습니다.\")"],"metadata":{"id":"2PnWa7Q6dHl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 메인 실행\n","# -----------------------------\n","def main(model_path: str = None,\n","         input_dir: str = '/content/data/images',\n","         output_dir: str = '/content/drive/MyDrive/yolo/output/results/test3',\n","         confidence: float = 0.3,\n","         patch_size: int = 416,\n","         stride: int = 416):\n","    config = Config()\n","\n","    if model_path is None:\n","        model_path = os.path.join(config.CHECKPOINT_DIR, 'best_model.pth')\n","\n","    if not os.path.exists(model_path):\n","        print(f\"모델 파일 없음: {model_path}\")\n","        return\n","\n","    device = torch.device(config.DEVICE if torch.cuda.is_available() else \"cpu\")\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    model = YOLOv2(num_classes=config.NUM_CLASSES, num_anchors=config.NUM_ANCHORS, config=config)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model = model.to(device)\n","    model.eval()\n","\n","    print(f\"모델 로드 완료: {model_path}\")\n","    if 'epoch' in checkpoint and 'loss' in checkpoint:\n","        print(f\"Epoch: {checkpoint['epoch']}, Loss: {checkpoint['loss']:.4f}\")\n","\n","    process_images(model, config, input_dir, output_dir,\n","                   confidence_threshold=confidence,\n","                   patch_size=patch_size, stride=stride)\n","\n","# -----------------------------\n","# 실행\n","# -----------------------------\n","if __name__ == \"__main__\":\n","    main(input_dir='/content/data/images',\n","         output_dir='/content/drive/MyDrive/yolo/output/results/test3',\n","         confidence=0.01,\n","         patch_size=416,\n","         stride=416)"],"metadata":{"id":"7iH4eFD7dVKk"},"execution_count":null,"outputs":[]}]}