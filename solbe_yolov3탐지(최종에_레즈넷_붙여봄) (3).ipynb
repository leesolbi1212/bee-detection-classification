{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_apKZ09nUE5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMDV6zayWAMp"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/MyDrive/AI활용 소프트웨어 개발/10. 파이썬 웹 서비스/Bee/data/Bee (1).zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUiDE62jWMie"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [STEP 1-2] 임포트 & 하드웨어 점검 & 시드 고정\n",
        "# ============================\n",
        "import os, random, json, math, time                     # 표준 라이브러리\n",
        "from pathlib import Path                                # 경로 다루기\n",
        "import numpy as np                                      # 수치 연산\n",
        "import torch                                            # PyTorch 메인\n",
        "import torch.backends.cudnn as cudnn                    # CUDNN 옵션\n",
        "from torch.cuda.amp import GradScaler, autocast         # AMP(Mixed Precision)\n",
        "import math, random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukc47ZfxWQAj"
      },
      "outputs": [],
      "source": [
        "# GPU 사용 가능 여부 출력 (A100이 보이는지 확인)\n",
        "print('[INFO] CUDA available:', torch.cuda.is_available())          # True면 GPU 사용 가능\n",
        "if torch.cuda.is_available():                                       # GPU가 있으면\n",
        "    print('[INFO] CUDA device:', torch.cuda.get_device_name(0))     # GPU 장치명 출력\n",
        "    !nvidia-smi                                                      # 드라이버/메모리 상태 출력\n",
        "else:\n",
        "    print('[WARN] GPU가 보이지 않습니다. 런타임 -> 런타임 유형 변경 -> 하드웨어 가속기: GPU(A100) 선택 필요')\n",
        "\n",
        "# 재현성(결과 일관성) 위해 시드 고정\n",
        "SEED = 42                                                           # 고정 시드 값\n",
        "random.seed(SEED)                                                   # 파이썬 랜덤 시드\n",
        "np.random.seed(SEED)                                                # 넘파이 시드\n",
        "torch.manual_seed(SEED)                                             # 파이토치 CPU 시드\n",
        "torch.cuda.manual_seed_all(SEED) if torch.cuda.is_available() else None  # 파이토치 GPU 시드\n",
        "\n",
        "# 연산 일관성/속도 설정 (deterministic은 True면 매우 일관, 다만 약간 느릴 수 있음)\n",
        "cudnn.deterministic = True                                          # 결정적 알고리즘 사용\n",
        "cudnn.benchmark    = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADR5Cq5tWRGY"
      },
      "outputs": [],
      "source": [
        "# 프로젝트 루트 경로 지정\n",
        "ROOT = Path('/content/bee_project')                   # 프로젝트 루트\n",
        "\n",
        "# 데이터/출력 폴더 구조 정의\n",
        "DIRS = [\n",
        "    ROOT / 'images' / 'train',                                      # 원본 이미지(train)\n",
        "    ROOT / 'images' / 'val',                                        # 원본 이미지(val)\n",
        "    ROOT / 'images' / 'test',                                       # 원본 이미지(test) - 선택\n",
        "    ROOT / 'labels' / 'train',                                      # 원본 라벨(JSON 등, train)\n",
        "    ROOT / 'labels' / 'val',                                        # 원본 라벨(JSON 등, val)\n",
        "    ROOT / 'labels' / 'test',                                       # 원본 라벨(JSON 등, test)\n",
        "    ROOT / 'yolo_labels' / 'train',                                 # 변환된 YOLO 형식 라벨(txt, train)\n",
        "    ROOT / 'yolo_labels' / 'val',                                   # 변환된 YOLO 형식 라벨(txt, val)\n",
        "    ROOT / 'crops' / 'train',                                       # 탐지/GT 크롭(train)\n",
        "    ROOT / 'crops' / 'val',                                         # 탐지/GT 크롭(val)\n",
        "    ROOT / 'outputs' / 'weights',                                   # 체크포인트 저장 경로\n",
        "    ROOT / 'outputs' / 'reports',                                   # 리포트/CSV/그래프 저장 경로\n",
        "    ROOT / 'outputs' / 'viz',                                       # 시각화 결과(박스/크롭 등)\n",
        "]\n",
        "\n",
        "# 폴더 생성 (이미 있으면 건너뜀)\n",
        "for d in DIRS:\n",
        "    d.mkdir(parents=True, exist_ok=True)                             # 상위 폴더까지 생성\n",
        "print('[INFO] Project root:', ROOT)                                  # 루트 경로 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjjsmojZWVBh"
      },
      "outputs": [],
      "source": [
        "# Robust Split + Rename: \"__\" 뒤 토큰/JSON 이미지명 활용해서 매칭 → 7:2:1 분할 후 강제 베이스네임으로 저장\n",
        "import re, json, random, shutil, collections\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) SOURCE 경로(원천)\n",
        "SRC_IMG_BASE = Path('/content/images')\n",
        "SRC_LBL_BASE = Path('/content/jsons')\n",
        "\n",
        "# 2) DEST 경로(저장)\n",
        "DEST_IMG_BASE = Path('/content/bee_project/images')\n",
        "DEST_LBL_BASE = Path('/content/bee_project/labels')\n",
        "\n",
        "# 3) 옵션\n",
        "RATIOS      = (0.7, 0.2, 0.1)     # train, val, test\n",
        "MOVE_FILES  = True                # 원본 이동(True) / 복사(False)\n",
        "SEED        = 42\n",
        "IMG_EXTS    = {'.jpg','.jpeg','.png','.bmp','.webp','.JPG','.JPEG','.PNG'}\n",
        "LBL_EXTS    = {'.json','.JSON'}\n",
        "\n",
        "# ------------ 유틸 ------------\n",
        "def under_split_dir(p: Path, base: Path) -> bool:\n",
        "    try:\n",
        "        rel = p.relative_to(base)\n",
        "    except ValueError:\n",
        "        return False\n",
        "    return len(rel.parts) > 0 and rel.parts[0] in {'train','val','test'}\n",
        "\n",
        "def rscan(base: Path, exts: set):\n",
        "    return sorted([p for p in base.rglob('*')\n",
        "                   if p.is_file() and p.suffix in exts and not under_split_dir(p, base)])\n",
        "\n",
        "def tail_after_double_underscore(name: str) -> str:\n",
        "    \"\"\"'...__01_1_D_AB_AP_20220812_01_0011.jpg' -> '01_1_D_AB_AP_20220812_01_0011'\"\"\"\n",
        "    stem = Path(name).stem\n",
        "    return stem.split('__')[-1] if '__' in stem else stem\n",
        "\n",
        "def read_image_name_from_json(jp: Path):\n",
        "    try:\n",
        "        d = json.loads(jp.read_text(encoding='utf-8'))\n",
        "    except Exception:\n",
        "        return None\n",
        "    if isinstance(d, dict) and isinstance(d.get('IMAGE'), dict):\n",
        "        for k in ('IMAGE_FILE_NAME','FILE_NAME','filename'):\n",
        "            v = d['IMAGE'].get(k)\n",
        "            if isinstance(v, str) and v.strip():\n",
        "                return Path(v).name\n",
        "    for k in ('imagePath','filename','file_name','image'):\n",
        "        v = d.get(k)\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            return Path(v).name\n",
        "    return None\n",
        "\n",
        "def update_json_image_name(jp: Path, new_img_name: str):\n",
        "    try:\n",
        "        data = json.loads(jp.read_text(encoding='utf-8'))\n",
        "    except Exception:\n",
        "        return\n",
        "    if isinstance(data, dict):\n",
        "        if isinstance(data.get('IMAGE'), dict):\n",
        "            for k in ('IMAGE_FILE_NAME','FILE_NAME','filename'):\n",
        "                if k in data['IMAGE']:\n",
        "                    data['IMAGE'][k] = new_img_name\n",
        "        for k in ('imagePath','filename','file_name','image'):\n",
        "            if k in data:\n",
        "                data[k] = new_img_name\n",
        "    jp.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "def transfer(src: Path, dst: Path, move=True):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if move:\n",
        "        shutil.move(str(src), str(dst))\n",
        "    else:\n",
        "        shutil.copy2(str(src), str(dst))\n",
        "\n",
        "def unique_basename(dst_img_dir: Path, dst_lbl_dir: Path, base: str, img_ext: str) -> str:\n",
        "    \"\"\"이미지/라벨 모두와 충돌 없도록 베이스네임 고유화\"\"\"\n",
        "    b = base\n",
        "    k = 1\n",
        "    while (dst_img_dir / f\"{b}{img_ext}\").exists() or (dst_lbl_dir / f\"{b}.json\").exists():\n",
        "        b = f\"{base}_{k}\"\n",
        "        k += 1\n",
        "    return b\n",
        "\n",
        "# ------------ 수집 ------------\n",
        "imgs = rscan(SRC_IMG_BASE, IMG_EXTS)\n",
        "lbls = rscan(SRC_LBL_BASE, LBL_EXTS)\n",
        "\n",
        "print(f\"[DEBUG] found images={len(imgs)}, labels={len(lbls)}\")\n",
        "if not imgs: print(f\"[HINT] 이미지가 없어요: {SRC_IMG_BASE}\")\n",
        "if not lbls: print(f\"[HINT] 라벨이 없어요: {SRC_LBL_BASE}\")\n",
        "\n",
        "# 이미지 인덱스: 여러 키로 접근 가능하게 구성\n",
        "img_index = {}  # key(lower) -> Path\n",
        "def add_img_key(key: str, p: Path):\n",
        "    key = key.strip().lower()\n",
        "    if key and key not in img_index:\n",
        "        img_index[key] = p\n",
        "\n",
        "for ip in imgs:\n",
        "    stem = ip.stem\n",
        "    tail = tail_after_double_underscore(ip.name)\n",
        "    add_img_key(stem, ip)\n",
        "    add_img_key(tail, ip)\n",
        "\n",
        "# 라벨→이미지 매칭\n",
        "pairs = []\n",
        "unmatched = []\n",
        "for jp in lbls:\n",
        "    # 1) JSON 내부 이미지명 기반 후보\n",
        "    nm = read_image_name_from_json(jp)\n",
        "    cand_keys = []\n",
        "    if nm:\n",
        "        cand_keys += [Path(nm).stem, tail_after_double_underscore(nm)]\n",
        "    # 2) 라벨 파일명 기반 후보\n",
        "    cand_keys += [jp.stem, tail_after_double_underscore(jp.name)]\n",
        "    # 매칭\n",
        "    ip = None\n",
        "    for k in cand_keys:\n",
        "        ip = img_index.get(k.strip().lower())\n",
        "        if ip:\n",
        "            break\n",
        "    if ip:\n",
        "        pairs.append((ip, jp))\n",
        "    else:\n",
        "        unmatched.append(jp)\n",
        "\n",
        "print(f\"[DEBUG] paired={len(pairs)}  unmatched_labels={len(unmatched)}\")\n",
        "if unmatched[:5]:\n",
        "    print(\"[DEBUG] sample unmatched label stems:\", [p.stem for p in unmatched[:5]])\n",
        "\n",
        "# 쌍이 0이면 여기서 종료(원인 로그 확인)\n",
        "if len(pairs) == 0:\n",
        "    print(\"[ERROR] 이미지-라벨 매칭 0건입니다. 위 [DEBUG] 출력으로 stem/tail/JSON 이미지명을 확인하세요.\")\n",
        "else:\n",
        "    # ------------ 분할 ------------\n",
        "    random.seed(SEED)\n",
        "    random.shuffle(pairs)\n",
        "    N = len(pairs)\n",
        "    n_tr = int(N*RATIOS[0]); n_va = int(N*RATIOS[1]); n_te = N - n_tr - n_va\n",
        "    splits = {'train': pairs[:n_tr], 'val': pairs[n_tr:n_tr+n_va], 'test': pairs[n_tr+n_va:]}\n",
        "\n",
        "    for sp in splits:\n",
        "        (DEST_IMG_BASE/sp).mkdir(parents=True, exist_ok=True)\n",
        "        (DEST_LBL_BASE/sp).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 같은 이미지가 여러 라벨과 매칭되었는지 체크(있으면 이미지만 copy로 강제)\n",
        "    img_counts = collections.Counter([ip for ip, _ in pairs])\n",
        "    dup_imgs_exist = any(c > 1 for c in img_counts.values())\n",
        "    MOVE_IMG = MOVE_FILES and not dup_imgs_exist\n",
        "    if dup_imgs_exist:\n",
        "        print(\"[WARN] 동일 이미지에 여러 라벨이 연결됨 → 이미지 파일은 copy 모드로 전환(MOVE_IMG=False)\")\n",
        "\n",
        "    # ------------ 저장(이름 강제 규칙) ------------\n",
        "    for sp, items in splits.items():\n",
        "        dst_idir = DEST_IMG_BASE/sp\n",
        "        dst_ldr  = DEST_LBL_BASE/sp\n",
        "        for ip, jp in items:\n",
        "            # 최종 베이스네임: \"__\" 뒤 토큰 우선, 없으면 stem\n",
        "            nm_json = read_image_name_from_json(jp)\n",
        "            if nm_json:\n",
        "                base = tail_after_double_underscore(nm_json)\n",
        "            else:\n",
        "                base = tail_after_double_underscore(jp.name)\n",
        "\n",
        "            img_ext = ip.suffix.lower()\n",
        "            base = unique_basename(dst_idir, dst_ldr, base, img_ext)\n",
        "\n",
        "            dst_img = dst_idir / f\"{base}{img_ext}\"\n",
        "            dst_lbl = dst_ldr  / f\"{base}.json\"\n",
        "\n",
        "            # 이미지/라벨 저장\n",
        "            transfer(ip, dst_img, move=MOVE_IMG)          # 이미지: 중복 시 copy\n",
        "            transfer(jp, dst_lbl, move=MOVE_FILES)        # 라벨: 설정 그대로\n",
        "\n",
        "            # JSON 내부 이미지 파일명 동기화\n",
        "            update_json_image_name(dst_lbl, dst_img.name)\n",
        "\n",
        "    print(f\"[DONE] total pairs={N}  train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}\")\n",
        "    print(f\"[DEST] images -> {DEST_IMG_BASE}\")\n",
        "    print(f\"[DEST] labels -> {DEST_LBL_BASE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gpYueIRWVuE"
      },
      "outputs": [],
      "source": [
        "# 원본 폴더 삭제해주기\n",
        "!rm -rf /content/images\n",
        "!rm -rf /content/jsons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsFF3H5pWfoo"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [STEP 2-1] 경로/설정 준비\n",
        "#  - 1단계에서 만든 CONFIG가 없어도 안전하게 동작하도록 방어 코드를 포함\n",
        "# ============================\n",
        "import os, json, csv, math\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) 프로젝트 루트 기본값 (1단계와 동일)\n",
        "DEFAULT_ROOT = Path('/content/bee_project')\n",
        "\n",
        "# 2) 실제 데이터가 있는 원본 루트(사용자 제공 구조)\n",
        "#    sample_2400/\n",
        "#      ├─ images/{train,val,test}/*.jpg\n",
        "#      └─ labels/{train,val,test}/*.json\n",
        "\n",
        "# RAW_ROOT = Path('/content/drive/MyDrive/AI활용 소프트웨어 개발/10. 파이썬 웹 서비스/Bee/data/sample_2400') # 드라이브 버전\n",
        "RAW_ROOT = Path('/content/bee_project')\n",
        "\n",
        "\n",
        "# 3) 프로젝트 루트 결정: 1단계 CONFIG가 있으면 그걸, 없으면 기본값\n",
        "CONFIG_PATH = DEFAULT_ROOT / 'project_config.yaml'\n",
        "if CONFIG_PATH.exists():\n",
        "    import yaml\n",
        "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
        "        CFG = yaml.safe_load(f)\n",
        "    ROOT = Path(CFG.get('project_root', str(DEFAULT_ROOT)))\n",
        "else:\n",
        "    ROOT = DEFAULT_ROOT\n",
        "\n",
        "# 4) 출력 폴더들 보장 생성\n",
        "( ROOT / 'yolo_labels' / 'train' ).mkdir(parents=True, exist_ok=True)\n",
        "( ROOT / 'yolo_labels' / 'val'   ).mkdir(parents=True, exist_ok=True)\n",
        "( ROOT / 'yolo_labels' / 'test'  ).mkdir(parents=True, exist_ok=True)\n",
        "( ROOT / 'outputs' / 'reports'   ).mkdir(parents=True, exist_ok=True)\n",
        "( ROOT / 'outputs' / 'viz'       ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('[INFO] PROJECT ROOT =', ROOT)\n",
        "print('[INFO] RAW ROOT     =', RAW_ROOT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns6rHTeWesPt"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [STEP 2-2] 유틸: YOLO 포맷 변환\n",
        "# ============================\n",
        "def _clamp(v, lo, hi):\n",
        "    \"\"\"값 v를 [lo, hi] 범위로 자르기(좌표 안정화)\"\"\"\n",
        "    return max(lo, min(hi, v))\n",
        "\n",
        "def bbox_to_yolo_line(x1, y1, x2, y2, W, H, cls_id=0):\n",
        "    # 1) 좌표 정리\n",
        "    x1, x2 = min(x1, x2), max(x1, x2)\n",
        "    y1, y2 = min(y1, y2), max(y1, y2)\n",
        "\n",
        "    # 2) 이미지 경계로 클램프(음수/초과 방지)\n",
        "    x1 = _clamp(x1, 0, W - 1)\n",
        "    x2 = _clamp(x2, 0, W - 1)\n",
        "    y1 = _clamp(y1, 0, H - 1)\n",
        "    y2 = _clamp(y2, 0, H - 1)\n",
        "\n",
        "    # 3) 중심/크기 계산\n",
        "    cx = (x1 + x2) / 2.0\n",
        "    cy = (y1 + y2) / 2.0\n",
        "    bw = (x2 - x1)\n",
        "    bh = (y2 - y1)\n",
        "\n",
        "    # 4) 극단값 방지\n",
        "    if bw <= 0 or bh <= 0:\n",
        "        return None\n",
        "\n",
        "    # 5) [0,1] 정규화\n",
        "    cx_n = cx / W\n",
        "    cy_n = cy / H\n",
        "    bw_n = bw / W\n",
        "    bh_n = bh / H\n",
        "\n",
        "    # ======== ★ 여기 추가: 미세 오차 제거 (정확한 삽입 위치) ========\n",
        "    import numpy as np\n",
        "    eps = 1e-6\n",
        "    cx_n = float(np.clip(cx_n, eps, 1.0 - eps))\n",
        "    cy_n = float(np.clip(cy_n, eps, 1.0 - eps))\n",
        "    bw_n = float(np.clip(bw_n, eps, 1.0 - eps))\n",
        "    bh_n = float(np.clip(bh_n, eps, 1.0 - eps))\n",
        "    # ============================================================\n",
        "\n",
        "    # 6) YOLO txt 한 줄 문자열\n",
        "    return f\"{cls_id} {cx_n:.6f} {cy_n:.6f} {bw_n:.6f} {bh_n:.6f}\"\n",
        "\n",
        "\n",
        "def read_image_size(img_path, fallback_W=None, fallback_H=None):\n",
        "    \"\"\"\n",
        "    이미지 크기 읽기. 실패 시 폴백 사용 (JSON WIDTH/HEIGHT가 있으면 폴백으로 넣기)\n",
        "    \"\"\"\n",
        "    img = cv2.imread(str(img_path))\n",
        "    if img is not None:\n",
        "        H, W = img.shape[:2]\n",
        "        return W, H\n",
        "    # 이미지 읽기 실패하면 폴백\n",
        "    return fallback_W, fallback_H\n",
        "\n",
        "# ============================\n",
        "# [STEP 2-3] 변환기 본체\n",
        "#  - JSON → YOLO .txt\n",
        "#  - 리포트 CSV 생성\n",
        "#  - 샘플 시각화 저장\n",
        "# ============================\n",
        "def convert_split(split='train', visualize_n=10):\n",
        "    \"\"\"\n",
        "    주어진 split(train/val/test)에 대해:\n",
        "      - labels/*.json 읽어 bbox → yolo_labels/*.txt 저장\n",
        "      - 무결성 리포트 CSV 저장\n",
        "      - 첫 N개 샘플 시각화 이미지 저장\n",
        "    \"\"\"\n",
        "    json_dir = RAW_ROOT / 'labels' / split\n",
        "    img_dir  = RAW_ROOT / 'images' / split\n",
        "    out_dir  = ROOT / 'yolo_labels' / split\n",
        "    viz_dir  = ROOT / 'outputs' / 'viz' / split\n",
        "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 리포트 파일 경로\n",
        "    report_csv = ROOT / 'outputs' / 'reports' / f'yolo_convert_report_{split}.csv'\n",
        "\n",
        "    json_files = sorted(list(json_dir.glob('*.json')))\n",
        "    if len(json_files) == 0:\n",
        "        print(f'[WARN] No JSONs in {json_dir}')\n",
        "        return\n",
        "\n",
        "    rows = []  # 리포트 누적\n",
        "    vcount = 0 # 시각화 저장 개수\n",
        "\n",
        "    for jp in tqdm(json_files, desc=f'[{split}] converting', ncols=80):\n",
        "        # 1) JSON 로드\n",
        "        with open(jp, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # 2) 파일명/이미지 경로 결정\n",
        "        #    - JSON 내부 파일명이 있으면 우선 사용, 없으면 json 스템으로 유추\n",
        "        jname = data.get('IMAGE', {}).get('IMAGE_FILE_NAME', jp.stem + '.jpg')\n",
        "        ipath = img_dir / jname\n",
        "\n",
        "        # 3) 이미지 크기(W,H) 확보: JSON → 실패 시 실제 이미지에서 읽기\n",
        "        W_json = data.get('IMAGE', {}).get('WIDTH', None)\n",
        "        H_json = data.get('IMAGE', {}).get('HEIGHT', None)\n",
        "        W, H   = read_image_size(ipath, fallback_W=W_json, fallback_H=H_json)\n",
        "\n",
        "        # 4) 이미지/치수 유효성 체크\n",
        "        if W is None or H is None:\n",
        "            rows.append([jp.name, jname, 'FAIL', 'image_size_missing', 0])\n",
        "            continue\n",
        "        if not ipath.exists():\n",
        "            rows.append([jp.name, jname, 'FAIL', 'image_missing', 0])\n",
        "            continue\n",
        "\n",
        "        # 5) 어노테이션 목록\n",
        "        anns = data.get('ANNOTATION_INFO', []) or []\n",
        "        yolo_lines = []\n",
        "        valid_boxes = 0\n",
        "\n",
        "        for ann in anns:\n",
        "            # JSON 키: XTL, YTL, XBR, YBR (업로드 예시와 동일)  :contentReference[oaicite:1]{index=1}\n",
        "            x1 = float(ann.get('XTL', 0))\n",
        "            y1 = float(ann.get('YTL', 0))\n",
        "            x2 = float(ann.get('XBR', 0))\n",
        "            y2 = float(ann.get('YBR', 0))\n",
        "\n",
        "            line = bbox_to_yolo_line(x1, y1, x2, y2, W, H, cls_id=0)\n",
        "            if line is None:\n",
        "                continue\n",
        "            yolo_lines.append(line)\n",
        "            valid_boxes += 1\n",
        "\n",
        "        # 6) YOLO 라벨 저장 (박스가 0개여도 빈 파일로 생성하는 쪽이 로더 호환성 ↑)\n",
        "        out_txt = out_dir / f\"{Path(jname).stem}.txt\"\n",
        "        with open(out_txt, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(yolo_lines))\n",
        "\n",
        "        # 7) 리포트 누적\n",
        "        rows.append([jp.name, jname, 'OK', 'saved', valid_boxes])\n",
        "\n",
        "        # 8) 시각화 샘플 저장 (선택)\n",
        "        if vcount < visualize_n:\n",
        "            img = cv2.imread(str(ipath))\n",
        "            if img is not None:\n",
        "                for line in yolo_lines:\n",
        "                    _, cx, cy, bw, bh = line.split()\n",
        "                    cx = float(cx) * W\n",
        "                    cy = float(cy) * H\n",
        "                    bw = float(bw) * W\n",
        "                    bh = float(bh) * H\n",
        "\n",
        "                    x1 = cx - bw/2\n",
        "                    y1 = cy - bh/2\n",
        "                    x2 = cx + bw/2\n",
        "                    y2 = cy + bh/2\n",
        "\n",
        "                    # ======== ★ 여기 추가: 최종 픽셀 경계 clip (정확한 삽입 위치) ========\n",
        "                    x1 = int(max(0, min(W-1, x1)))\n",
        "                    y1 = int(max(0, min(H-1, y1)))\n",
        "                    x2 = int(max(0, min(W-1, x2)))\n",
        "                    y2 = int(max(0, min(H-1, y2)))\n",
        "                    if x2 <= x1 or y2 <= y1:\n",
        "                        continue  # 면적 없는 박스는 스킵\n",
        "                    # ============================================================\n",
        "\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "\n",
        "                out_img = viz_dir / f\"{Path(jname).stem}_viz.jpg\"\n",
        "                cv2.imwrite(str(out_img), img)\n",
        "                vcount += 1\n",
        "\n",
        "\n",
        "    # 9) 리포트 CSV 저장\n",
        "    with open(report_csv, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['json_name','image_name','status','note','num_boxes'])\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    # 10) 결과 요약 출력\n",
        "    ok = sum(1 for r in rows if r[2]=='OK')\n",
        "    fail = len(rows) - ok\n",
        "    total_boxes = sum(int(r[4]) for r in rows)\n",
        "    print(f'[SUMMARY][{split}] files: {len(rows)} | OK: {ok} | FAIL: {fail} | boxes: {total_boxes}')\n",
        "    print(f'[REPORT] {report_csv}')\n",
        "    print(f'[VIZ]    {viz_dir} (saved {vcount} preview images)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y237pJMzWxjI"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [STEP 2-4] 변환 실행 (train/val/test)\n",
        "# ============================\n",
        "for sp in ['train', 'val', 'test']:\n",
        "    convert_split(sp, visualize_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6faGfKvXVEj"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [STEP 1-4] 프로젝트 설정(config) 저장\n",
        "#  - YOLOv3 탐지: 단일 클래스 'bee' (id=0)\n",
        "#  - 분류(ResNet-18): 클래스 이름은 추후 확정/수정 가능 (임시 예시)\n",
        "#  - 입력 해상도, 학습 하이퍼파라미터를 한 곳에서 관리\n",
        "# ============================\n",
        "import yaml                                                          # YAML 포맷 저장\n",
        "\n",
        "config = {\n",
        "    'project_root': str(ROOT),                                       # 프로젝트 루트 경로\n",
        "    'random_seed': SEED,                                             # 고정 시드 값\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',        # 디바이스 기본값\n",
        "    'detection': {                                                   # 탐지(YOLOv3) 설정\n",
        "        'classes': ['bee'],                                          # 단일 클래스: bee\n",
        "        'num_classes': 1,                                            # 클래스 수\n",
        "        'img_size': 416,                                             # 입력 해상도(권장 640, 416도 가능)\n",
        "        'epochs': 30,                                               # 학습 에폭\n",
        "        'batch_size': 16,                                            # 배치 크기 (A100이면 16~32 권장)\n",
        "        'learning_rate': 1e-3,                                       # 초기 학습률(코사인/워밍업과 함께 조정)\n",
        "        'weight_decay': 5e-4,                                        # 가중치 감쇠\n",
        "        'warmup_epochs': 3,                                          # 워밍업 에폭\n",
        "        'amp': True,                                                 # AMP(Mixed Precision) 사용\n",
        "        'train_images': str(ROOT / 'images' / 'train'),              # 학습 이미지 경로\n",
        "        'val_images':   str(ROOT / 'images' / 'val'),                # 검증 이미지 경로\n",
        "        'train_labels': str(ROOT / 'yolo_labels' / 'train'),         # YOLO txt 라벨(train)\n",
        "        'val_labels':   str(ROOT / 'yolo_labels' / 'val'),           # YOLO txt 라벨(val)\n",
        "        'optimizer': 'sgd',                                          # 'sgd' 또는 'adamw' 등\n",
        "        'conf_threshold': 0.6,                                      # 추론 시 confidence 임계값\n",
        "        'iou_threshold': 0.4,                                        # NMS IoU 임계값\n",
        "        'box_margin_ratio': 0.08,                                    # 크롭 시 여백 비율(8%)\n",
        "        'augment': {                                                 # 탐지용 증강(필요시 2단계에서 조정)\n",
        "            'hflip': True,                                           # 좌우 뒤집기\n",
        "            'hsv': True,                                             # 색상(H,S,V) 변형\n",
        "            'mosaic': False                                          # Mosaic는 안정성상 기본 False (원하면 True로)\n",
        "        }\n",
        "    },\n",
        "    'classification': {                                              # 분류(ResNet-18) 설정\n",
        "        # 아래 클래스 이름은 임시 예시입니다. 2단계에서 확정 후 수정 예정\n",
        "        'classes': [\n",
        "            '수일벌-이탈리안','수일벌-카니올란','수일벌-한봉','수일벌-호박벌',\n",
        "            '여왕벌-이탈리안','여왕벌-카니올란','여왕벌-한봉','여왕벌-호박벌'\n",
        "        ],\n",
        "        'img_size': 224,                                             # ResNet-18 입력 크기\n",
        "        'epochs': 20,                                                # 분류 학습 에폭\n",
        "        'batch_size': 64,                                            # 분류 배치 (A100이면 넉넉)\n",
        "        'learning_rate': 1e-3,                                       # 초기 학습률\n",
        "        'weight_decay': 1e-4,                                        # 가중치 감쇠\n",
        "        'amp': True,                                                 # AMP 사용\n",
        "        'augment': {                                                 # 분류용 증강\n",
        "            'randaugment': True,                                     # 강한 증강(필요시 강도 조절)\n",
        "            'color_jitter': True,                                    # 색상 조절\n",
        "            'hflip': True                                            # 좌우 뒤집기\n",
        "        }\n",
        "    },\n",
        "    'paths': {                                                       # 자주 쓰는 경로 모음\n",
        "        'json_labels_train': str(ROOT / 'labels' / 'train'),         # 원본 JSON 라벨(train)\n",
        "        'json_labels_val':   str(ROOT / 'labels' / 'val'),           # 원본 JSON 라벨(val)\n",
        "        'json_labels_test':  str(ROOT / 'labels' / 'test'),          # 원본 JSON 라벨(test)\n",
        "        'yolo_labels_train': str(ROOT / 'yolo_labels' / 'train'),    # 변환된 YOLO 라벨(train)\n",
        "        'yolo_labels_val':   str(ROOT / 'yolo_labels' / 'val'),      # 변환된 YOLO 라벨(val)\n",
        "        'crops_train':       str(ROOT / 'crops' / 'train'),          # 크롭 이미지(train)\n",
        "        'crops_val':         str(ROOT / 'crops' / 'val'),            # 크롭 이미지(val)\n",
        "        'weights':           str(ROOT / 'outputs' / 'weights'),      # 모델 가중치 저장\n",
        "        'reports':           str(ROOT / 'outputs' / 'reports'),      # CSV/리포트\n",
        "        'viz':               str(ROOT / 'outputs' / 'viz')           # 시각화 결과\n",
        "    }\n",
        "}\n",
        "\n",
        "# 설정을 YAML로 저장 (나중에 로더가 이 파일만 읽으면 전체 파이프라인 재현 가능)\n",
        "CONFIG_PATH = ROOT / 'project_config.yaml'                           # 설정 파일 경로\n",
        "with open(CONFIG_PATH, 'w', encoding='utf-8') as f:                  # 파일 열기(UTF-8)\n",
        "    yaml.safe_dump(config, f, allow_unicode=True, sort_keys=False)   # 한글 보존하며 저장\n",
        "print('[INFO] Config saved to:', CONFIG_PATH)                         # 저장 경로 출력\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alVtk9-2YPDs"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [STEP 1-5] 유틸 함수: 설정 로드 & AMP 스칼라 생성\n",
        "# ============================\n",
        "def load_config(cfg_path: Path) -> dict:\n",
        "    \"\"\"YAML 설정 파일을 읽어 dict로 반환\"\"\"\n",
        "    with open(cfg_path, 'r', encoding='utf-8') as f:   # 설정 파일 열기\n",
        "        data = yaml.safe_load(f)                       # YAML -> dict 변환\n",
        "    return data                                        # 설정 딕셔너리 반환\n",
        "\n",
        "def build_amp_scaler(enabled: bool = True) -> GradScaler:\n",
        "    \"\"\"AMP 사용 플래그에 따라 GradScaler 생성\"\"\"\n",
        "    return GradScaler(enabled=enabled)                 # enabled=False면 FP32로 동작\n",
        "\n",
        "# 설정 로드/프린트 테스트\n",
        "CFG = load_config(CONFIG_PATH)                         # 방금 저장한 설정 불러오기\n",
        "print('[INFO] Loaded config keys:', list(CFG.keys()))  # 최상위 키 확인\n",
        "print('[INFO] Detection img_size =', CFG['detection']['img_size'])  # 예시 출력\n",
        "SCALER_DET = build_amp_scaler(CFG['detection']['amp']) # 탐지용 AMP 스칼라\n",
        "SCALER_CLS = build_amp_scaler(CFG['classification']['amp']) # 분류용 AMP 스칼라\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 충돌 가능성 있는 패키지 제거\n",
        "!pip uninstall -y albumentations albucore opencv-python opencv-python-headless numpy\n",
        "\n",
        "# 2) 호환성 검증된 조합으로 재설치\n",
        "#  - albumentations 1.4.16 : albucore 사용 버전 (HF 이슈 스레드에서 OK 보고)\n",
        "#  - albucore 0.0.16       : preserve_channel_dim 제공\n",
        "#  - numpy 1.26.4           : PyTorch/Colab와 안정 조합\n",
        "#  - opencv 4.9.0.80        : 알부/넘파이와 궁합 양호\n",
        "!pip install albumentations==1.4.16 albucore==0.0.16 numpy==1.26.4 opencv-python==4.9.0.80\n"
      ],
      "metadata": {
        "id": "Xl7E1YaF0uRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (이미 언인스톨했으니) 호환 버전 '한 번에' 설치\n",
        "!pip install --no-cache-dir \\\n",
        "  numpy==1.26.4 \\\n",
        "  albucore==0.0.17 \\\n",
        "  albumentations==1.4.16 \\\n",
        "  opencv-python-headless==4.9.0.80\n"
      ],
      "metadata": {
        "id": "1YKwxorv06ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_-_kLNfYQvC"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# [STEP 3-1] YOLO 데이터로더 (설치 없이 동작하는 버전)\n",
        "# ============================================\n",
        "import os, glob, math, random, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# OpenCV 대신 PIL 사용 (기본 설치됨)\n",
        "try:\n",
        "    import cv2\n",
        "    USE_CV2 = True\n",
        "except ImportError:\n",
        "    from PIL import Image\n",
        "    USE_CV2 = False\n",
        "    print(\"[INFO] OpenCV not found, using PIL instead\")\n",
        "\n",
        "# 설정 파일 로드 (yaml 없이 처리)\n",
        "try:\n",
        "    import yaml\n",
        "    CFG_PATH = Path('/content/bee_project/project_config.yaml')\n",
        "    if CFG_PATH.exists():\n",
        "        with open(CFG_PATH, 'r', encoding='utf-8') as f:\n",
        "            CFG = yaml.safe_load(f)\n",
        "    else:\n",
        "        CFG = None\n",
        "except ImportError:\n",
        "    print(\"[INFO] PyYAML not found, using default config\")\n",
        "    CFG = None\n",
        "\n",
        "# 기본 설정\n",
        "if CFG is None:\n",
        "    CFG = {\n",
        "        'detection': {\n",
        "            'img_size': 416, 'batch_size': 16, 'amp': True,\n",
        "            'train_images': '/content/bee_project/images/train',\n",
        "            'val_images':   '/content/bee_project/images/val',\n",
        "            'train_labels': '/content/bee_project/yolo_labels/train',\n",
        "            'val_labels':   '/content/bee_project/yolo_labels/val',\n",
        "        }\n",
        "    }\n",
        "\n",
        "# 경로/파라미터 가져오기\n",
        "DET = CFG['detection']\n",
        "IMG_SIZE = int(DET['img_size'])\n",
        "TRAIN_IMG_DIR = Path('/content/bee_project/images/train')\n",
        "VAL_IMG_DIR   = Path('/content/bee_project/images/val')\n",
        "TRAIN_LBL_DIR = Path('/content/bee_project/yolo_labels/train')\n",
        "VAL_LBL_DIR   = Path('/content/bee_project/yolo_labels/val')\n",
        "\n",
        "# ---------------- 이미지 변환 함수들 (Albumentations 대체) ----------------\n",
        "def resize_with_padding(image, target_size, fill_value=114):\n",
        "    \"\"\"이미지를 비율 유지하며 리사이즈하고 패딩 추가\"\"\"\n",
        "    if USE_CV2:\n",
        "        h, w = image.shape[:2]\n",
        "        scale = min(target_size / h, target_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # 리사이즈\n",
        "        resized = cv2.resize(image, (new_w, new_h))\n",
        "\n",
        "        # 패딩 계산\n",
        "        pad_h = target_size - new_h\n",
        "        pad_w = target_size - new_w\n",
        "        top = pad_h // 2\n",
        "        bottom = pad_h - top\n",
        "        left = pad_w // 2\n",
        "        right = pad_w - left\n",
        "\n",
        "        # 패딩 적용\n",
        "        padded = cv2.copyMakeBorder(resized, top, bottom, left, right,\n",
        "                                   cv2.BORDER_CONSTANT, value=(fill_value, fill_value, fill_value))\n",
        "        return padded, scale, (left, top)\n",
        "    else:\n",
        "        # PIL 버전\n",
        "        w, h = image.size\n",
        "        scale = min(target_size / h, target_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # 리사이즈\n",
        "        resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # 새 이미지 생성 (패딩)\n",
        "        new_image = Image.new('RGB', (target_size, target_size), (fill_value, fill_value, fill_value))\n",
        "\n",
        "        # 패딩 계산\n",
        "        pad_h = target_size - new_h\n",
        "        pad_w = target_size - new_w\n",
        "        top = pad_h // 2\n",
        "        left = pad_w // 2\n",
        "\n",
        "        # 붙여넣기\n",
        "        new_image.paste(resized, (left, top))\n",
        "\n",
        "        return np.array(new_image), scale, (left, top)\n",
        "\n",
        "def transform_bboxes(bboxes, scale, pad_offset):\n",
        "    \"\"\"바운딩 박스를 변환된 이미지에 맞게 조정\"\"\"\n",
        "    if len(bboxes) == 0:\n",
        "        return bboxes\n",
        "\n",
        "    transformed = []\n",
        "    pad_x, pad_y = pad_offset\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        # 스케일 적용\n",
        "        x1 *= scale\n",
        "        y1 *= scale\n",
        "        x2 *= scale\n",
        "        y2 *= scale\n",
        "        # 패딩 오프셋 적용\n",
        "        x1 += pad_x\n",
        "        y1 += pad_y\n",
        "        x2 += pad_x\n",
        "        y2 += pad_y\n",
        "        transformed.append([x1, y1, x2, y2])\n",
        "\n",
        "    return transformed\n",
        "\n",
        "# ---------------- 좌표 변환/클립 유틸 ----------------\n",
        "EPS = 1e-6\n",
        "\n",
        "def _clip_yolo_norm(boxes, eps=EPS, min_wh=EPS):\n",
        "    \"\"\"\n",
        "    YOLO 정규화 [[cx,cy,w,h], ...]를 [eps,1-eps]로 강제 + 최소 w/h 보장.\n",
        "    (미세 음수/초과를 전부 흡수)\n",
        "    \"\"\"\n",
        "    out=[]\n",
        "    for cx, cy, w, h in boxes:\n",
        "        cx = float(min(1.0 - eps, max(eps, float(cx))))\n",
        "        cy = float(min(1.0 - eps, max(eps, float(cy))))\n",
        "        w  = float(min(1.0 - eps, max(min_wh, float(w))))\n",
        "        h  = float(min(1.0 - eps, max(min_wh, float(h))))\n",
        "        out.append([cx, cy, w, h])\n",
        "    return out\n",
        "\n",
        "def yolo_norm_to_voc_abs(boxes_yolo, W, H):\n",
        "    \"\"\"[cx,cy,w,h] (0~1) -> [x1,y1,x2,y2] (pixels)\"\"\"\n",
        "    out = []\n",
        "    for cx, cy, w, h in boxes_yolo:\n",
        "        bw, bh = float(w)*W, float(h)*H\n",
        "        x1 = float(cx)*W - bw/2.0\n",
        "        y1 = float(cy)*H - bh/2.0\n",
        "        x2 = x1 + bw\n",
        "        y2 = y1 + bh\n",
        "        out.append([x1, y1, x2, y2])\n",
        "    return out\n",
        "\n",
        "def _clip_voc_abs(boxes, W, H, eps=EPS):\n",
        "    \"\"\"\n",
        "    픽셀 절대좌표를 이미지 경계 [0,W-1],[0,H-1]로 교차-클립.\n",
        "    무면적(<=eps)은 제거.\n",
        "    \"\"\"\n",
        "    out=[]\n",
        "    for x1,y1,x2,y2 in boxes:\n",
        "        x1 = float(np.clip(x1, 0, W-1)); y1 = float(np.clip(y1, 0, H-1))\n",
        "        x2 = float(np.clip(x2, 0, W-1)); y2 = float(np.clip(y2, 0, H-1))\n",
        "        if x2 <= x1 + eps or y2 <= y1 + eps:\n",
        "            continue\n",
        "        out.append([x1,y1,x2,y2])\n",
        "    return out\n",
        "\n",
        "def voc_abs_to_yolo_norm(boxes_voc, W, H, eps=EPS):\n",
        "    \"\"\"[x1,y1,x2,y2] (pixels) -> [cx,cy,w,h] (0~1) with final eps-clip\"\"\"\n",
        "    out = []\n",
        "    for x1, y1, x2, y2 in boxes_voc:\n",
        "        # 경계 clip + 면적 0 방지 (재확인)\n",
        "        x1 = float(np.clip(x1, 0, W-1)); y1 = float(np.clip(y1, 0, H-1))\n",
        "        x2 = float(np.clip(x2, 0, W-1)); y2 = float(np.clip(y2, 0, H-1))\n",
        "        if x2 <= x1 + eps or y2 <= y1 + eps:\n",
        "            continue\n",
        "        bw = x2 - x1; bh = y2 - y1\n",
        "        cx = x1 + bw/2.0; cy = y1 + bh/2.0\n",
        "        cx_n = cx / W; cy_n = cy / H; bw_n = bw / W; bh_n = bh / H\n",
        "        # 마지막 한 번 더 클립 (부동소수점 마감)\n",
        "        cx_n = float(np.clip(cx_n, eps, 1.0 - eps))\n",
        "        cy_n = float(np.clip(cy_n, eps, 1.0 - eps))\n",
        "        bw_n = float(np.clip(bw_n, eps, 1.0 - eps))\n",
        "        bh_n = float(np.clip(bh_n, eps, 1.0 - eps))\n",
        "        out.append([cx_n, cy_n, bw_n, bh_n])\n",
        "    return out\n",
        "\n",
        "# ---------------- Dataset ----------------\n",
        "class YoloDetectionDataset(Dataset):\n",
        "    \"\"\"YOLO txt 라벨을 읽어 오는 커스텀 데이터셋 (음수좌표 방지 패치 포함)\"\"\"\n",
        "    def __init__(self, img_dir: Path, lbl_dir: Path, transforms=None, num_classes=1, debug=False):\n",
        "        self.img_paths = sorted([p for p in img_dir.glob('*.jpg')] + [p for p in img_dir.glob('*.png')])\n",
        "        self.lbl_dir   = lbl_dir\n",
        "        self.transforms = transforms\n",
        "        self.num_classes = num_classes\n",
        "        self.debug_print_count = 0\n",
        "        self.debug = debug  # 디버그 모드 제어\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def _read_yolo_txt(self, txt_path: Path):\n",
        "        boxes, labels = [], []\n",
        "        if not txt_path.exists():\n",
        "            return boxes, labels\n",
        "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "            for ln in f.read().strip().splitlines():\n",
        "                if not ln.strip():\n",
        "                    continue\n",
        "                parts = ln.split()\n",
        "                if len(parts) < 5:\n",
        "                    continue\n",
        "                cls = int(float(parts[0]))\n",
        "                cx, cy, w, h = map(float, parts[1:5])\n",
        "                boxes.append([cx, cy, w, h])\n",
        "                labels.append(cls)\n",
        "        return boxes, labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "\n",
        "        # 이미지 로드\n",
        "        if USE_CV2:\n",
        "            img_bgr = cv2.imread(str(img_path))\n",
        "            assert img_bgr is not None, f'이미지 로드 실패: {img_path}'\n",
        "            img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            img_pil = Image.open(str(img_path)).convert('RGB')\n",
        "            img = np.array(img_pil)\n",
        "\n",
        "        H0, W0 = img.shape[:2]\n",
        "\n",
        "        txt_path = self.lbl_dir / f'{img_path.stem}.txt'\n",
        "        boxes_yolo, labels = self._read_yolo_txt(txt_path)\n",
        "\n",
        "        # (A) YOLO 정규화 좌표를 먼저 eps-clip (미세 음수/초과 흡수)\n",
        "        boxes_yolo = _clip_yolo_norm(boxes_yolo, eps=EPS, min_wh=EPS)\n",
        "\n",
        "        # (B) YOLO -> VOC(픽셀) 변환\n",
        "        boxes_voc_abs = yolo_norm_to_voc_abs(boxes_yolo, W0, H0)\n",
        "\n",
        "        # (C) 변환 '직전' 픽셀 좌표 교차-클립\n",
        "        boxes_voc_abs = _clip_voc_abs(boxes_voc_abs, W0, H0, eps=EPS)\n",
        "\n",
        "        # 이미지 변환 (Albumentations 대체)\n",
        "        transformed_img, scale, pad_offset = resize_with_padding(img, IMG_SIZE)\n",
        "\n",
        "        # 바운딩 박스 변환\n",
        "        if len(boxes_voc_abs) > 0:\n",
        "            bxs_voc = transform_bboxes(boxes_voc_abs, scale, pad_offset)\n",
        "            clss = labels[:len(bxs_voc)]  # 길이 맞추기\n",
        "        else:\n",
        "            bxs_voc = []\n",
        "            clss = []\n",
        "\n",
        "        # 이미지를 텐서로 변환\n",
        "        if USE_CV2:\n",
        "            timg = torch.from_numpy(transformed_img).permute(2, 0, 1).float() / 255.0\n",
        "        else:\n",
        "            timg = torch.from_numpy(transformed_img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # (D) 변환 '직후' 픽셀 좌표 재-클립\n",
        "        H1, W1 = timg.shape[1], timg.shape[2]\n",
        "        bxs_voc = _clip_voc_abs(bxs_voc, W1, H1, eps=EPS)\n",
        "\n",
        "        # (E) 최종: VOC(픽셀) -> YOLO(정규화) with eps-clip\n",
        "        boxes_yolo_final = voc_abs_to_yolo_norm(bxs_voc, W1, H1, eps=EPS)\n",
        "\n",
        "        # (F) 텐서 구성 (라벨 길이와 매칭)\n",
        "        if len(boxes_yolo_final) > 0:\n",
        "            # clss 길이가 박스보다 길 수 있으니 zip으로 안전 매칭\n",
        "            targets = torch.tensor([list(b)+[c] for b, c in zip(boxes_yolo_final, clss)], dtype=torch.float32)\n",
        "        else:\n",
        "            targets = torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "        # 디버그 출력 제거 (깔끔한 학습을 위해)\n",
        "        if self.debug and self.debug_print_count < 3:\n",
        "            print(f\"[DEBUG] {img_path.name}  H0xW0={H0}x{W0} -> H1xW1={H1}x{W1}\")\n",
        "            print(\"  yolo_in :\", boxes_yolo[:3])\n",
        "            print(\"  voc_pre :\", boxes_voc_abs[:3])\n",
        "            print(\"  voc_post:\", bxs_voc[:3])\n",
        "            print(\"  yolo_out:\", boxes_yolo_final[:3])\n",
        "            self.debug_print_count += 1\n",
        "\n",
        "        return timg, targets, img_path.name\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets, names = [], [], []\n",
        "    for i, (img, t, name) in enumerate(batch):\n",
        "        imgs.append(img)\n",
        "        if t.numel() > 0:\n",
        "            ti = torch.zeros((t.size(0), 6), dtype=torch.float32)\n",
        "            ti[:,0] = i\n",
        "            ti[:,1:] = t\n",
        "            targets.append(ti)\n",
        "        names.append(name)\n",
        "    targets = torch.cat(targets, dim=0) if len(targets)>0 else torch.zeros((0,6), dtype=torch.float32)\n",
        "    return torch.stack(imgs, 0), targets, names\n",
        "\n",
        "# ---------------- DataLoader ----------------\n",
        "train_ds = YoloDetectionDataset(TRAIN_IMG_DIR, TRAIN_LBL_DIR, num_classes=1, debug=True)   # 훈련 시에만 디버그\n",
        "val_ds   = YoloDetectionDataset(VAL_IMG_DIR,   VAL_LBL_DIR,   num_classes=1, debug=False)  # validation 시 디버그 끄기\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=int(DET['batch_size']), shuffle=True,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_fn, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=int(DET['batch_size']), shuffle=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_fn, drop_last=False)\n",
        "\n",
        "print('[INFO] train samples:', len(train_ds), '| val samples:', len(val_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u7eN1Tof1yk"
      },
      "outputs": [],
      "source": [
        "imgs, targets, names = next(iter(train_loader))\n",
        "assert imgs.dim() == 4 and imgs.shape[1] == 3\n",
        "if targets.numel() > 0:\n",
        "    assert torch.all((targets[:,1:5] > 0) & (targets[:,1:5] < 1)), \"좌표 0~1 범위를 벗어났습니다.\"\n",
        "print(\"OK: dataloader pipeline (pascal_voc → yolo) works.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adNlMjMTYSG8"
      },
      "outputs": [],
      "source": [
        "print(\"IMG DIRS:\", TRAIN_IMG_DIR, VAL_IMG_DIR)\n",
        "print(\"LBL DIRS:\", TRAIN_LBL_DIR, VAL_LBL_DIR)\n",
        "print(\"EXIST?  :\", TRAIN_IMG_DIR.exists(), VAL_IMG_DIR.exists(), TRAIN_LBL_DIR.exists(), VAL_LBL_DIR.exists())\n",
        "\n",
        "def count_imgs(root: Path):\n",
        "    exts = ['*.jpg','*.jpeg','*.png','*.JPG','*.JPEG','*.PNG']\n",
        "    return sum(len(list(root.rglob(e))) for e in exts)\n",
        "\n",
        "print(\"Found(train imgs):\", count_imgs(TRAIN_IMG_DIR))\n",
        "print(\"Found(val   imgs):\", count_imgs(VAL_IMG_DIR))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QebtLymxYTtE"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# [STEP 3-2] YOLOv3 네트워크 (Darknet-53 백본 + 3-스케일 헤드)\n",
        "#  - 출력: 3개 스케일의 예측 텐서 [B, A*(5+C), S, S]\n",
        "#  - 앵커: YOLOv3 COCO 기본 앵커 사용(스케일별 3개), stride로 자동 스케일링\n",
        "# ============================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Tuple\n",
        "\n",
        "# 기본 Conv-BN-Leaky 블록 정의\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, act=True):\n",
        "        super().__init__()                                                                                # 부모 초기화\n",
        "        p = k // 2 if p is None else p                                                                    # 패딩 자동값\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, p, bias=False)                                                # 컨볼루션\n",
        "        self.bn   = nn.BatchNorm2d(c2)                                                                    # 배치정규화\n",
        "        self.act  = nn.LeakyReLU(0.1, inplace=True) if act else nn.Identity()                             # 활성화\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))                                                            # 순전파\n",
        "\n",
        "# 다크넷 Residual 블록 (1x1 -> 3x3)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super().__init__()                                                                                # 부모 초기화\n",
        "        self.cv1 = Conv(c, c//2, k=1, s=1)                                                                # 1x1 축소\n",
        "        self.cv2 = Conv(c//2, c, k=3, s=1)                                                                # 3x3 확장\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x))                                                                  # 스킵연결\n",
        "\n",
        "# Darknet-53 백본 구성\n",
        "class Darknet53(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()                                                                                # 부모 초기화\n",
        "        self.cv1 = Conv(3, 32, 3, 1)                                                                      # stem conv\n",
        "        self.cv2 = Conv(32, 64, 3, 2)                                                                     # downsample\n",
        "        self.res1 = nn.Sequential(*[ResBlock(64) for _ in range(1)])                                      # 1 residual\n",
        "        self.cv3 = Conv(64, 128, 3, 2)                                                                    # downsample\n",
        "        self.res2 = nn.Sequential(*[ResBlock(128) for _ in range(2)])                                     # 2 residual\n",
        "        self.cv4 = Conv(128, 256, 3, 2)                                                                   # downsample\n",
        "        self.res3 = nn.Sequential(*[ResBlock(256) for _ in range(8)])                                     # 8 residual\n",
        "        self.cv5 = Conv(256, 512, 3, 2)                                                                   # downsample\n",
        "        self.res4 = nn.Sequential(*[ResBlock(512) for _ in range(8)])                                     # 8 residual\n",
        "        self.cv6 = Conv(512, 1024, 3, 2)                                                                  # downsample\n",
        "        self.res5 = nn.Sequential(*[ResBlock(1024) for _ in range(4)])                                    # 4 residual\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)                                                                                   # 32x\n",
        "        x = self.cv2(x) ; x = self.res1(x)                                                                # 64x\n",
        "        x = self.cv3(x) ; x = self.res2(x)                                                                # 128x\n",
        "        x = self.cv4(x) ; x3 = self.res3(x)                                                               # 256x (route)\n",
        "        x = self.cv5(x) ; x2 = self.res4(x)                                                               # 512x (route)\n",
        "        x = self.cv6(x) ; x1 = self.res5(x)                                                               # 1024x (top)\n",
        "        return x1, x2, x3                                                                                 # 3 스케일 피처 반환\n",
        "\n",
        "# YOLOv3 Neck/Head(3-스케일 예측) 구성\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, num_classes=1, anchors=None, img_size=416):\n",
        "        super().__init__()                                                                                # 부모 초기화\n",
        "        self.nc = num_classes                                                                             # 클래스 수\n",
        "        self.na = 3                                                                                        # 스케일당 앵커 수\n",
        "        # COCO 기본 앵커 (픽셀 기준) - YOLOv3 논문 설정\n",
        "        self.anchors = anchors or [                                                                       # 3스케일*3개\n",
        "            [(116,90), (156,198), (373,326)],    # stride 32\n",
        "            [(30,61),  (62,45),   (59,119)],     # stride 16\n",
        "            [(10,13),  (16,30),   (33,23)]       # stride 8\n",
        "        ]\n",
        "        self.img_size = img_size                                                                          # 입력 해상도\n",
        "        # 백본 생성\n",
        "        self.backbone = Darknet53()                                                                        # Darknet-53\n",
        "        # 헤드 1 (최상단: 1024 → 512 → 1024 → pred)\n",
        "        self.head1 = nn.Sequential(Conv(1024, 512, 1, 1), Conv(512, 1024, 3, 1),\n",
        "                                   Conv(1024, 512, 1, 1), Conv(512, 1024, 3, 1),\n",
        "                                   Conv(1024, 512, 1, 1))                                                 # 출력 512\n",
        "        self.pred1 = nn.Conv2d(512, self.na*(5+self.nc), 1, 1, 0)                                         # 예측 conv\n",
        "        # 업샘플 + 합치기 (512→256)\n",
        "        self.up1   = nn.Upsample(scale_factor=2, mode='nearest')                                          # 2배 업샘플\n",
        "        self.reduce1 = Conv(512, 256, 1, 1)                                                               # 채널 축소\n",
        "        # 헤드 2 (합쳐진 피처: 256 + 512 = 768 → ... → pred)\n",
        "        self.head2 = nn.Sequential(Conv(768, 256, 1, 1), Conv(256, 512, 3, 1),\n",
        "                                   Conv(512, 256, 1, 1), Conv(256, 512, 3, 1),\n",
        "                                   Conv(512, 256, 1, 1))                                                  # 출력 256\n",
        "        self.pred2 = nn.Conv2d(256, self.na*(5+self.nc), 1, 1, 0)                                         # 예측 conv\n",
        "        # 또 업샘플 + 합치기 (256→128)\n",
        "        self.up2   = nn.Upsample(scale_factor=2, mode='nearest')                                          # 2배 업샘플\n",
        "        self.reduce2 = Conv(256, 128, 1, 1)                                                               # 채널 축소\n",
        "        # 헤드 3 (합쳐진 피처: 128 + 256 = 384 → ... → pred)\n",
        "        self.head3 = nn.Sequential(Conv(384, 128, 1, 1), Conv(128, 256, 3, 1),\n",
        "                                   Conv(256, 128, 1, 1), Conv(128, 256, 3, 1),\n",
        "                                   Conv(256, 128, 1, 1))                                                  # 출력 128\n",
        "        self.pred3 = nn.Conv2d(128, self.na*(5+self.nc), 1, 1, 0)                                         # 예측 conv\n",
        "        # 스트라이드 값 (입력크기/출력 맵 크기) - forward에서 동적으로 계산\n",
        "        self.strides = None                                                                                # 나중 설정\n",
        "    def _make_grid(self, nx, ny, device):\n",
        "        \"\"\"셀 좌표 그리드 생성 (각 위치의 (x,y) 인덱스)\"\"\"\n",
        "        yv, xv = torch.meshgrid(torch.arange(ny, device=device), torch.arange(nx, device=device), indexing='ij')  # 그리드\n",
        "        return xv, yv                                                                                     # x,y 그리드\n",
        "    def forward(self, x):\n",
        "        x1, x2, x3 = self.backbone(x)                                                                     # 백본 출력\n",
        "        p1 = self.head1(x1)                                                                               # 헤드1\n",
        "        out1 = self.pred1(p1)                                                                             # 예측1 (stride 32)\n",
        "        u1 = self.up1(self.reduce1(p1))                                                                   # 512→256 업샘플\n",
        "        f2 = torch.cat([u1, x2], dim=1)                                                                   # 상하위 피처 결합\n",
        "        p2 = self.head2(f2)                                                                               # 헤드2\n",
        "        out2 = self.pred2(p2)                                                                             # 예측2 (stride 16)\n",
        "        u2 = self.up2(self.reduce2(p2))                                                                   # 256→128 업샘플\n",
        "        f3 = torch.cat([u2, x3], dim=1)                                                                   # 결합\n",
        "        p3 = self.head3(f3)                                                                               # 헤드3\n",
        "        out3 = self.pred3(p3)                                                                             # 예측3 (stride 8)\n",
        "        return [out1, out2, out3]                                                                         # 3 스케일 반환\n",
        "\n",
        "# 디코더 유틸 (raw pred -> 박스/obj/cls)\n",
        "def yolo_decode(outputs: List[torch.Tensor], img_size: int, num_classes: int,\n",
        "                anchors: List[List[Tuple[int,int]]]):\n",
        "    \"\"\"3-스케일 raw 출력을 디코딩해 [B, N, 4], [B, N], [B, N, C]로 반환\"\"\"\n",
        "    decoded = []                                                                                          # 스케일별 결과\n",
        "    device = outputs[0].device                                                                            # 디바이스\n",
        "    strides = [32, 16, 8]                                                                                 # YOLOv3 표준 스트라이드\n",
        "    for i, out in enumerate(outputs):                                                                     # 각 스케일 순회\n",
        "        bs, ch, ny, nx = out.shape                                                                        # 배치/채널/크기\n",
        "        na = 3                                                                                            # 앵커 수\n",
        "        no = 5 + num_classes                                                                              # (x,y,w,h,obj+cls)\n",
        "        out = out.view(bs, na, no, ny, nx).permute(0,1,3,4,2).contiguous()                                # [B,A,ny,nx,no]\n",
        "        # 앵커/그리드/스트라이드 설정\n",
        "        stride = strides[i]                                                                               # 현재 스케일 stride\n",
        "        anc = torch.tensor(anchors[i], device=device).float() / stride                                    # 앵커를 셀 단위로\n",
        "        xv, yv = torch.meshgrid(torch.arange(nx, device=device), torch.arange(ny, device=device), indexing='xy')  # 그리드\n",
        "        # 시그모이드/지수 적용하여 실제 좌표로 변환\n",
        "        x = (out[..., 0].sigmoid() + xv) * stride                                                         # cx 픽셀\n",
        "        y = (out[..., 1].sigmoid() + yv) * stride                                                         # cy 픽셀\n",
        "        w = (out[..., 2].exp() * anc[:,0].view(na,1,1)) * stride                                          # w  픽셀\n",
        "        h = (out[..., 3].exp() * anc[:,1].view(na,1,1)) * stride                                          # h  픽셀\n",
        "        obj = out[..., 4].sigmoid()                                                                       # objectness\n",
        "        cls = out[..., 5:].sigmoid()                                                                      # class prob\n",
        "        # [B, A*ny*nx, ...] 형태로 펴기\n",
        "        boxes = torch.stack([x - w/2, y - h/2, x + w/2, y + h/2], dim=-1)                                 # x1y1x2y2\n",
        "        boxes = boxes.view(bs, -1, 4)                                                                     # 박스 전개\n",
        "        obj   = obj.view(bs, -1)                                                                          # obj 전개\n",
        "        cls   = cls.view(bs, -1, num_classes)                                                             # cls 전개\n",
        "        decoded.append((boxes, obj, cls))                                                                 # 누적\n",
        "    # 스케일 합치기\n",
        "    boxes = torch.cat([d[0] for d in decoded], dim=1)                                                     # 박스 합침\n",
        "    obj   = torch.cat([d[1] for d in decoded], dim=1)                                                     # obj 합침\n",
        "    cls   = torch.cat([d[2] for d in decoded], dim=1)                                                     # cls 합침\n",
        "    return boxes, obj, cls                                                                                # 최종 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CQHTKYfTaDw"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [K-means 앵커 계산] - 꿀벌 데이터셋 최적화\n",
        "# ============================\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def iou_width_height(boxes1, boxes2):\n",
        "    \"\"\"너비와 높이만으로 IoU 계산\"\"\"\n",
        "    boxes1 = boxes1[:, np.newaxis, :]\n",
        "    boxes2 = boxes2[np.newaxis, :, :]\n",
        "    intersection = np.minimum(boxes1, boxes2).prod(axis=2)\n",
        "    area1 = boxes1.prod(axis=2)\n",
        "    area2 = boxes2.prod(axis=2)\n",
        "    union = area1 + area2 - intersection\n",
        "    return intersection / (union + 1e-16)\n",
        "\n",
        "def kmeans_anchors(boxes, k=9, max_iter=300):\n",
        "    \"\"\"K-means clustering for anchor boxes\"\"\"\n",
        "    n = boxes.shape[0]\n",
        "    centers = boxes[np.random.choice(n, 1)]\n",
        "\n",
        "    # K-means++ initialization\n",
        "    for i in range(1, k):\n",
        "        dist_to_centers = 1 - iou_width_height(boxes,\n",
        "centers).max(axis=1)\n",
        "        probs = dist_to_centers / dist_to_centers.sum()\n",
        "        cumprobs = probs.cumsum()\n",
        "        r = np.random.rand()\n",
        "        centers = np.vstack([centers,\n",
        "boxes[np.searchsorted(cumprobs, r)]])\n",
        "\n",
        "    # Main loop\n",
        "    last_clusters = np.zeros((n,))\n",
        "    for iteration in range(max_iter):\n",
        "        ious = iou_width_height(boxes, centers)\n",
        "        clusters = ious.argmax(axis=1)\n",
        "        if (clusters == last_clusters).all():\n",
        "            break\n",
        "        for i in range(k):\n",
        "            if np.sum(clusters == i) > 0:\n",
        "                centers[i] = boxes[clusters == i].mean(axis=0)\n",
        "        last_clusters = clusters.copy()\n",
        "\n",
        "    avg_iou = ious.max(axis=1).mean()\n",
        "    areas = centers[:, 0] * centers[:, 1]\n",
        "    centers = centers[np.argsort(areas)[::-1]]\n",
        "\n",
        "    return centers, avg_iou\n",
        "\n",
        "# 꿀벌 데이터셋에서 박스 크기 수집\n",
        "print(\"Loading bee dataset boxes...\")\n",
        "all_boxes = []\n",
        "for split in ['train', 'val']:\n",
        "    label_dir = Path(f'/content/bee_project/yolo_labels/{split}')\n",
        "    for txt_file in tqdm(list(label_dir.glob('*.txt')),\n",
        "desc=f'Loading {split}'):\n",
        "        with open(txt_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    w, h = float(parts[3]), float(parts[4])\n",
        "                    all_boxes.append([w, h])\n",
        "\n",
        "boxes = np.array(all_boxes)\n",
        "print(f\"Loaded {len(boxes)} boxes\")\n",
        "\n",
        "# K-means 실행\n",
        "anchors, avg_iou = kmeans_anchors(boxes, k=9)\n",
        "print(f\"\\nAverage IoU: {avg_iou:.4f}\")\n",
        "\n",
        "# 픽셀 값으로 변환 (416x416 기준)\n",
        "pixel_anchors = (anchors * 416).astype(int)\n",
        "BEE_ANCHORS = [\n",
        "    pixel_anchors[0:3].tolist(),  # Large\n",
        "    pixel_anchors[3:6].tolist(),  # Medium\n",
        "    pixel_anchors[6:9].tolist()   # Small\n",
        "]\n",
        "\n",
        "print(\"\\n꿀벌 최적화 앵커 (416x416):\")\n",
        "print(f\"BEE_ANCHORS = {BEE_ANCHORS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfBXwuNBYV54"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# [STEP 3-3] 손실/타겟 빌드 + NMS + mAP@0.5 평가 유틸\n",
        "#  - 앵커-기반 매칭: GT box와 가장 IoU가 높은 앵커(스케일 내) 1개에 할당\n",
        "#  - 손실: BCE(obj/cls) + MSE(tx,ty,tw,th) (안정성을 위해 가중치 부여)\n",
        "#  - 평가: NMS 후 VOC 방식으로 AP 계산(클래스 1개)\n",
        "# ============================================\n",
        "import math\n",
        "from torchvision.ops import nms\n",
        "\n",
        "# IoU 계산 함수 (x1y1x2y2)\n",
        "def box_iou(box1, box2):\n",
        "    \"\"\"box1:[N,4], box2:[M,4] → IoU:[N,M]\"\"\"\n",
        "    area1 = (box1[:,2]-box1[:,0]).clamp(0) * (box1[:,3]-box1[:,1]).clamp(0)                               # 면적1\n",
        "    area2 = (box2[:,2]-box2[:,0]).clamp(0) * (box2[:,3]-box2[:,1]).clamp(0)                               # 면적2\n",
        "    lt = torch.max(box1[:,None,:2], box2[:,:2])                                                           # 좌상단\n",
        "    rb = torch.min(box1[:,None,2:], box2[:,2:])                                                           # 우하단\n",
        "    wh = (rb - lt).clamp(min=0)                                                                           # 교집합 너비/높이\n",
        "    inter = wh[:,:,0] * wh[:,:,1]                                                                         # 교집합 면적\n",
        "    return inter / (area1[:,None] + area2 - inter + 1e-6)                                                 # IoU 반환\n",
        "\n",
        "# 앵커-기반 타깃 빌더\n",
        "class TargetBuilder:\n",
        "    \"\"\"GT를 3개 스케일/3앵커 그리드에 할당하는 도우미\"\"\"\n",
        "    def __init__(self, img_size, num_classes, anchors):\n",
        "        self.img_size = img_size                                                                          # 입력 해상도\n",
        "        self.nc = num_classes                                                                             # 클래스 수\n",
        "        self.anchors = anchors                                                                            # 앵커 목록\n",
        "        self.strides = [32, 16, 8]                                                                        # 스트라이드\n",
        "    def build(self, targets, outputs):\n",
        "        \"\"\"\n",
        "        targets:[M,6](b, cx,cy,w,h,cls) 0~1 정규화, outputs: raw 출력 리스트\n",
        "        반환: (obj_t, cls_t, box_t, indices, anchors_scaled)\n",
        "        \"\"\"\n",
        "        na = 3                                                                                             # 스케일당 앵커 수\n",
        "        obj_t, cls_t, box_t, idxs, anchs = [], [], [], [], []                                              # 빈 리스트\n",
        "        device = outputs[0].device                                                                         # 디바이스\n",
        "        for i, out in enumerate(outputs):                                                                  # 스케일 순회\n",
        "            bs, ch, ny, nx = out.shape                                                                     # 맵 크기\n",
        "            stride = self.strides[i]                                                                       # 현재 스트라이드\n",
        "            scale = torch.tensor(self.anchors[i], device=device).float() / stride                          # 앵커(셀단위)\n",
        "            # 빈 타깃 텐서 생성\n",
        "            obj_map = torch.zeros(bs, na, ny, nx, device=device)                                           # objectness\n",
        "            cls_map = torch.zeros(bs, na, ny, nx, self.nc, device=device)                                  # 클래스원핫\n",
        "            box_map = torch.zeros(bs, na, ny, nx, 4, device=device)                                        # (tx,ty,tw,th)\n",
        "            # 현재 스케일에 들어올 타깃만 처리\n",
        "            if targets.numel() > 0:                                                                        # 타깃이 있다면\n",
        "                t = targets.clone()                                                                        # 복사\n",
        "                # 픽셀 좌표로 변환\n",
        "                gx = t[:,1] * nx                                                                           # 셀 좌표 x\n",
        "                gy = t[:,2] * ny                                                                           # 셀 좌표 y\n",
        "                gw =  t[:,3] * self.img_size / stride                                                      # 셀 스케일 w\n",
        "                gh =  t[:,4] * self.img_size / stride                                                      # 셀 스케일 h\n",
        "                gi = gx.long().clamp(0, nx-1)                                                              # 셀 인덱스 i\n",
        "                gj = gy.long().clamp(0, ny-1)                                                              # 셀 인덱스 j\n",
        "                # 앵커 매칭(wh IoU 기반) - 가장 유사한 앵커 하나 선택\n",
        "                gt_wh = torch.stack([gw, gh], dim=1)                                                       # [M,2]\n",
        "                anc_wh = scale                                                                             # [3,2]\n",
        "                # IoU를 너비/높이만으로 근사 계산\n",
        "                inter = torch.min(gt_wh[:,None,:], anc_wh[None,:,:]).prod(2)                               # 교집합(근사)\n",
        "                union = (gt_wh[:,None,:].prod(2) + anc_wh[None,:,:].prod(2) - inter + 1e-6)               # 합집합\n",
        "                ious = inter / union                                                                       # IoU 근사\n",
        "                best = ious.argmax(1)                                                                      # 최적 앵커 id\n",
        "                b = t[:,0].long()                                                                          # 배치 인덱스\n",
        "                c = t[:,5].long()                                                                          # 클래스 id\n",
        "                # 지도 값 계산 (tx,ty,tw,th)\n",
        "                tx = gx - gi.float()                                                                       # 셀 내 offset x\n",
        "                ty = gy - gj.float()                                                                       # 셀 내 offset y\n",
        "                tw = (gw / scale[best,0]).clamp(min=1e-6).log()                                            # 로그폭\n",
        "                th = (gh / scale[best,1]).clamp(min=1e-6).log()                                            # 로그높이\n",
        "                # 맵에 쓰기\n",
        "                obj_map[b, best, gj, gi] = 1.0                                                             # objectness=1\n",
        "                cls_map[b, best, gj, gi, c] = 1.0                                                          # 원핫 클래스\n",
        "                box_map[b, best, gj, gi] = torch.stack([tx,ty,tw,th], dim=1)                               # 박스 타겟\n",
        "            obj_t.append(obj_map)                                                                          # 누적\n",
        "            cls_t.append(cls_map)                                                                          # 누적\n",
        "            box_t.append(box_map)                                                                          # 누적\n",
        "            idxs.append((ny, nx))                                                                          # 크기 정보 저장\n",
        "            anchs.append(scale)                                                                            # 앵커 저장\n",
        "        return obj_t, cls_t, box_t, idxs, anchs                                                            # 결과 반환\n",
        "\n",
        "# 손실 함수 (YOLOv3 스타일의 간단 가중합)\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, img_size, num_classes, anchors,\n",
        "                 lambda_box=0.05, lambda_obj=1.0, lambda_cls=0.5,\n",
        "                 neg_obj_weight=0.05):  # 음성 오브젝트 가중치(작게)\n",
        "        super().__init__()                                                  # 부모 초기화\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')                   # 맵 단위 BCE\n",
        "        self.mse = nn.MSELoss(reduction='none')                             # 맵 단위 MSE\n",
        "        self.builder = TargetBuilder(img_size, num_classes, anchors)        # 타깃 빌더\n",
        "        self.lambda_box = lambda_box                                        # 박스 가중치\n",
        "        self.lambda_obj = lambda_obj                                        # obj 가중치\n",
        "        self.lambda_cls = lambda_cls                                        # cls 가중치\n",
        "        self.neg_obj_weight = neg_obj_weight                                # 음성 obj 가중치\n",
        "        self.num_classes = num_classes                                      # 클래스 수\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        outputs: 모델의 raw 출력 리스트(3스케일)\n",
        "        targets: [M,6] = (b, cx,cy,w,h,cls) (0~1 정규화)\n",
        "        \"\"\"\n",
        "        # 타깃 맵 생성\n",
        "        obj_t, cls_t, box_t, _, _ = self.builder.build(targets, outputs)    # 각 스케일별 타깃\n",
        "\n",
        "        device = outputs[0].device                                          # 디바이스\n",
        "        l_box = torch.tensor(0., device=device)                             # 박스 손실 누적\n",
        "        l_obj = torch.tensor(0., device=device)                             # obj 손실 누적\n",
        "        l_cls = torch.tensor(0., device=device)                             # cls 손실 누적\n",
        "\n",
        "        for i, out in enumerate(outputs):                                    # 스케일 순회\n",
        "            bs, ch, ny, nx = out.shape                                      # 크기 정보\n",
        "            na = 3                                                          # 앵커 수\n",
        "            no = 5 + self.num_classes                                       # 출력 채널 수\n",
        "            out = out.view(bs, na, no, ny, nx).permute(0,1,3,4,2).contiguous()  # [B,A,ny,nx,no]\n",
        "\n",
        "            # 로짓 분해\n",
        "            px = out[..., 0]                                                # tx 로짓\n",
        "            py = out[..., 1]                                                # ty 로짓\n",
        "            pw = out[..., 2]                                                # tw 로짓 (로그폭)\n",
        "            ph = out[..., 3]                                                # th 로짓 (로그높이)\n",
        "            pobj = out[..., 4]                                              # obj 로짓\n",
        "            pcls = out[..., 5:] if self.num_classes > 0 else None           # cls 로짓\n",
        "\n",
        "            # 타깃 맵\n",
        "            tbox = box_t[i]                                                 # [B,A,ny,nx,4] (tx,ty,tw,th)\n",
        "            tobj = obj_t[i]                                                 # [B,A,ny,nx]\n",
        "            tcls = cls_t[i]                                                 # [B,A,ny,nx,C]\n",
        "\n",
        "            # 양성/음성 마스크\n",
        "            pos = tobj.bool()                                               # GT가 할당된 위치\n",
        "            # 같은 셀(같은 B, y, x)에 GT가 하나라도 있으면 그 셀의 모든 앵커를 음성에서 제외(ignore)\n",
        "            any_pos_cell = pos.any(dim=1, keepdim=True)                     # [B,1,ny,nx]\n",
        "            neg_mask = (~pos) & (~any_pos_cell.expand_as(pos))              # 진짜 음성만\n",
        "\n",
        "            # --- 박스 손실: 양성에서만 (xy는 sigmoid 후 MSE, wh는 로짓-MSE) ---\n",
        "            if pos.any():\n",
        "                l_box = l_box + self.mse(torch.sigmoid(px)[pos], tbox[...,0][pos]).mean()\n",
        "                l_box = l_box + self.mse(torch.sigmoid(py)[pos], tbox[...,1][pos]).mean()\n",
        "                l_box = l_box + self.mse(pw[pos],               tbox[...,2][pos]).mean()\n",
        "                l_box = l_box + self.mse(ph[pos],               tbox[...,3][pos]).mean()\n",
        "\n",
        "            # --- 오브젝트니스 손실: 양성 + (작은 가중)음성 ---\n",
        "            obj_loss_map = self.bce(pobj, tobj)                            # 위치별 BCE\n",
        "            if pos.any():\n",
        "                l_obj = l_obj + obj_loss_map[pos].mean()                   # 양성 부분\n",
        "            if neg_mask.any():\n",
        "                l_obj = l_obj + self.neg_obj_weight * obj_loss_map[neg_mask].mean()  # 음성 약하게\n",
        "\n",
        "            # --- 클래스 손실: 양성에서만 ---\n",
        "            if self.num_classes > 0 and pos.any():\n",
        "                l_cls = l_cls + self.bce(pcls[pos], tcls[pos]).mean()\n",
        "\n",
        "        # 가중합 손실\n",
        "        loss = self.lambda_box*l_box + self.lambda_obj*l_obj + self.lambda_cls*l_cls\n",
        "        return loss, (l_box.detach().item(), l_obj.detach().item(), l_cls.detach().item())\n",
        "# NMS + 스코어 기반 박스 필터링\n",
        "def postprocess(outputs, conf_thr=0.25, iou_thr=0.5, img_size=IMG_SIZE, num_classes=1, anchors=None):\n",
        "    \"\"\"모델 출력 → 최종 박스/점수/클래스 (이미지별 리스트)\"\"\"\n",
        "    boxes, obj, cls = yolo_decode(outputs, img_size, num_classes, anchors)                                 # 디코드\n",
        "    bs = boxes.size(0)                                                                                      # 배치 크기\n",
        "    results = []                                                                                            # 결과 리스트\n",
        "    for b in range(bs):                                                                                     # 배치 순회\n",
        "        scores = obj[b]                                                                                     # 오브젝트 확률\n",
        "        if num_classes>0:                                                                                   # 클래스 있으면\n",
        "            cls_scores, cls_ids = cls[b].max(dim=1)                                                         # 최고 클래스\n",
        "            scores = scores * cls_scores                                                                    # 최종 스코어\n",
        "        keep = scores > conf_thr                                                                            # 임계값 필터\n",
        "        bxs = boxes[b][keep]                                                                                # 박스 필터\n",
        "        scs = scores[keep]                                                                                  # 점수 필터\n",
        "        cids = cls_ids[keep] if num_classes>0 else torch.zeros_like(scs, dtype=torch.long)                 # 클래스 id\n",
        "        if bxs.numel()==0:                                                                                  # 없으면\n",
        "            results.append((torch.zeros((0,4)), torch.zeros((0,)), torch.zeros((0,), dtype=torch.long)))    # 빈 결과\n",
        "            continue                                                                                        # 다음으로\n",
        "        keep_idx = nms(bxs, scs, iou_thr)                                                                   # NMS 수행\n",
        "        results.append((bxs[keep_idx], scs[keep_idx], cids[keep_idx]))                                      # 결과 저장\n",
        "    return results                                                                                           # 리스트 반환\n",
        "\n",
        "# 간단 mAP@0.5 (VOC 방식, 단일 클래스)\n",
        "def eval_map50(model, dataloader, device, conf_thr=0.25, iou_thr=0.5, anchors=None):\n",
        "    \"\"\"단일 클래스 기준 mAP@0.5 계산 (VOC 곡선 적분)\"\"\"\n",
        "    model.eval()                                                                                            # 평가 모드\n",
        "    all_preds, all_gts = [], {}                                                                             # 예측/GT 저장\n",
        "    with torch.no_grad():                                                                                   # 그라드 끄기\n",
        "        for imgs, targets, names in dataloader:                                                             # 배치 순회\n",
        "            imgs = imgs.to(device, non_blocking=True)                                                       # 이미지 GPU\n",
        "            outputs = model(imgs)                                                                           # 모델 추론\n",
        "            batch_res = postprocess(outputs, conf_thr, iou_thr, IMG_SIZE, 1, model.anchors)                # 후처리\n",
        "            # GT를 이미지별로 수집 (픽셀 좌표 필요)\n",
        "            for bi, name in enumerate(names):                                                               # 이미지별\n",
        "                g = targets[targets[:,0]==bi] if targets.numel()>0 else torch.zeros((0,6))                  # 해당 GT\n",
        "                g = g[:,1:5] if g.numel()>0 else torch.zeros((0,4))                                         # [cx,cy,w,h]\n",
        "                # 정규화 → 픽셀 박스\n",
        "                if g.numel()>0:\n",
        "                    cx = g[:,0]*IMG_SIZE ; cy=g[:,1]*IMG_SIZE ; w=g[:,2]*IMG_SIZE ; h=g[:,3]*IMG_SIZE       # 픽셀\n",
        "                    gx1 = cx - w/2 ; gy1 = cy - h/2 ; gx2 = cx + w/2 ; gy2 = cy + h/2                       # x1y1x2y2\n",
        "                    gxyxy = torch.stack([gx1,gy1,gx2,gy2], dim=1)                                           # GT 박스\n",
        "                else:\n",
        "                    gxyxy = torch.zeros((0,4))                                                              # 빈 GT\n",
        "                all_gts[name] = {'boxes': gxyxy.cpu(), 'detected': np.zeros(len(gxyxy), dtype=bool)}        # GT 저장\n",
        "            # 예측 수집 (이미지명 단위)\n",
        "            for (bxs, scs, cids), name in zip(batch_res, names):                                            # 예측/이름\n",
        "                for i in range(len(bxs)):                                                                   # 각 박스\n",
        "                    all_preds.append((name, scs[i].item(), bxs[i].cpu().numpy()))                           # (img,score,box)\n",
        "    # 스코어 내림차순 정렬\n",
        "    all_preds.sort(key=lambda x: x[1], reverse=True)                                                        # 정렬\n",
        "    tp, fp = [], []                                                                                         # TP/FP 기록\n",
        "    for name, score, pbox in all_preds:                                                                     # 예측 순회\n",
        "        gt = all_gts[name]['boxes']                                                                         # GT 박스들\n",
        "        if gt.numel()==0:                                                                                   # GT 없음\n",
        "            tp.append(0) ; fp.append(1)                                                                     # 무조건 FP\n",
        "            continue                                                                                        # 다음\n",
        "        ious = box_iou(torch.tensor(pbox[None,:]), gt).squeeze(0).numpy()                                   # IoU 계산\n",
        "        m = ious.argmax() # Fixed typo from 'm = ious.argmax()' to 'm = ious.argmax()'\n",
        "        if ious[m] >= 0.5 and not all_gts[name]['detected'][m]:                                             # 매칭 가능\n",
        "            tp.append(1) ; fp.append(0)                                                                     # TP 기록\n",
        "            all_gts[name]['detected'][m] = True                                                             # 사용 처리\n",
        "        else:\n",
        "            tp.append(0) ; fp.append(1)                                                                     # FP 기록\n",
        "    # 누적 TPs/FPs → P/R 곡선\n",
        "    tp = np.cumsum(tp)                                                                                      # 누적 TP\n",
        "    fp = np.cumsum(fp)                                                                                      # 누적 FP\n",
        "    eps = 1e-9                                                                                              # 0나눗셈 방지\n",
        "    recalls = tp / (sum(len(v['boxes']) for v in all_gts.values()) + eps)                                   # 재현율\n",
        "    precisions = tp / (tp + fp + eps)                                                                       # 정밀도\n",
        "    # AP 계산(단순 적분)\n",
        "    def compute_ap(rec, prec):                                                                              # AP 함수\n",
        "        mrec = np.concatenate(([0.0], rec, [1.0]))                                                          # 경계 추가\n",
        "        mpre = np.concatenate(([0.0], prec, [0.0]))                                                         # 경계 추가\n",
        "        for i in range(mpre.size-1, 0, -1):                                                                 # 단조감소 보정\n",
        "            mpre[i-1] = np.maximum(mpre[i-1], mpre[i])                                                      # 보정\n",
        "        idx = np.where(mrec[1:] != mrec[:-1])[0]                                                            # 변화 구간\n",
        "        ap = np.sum((mrec[idx+1]-mrec[idx]) * mpre[idx+1])                                                  # 면적 합\n",
        "        return ap                                                                                            # AP 반환\n",
        "    ap50 = compute_ap(recalls, precisions)                                                                  # AP@0.5\n",
        "    return float(ap50), float(precisions[-1] if precisions.size>0 else 0.0), float(recalls[-1] if recalls.size>0 else 0.0)  # (mAP, P, R)\n",
        "\n",
        "# ======================================================\n",
        "# 업그레이드 탐지 평가: AP50, AP50-95, P/R, mean IoU(TP),\n",
        "#                       속도 분해(전처리/추론/NMS)\n",
        "# ======================================================\n",
        "\n",
        "import time, numpy as np, torch\n",
        "from torchvision.ops import nms\n",
        "\n",
        "\n",
        "def compute_ap(rec, prec):\n",
        "    mrec = np.concatenate(([0.0], rec, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], prec, [0.0]))\n",
        "    for i in range(mpre.size-1, 0, -1):\n",
        "        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n",
        "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "    return np.sum((mrec[idx+1]-mrec[idx]) * mpre[idx+1])\n",
        "\n",
        "def _gather_gt_pred(model, dataloader, device, conf_thr, iou_thr):\n",
        "    model.eval()\n",
        "    all_preds = []  # (name, score, box[x1y1x2y2])\n",
        "    gts = {}        # name -> {boxes:Tensor[N,4], detected:bool[N]}\n",
        "    t_mean_iou = [] # TPs의 IoU 저장\n",
        "    t_pre_ms, t_inf_ms, t_nms_ms = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets, names in dataloader:\n",
        "            # --- 전처리 시간: 여기선 거의 없음(이미 텐서) ---\n",
        "            t0 = time.time()\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            torch.cuda.synchronize() if device.type=='cuda' else None\n",
        "            t1 = time.time()\n",
        "\n",
        "            # --- 모델 추론 ---\n",
        "            out = model(imgs)\n",
        "            torch.cuda.synchronize() if device.type=='cuda' else None\n",
        "            t2 = time.time()\n",
        "\n",
        "            # --- 디코드 + NMS ---\n",
        "            boxes, obj, cls = yolo_decode(out, IMG_SIZE, 1, model.anchors)\n",
        "            batch_res = []\n",
        "            for b in range(imgs.size(0)):\n",
        "                scores = obj[b]\n",
        "                keep = scores > conf_thr\n",
        "                bxs, scs = boxes[b][keep], scores[keep]\n",
        "                if bxs.numel()==0:\n",
        "                    batch_res.append((torch.zeros((0,4)), torch.zeros((0,))))\n",
        "                    continue\n",
        "                keep_idx = nms(bxs, scs, iou_thr)\n",
        "                batch_res.append((bxs[keep_idx], scs[keep_idx]))\n",
        "            torch.cuda.synchronize() if device.type=='cuda' else None\n",
        "            t3 = time.time()\n",
        "\n",
        "            t_pre_ms.append((t1 - t0) * 1000.0)\n",
        "            t_inf_ms.append((t2 - t1) * 1000.0)\n",
        "            t_nms_ms.append((t3 - t2) * 1000.0)\n",
        "\n",
        "            # --- GT(픽셀좌표) 누적 ---\n",
        "            for bi, name in enumerate(names):\n",
        "                g = targets[targets[:,0]==bi] if targets.numel()>0 else torch.zeros((0,6))\n",
        "                g = g[:,1:5] if g.numel()>0 else torch.zeros((0,4))\n",
        "                if g.numel()>0:\n",
        "                    cx = g[:,0]*IMG_SIZE; cy=g[:,1]*IMG_SIZE; w=g[:,2]*IMG_SIZE; h=g[:,3]*IMG_SIZE\n",
        "                    gx1, gy1, gx2, gy2 = cx-w/2, cy-h/2, cx+w/2, cy+h/2\n",
        "                    gxyxy = torch.stack([gx1,gy1,gx2,gy2], dim=1).cpu()\n",
        "                else:\n",
        "                    gxyxy = torch.zeros((0,4))\n",
        "                gts[name] = {'boxes': gxyxy, 'detected': np.zeros(len(gxyxy), dtype=bool)}\n",
        "\n",
        "            # --- 예측 누적 + TP IoU 기록 ---\n",
        "            for (bxs, scs), name in zip(batch_res, names):\n",
        "                bxs = bxs.cpu()\n",
        "                for i in range(len(bxs)):\n",
        "                    all_preds.append((name, float(scs[i].item()), bxs[i].numpy()))\n",
        "\n",
        "                # TP IoU(참고용): 가장 높은 IoU와 매칭되면 기록\n",
        "                if len(bxs) and len(gts[name]['boxes']):\n",
        "                    ious = box_iou(bxs, gts[name]['boxes']).numpy()\n",
        "                    # Ensure ious is not empty before taking max\n",
        "                    if ious.size > 0:\n",
        "                        max_per_pred = ious.max(axis=1)\n",
        "                        if max_per_pred.size:\n",
        "                            # 실제 TP인지 여부는 AP 계산 시 확정되므로, 여기선 최대 IoU 분포만 참고(근사)\n",
        "                            t_mean_iou.extend(max_per_pred.tolist())\n",
        "                        else: # Handle case where max_per_pred is empty\n",
        "                             t_mean_iou.extend([0.0] * len(bxs)) # Append 0.0 for each predicted box\n",
        "\n",
        "    return all_preds, gts, t_mean_iou, t_pre_ms, t_inf_ms, t_nms_ms\n",
        "\n",
        "def _ap_at_iou_threshold(all_preds, gts, iou_thr):\n",
        "    all_preds = sorted(all_preds, key=lambda x: x[1], reverse=True)\n",
        "    tp = [] # TP 기록\n",
        "    fp = [] # FP 기록\n",
        "    local_gts = {k: {'boxes': v['boxes'], 'detected': v['detected'].copy()} for k,v in gts.items()}\n",
        "    for name, score, pbox in all_preds:\n",
        "        gt = local_gts[name]['boxes']\n",
        "        if gt.numel()==0: # GT 없음\n",
        "            tp.append(0) ; fp.append(1); continue\n",
        "        ious = box_iou(torch.tensor(pbox[None,:]), gt).squeeze(0).numpy()\n",
        "        # Ensure ious is not empty before taking argmax\n",
        "        if ious.size > 0:\n",
        "            m = ious.argmax()\n",
        "            if ious[m] >= iou_thr and not local_gts[name]['detected'][m]: # 매칭 가능\n",
        "                tp.append(1) ; fp.append(0); local_gts[name]['detected'][m] = True # TP 기록\n",
        "            else:\n",
        "                tp.append(0) ; fp.append(1) # FP 기록\n",
        "        else: # No IoUs calculated (e.g., pred box is invalid)\n",
        "            tp.append(0) ; fp.append(1) # Treat as FP\n",
        "\n",
        "\n",
        "    # 누적 TPs/FPs\n",
        "    tp = np.cumsum(tp)\n",
        "    fp = np.cumsum(fp)\n",
        "\n",
        "    eps = 1e-9\n",
        "    # Handle cases where there are no GT boxes or no predictions\n",
        "    total_gt = sum(len(v['boxes']) for v in gts.values())\n",
        "    recalls = tp / (total_gt + eps) if total_gt > 0 else np.zeros_like(tp)\n",
        "    precisions = tp / (tp + fp + eps) if (tp + fp).sum() > 0 else np.zeros_like(tp)\n",
        "\n",
        "\n",
        "    # AP 계산\n",
        "    ap = compute_ap(recalls, precisions)\n",
        "\n",
        "    # Handle cases where precision or recall arrays might be empty\n",
        "    final_precision = float(precisions[-1] if precisions.size > 0 else 0.0)\n",
        "    final_recall = float(recalls[-1] if recalls.size > 0 else 0.0)\n",
        "\n",
        "\n",
        "    return ap, final_precision, final_recall\n",
        "\n",
        "\n",
        "def eval_detection(model, dataloader, device, conf_thr=0.25, iou_thr=0.5):\n",
        "    # 1) 예측/GT 수집 + 속도\n",
        "    preds, gts, tp_ious, pre_ms, inf_ms, nms_ms = _gather_gt_pred(model, dataloader, device, conf_thr, iou_thr)\n",
        "\n",
        "    # 2) AP@0.5\n",
        "    ap50, p50, r50 = _ap_at_iou_threshold(preds, gts, 0.5)\n",
        "\n",
        "    # 3) AP@0.5:0.95\n",
        "    # Only calculate AP for IoU thresholds if there are any ground truth boxes\n",
        "    total_gt_boxes = sum(len(v['boxes']) for v in gts.values())\n",
        "    if total_gt_boxes > 0:\n",
        "        ious = np.arange(0.5, 0.95 + 1e-9, 0.05)\n",
        "        ap_list = [ _ap_at_iou_threshold(preds, gts, t)[0] for t in ious ]\n",
        "        ap5095 = float(np.mean(ap_list))\n",
        "    else:\n",
        "        ap5095 = 0.0 # If no GT boxes, AP@0.5:0.95 is 0\n",
        "\n",
        "    # 4) mean IoU of TPs (근사: 예측-최대 IoU 분포 평균)\n",
        "    mean_iou_tp = float(np.mean(tp_ious)) if len(tp_ious) else 0.0\n",
        "\n",
        "    # 5) 속도\n",
        "    pre_ms  = float(np.mean(pre_ms)) if pre_ms else 0.0\n",
        "    inf_ms  = float(np.mean(inf_ms)) if inf_ms else 0.0\n",
        "    nms_ms  = float(np.mean(nms_ms)) if nms_ms else 0.0\n",
        "    total_ms= pre_ms + inf_ms + nms_ms\n",
        "    # Avoid division by zero if total_ms is zero\n",
        "    fps     = 1000.0 / total_ms if total_ms > 0 else 0.0\n",
        "\n",
        "\n",
        "    return {\n",
        "        'AP50': float(ap50),\n",
        "        'AP50_95': float(ap5095),\n",
        "        'Precision@0.5': float(p50),\n",
        "        'Recall@0.5': float(r50),\n",
        "        'mean_IoU_TP': float(mean_iou_tp),\n",
        "        'pre_ms': pre_ms, 'infer_ms': inf_ms, 'nms_ms': nms_ms,\n",
        "        'latency_ms': total_ms, 'FPS': fps\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po6Q-pojTaDw"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [수정된 YOLOv3 손실함수] - 논문 가중치 적용\n",
        "# ============================\n",
        "\n",
        "class YoloLossFixed(nn.Module):\n",
        "    \"\"\"YOLOv3 손실함수 - 논문 가중치 수정 버전\"\"\"\n",
        "    def __init__(self, img_size, num_classes, anchors,\n",
        "                lambda_box=5.0,      # 수정: 0.05 → 5.0\n",
        "                lambda_obj=1.0,\n",
        "                lambda_cls=1.0,      # 수정: 0.5 → 1.0\n",
        "                neg_obj_weight=0.5): # 수정: 0.05 → 0.5\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "        self.builder = TargetBuilder(img_size, num_classes,anchors)\n",
        "        self.lambda_box = lambda_box\n",
        "        self.lambda_obj = lambda_obj\n",
        "        self.lambda_cls = lambda_cls\n",
        "        self.neg_obj_weight = neg_obj_weight\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        obj_t, cls_t, box_t, _, _ = self.builder.build(targets,outputs)\n",
        "\n",
        "        device = outputs[0].device\n",
        "        l_box = torch.tensor(0., device=device)\n",
        "        l_obj = torch.tensor(0., device=device)\n",
        "        l_cls = torch.tensor(0., device=device)\n",
        "\n",
        "        for i, out in enumerate(outputs):\n",
        "            bs, ch, ny, nx = out.shape\n",
        "            na = 3\n",
        "            no = 5 + self.num_classes\n",
        "            out = out.view(bs, na, no, ny,nx).permute(0,1,3,4,2).contiguous()\n",
        "\n",
        "            px = out[..., 0]\n",
        "            py = out[..., 1]\n",
        "            pw = out[..., 2]\n",
        "            ph = out[..., 3]\n",
        "            pobj = out[..., 4]\n",
        "            pcls = out[..., 5:] if self.num_classes > 0 else None\n",
        "\n",
        "            tbox = box_t[i]\n",
        "            tobj = obj_t[i]\n",
        "            tcls = cls_t[i]\n",
        "\n",
        "            pos = tobj.bool()\n",
        "            neg = ~pos\n",
        "\n",
        "            # 박스 손실 (가중치 5.0)\n",
        "            if pos.any():\n",
        "                l_box = l_box + self.mse(torch.sigmoid(px)[pos],tbox[...,0][pos]).mean()\n",
        "                l_box = l_box + self.mse(torch.sigmoid(py)[pos],tbox[...,1][pos]).mean()\n",
        "                l_box = l_box + self.mse(pw[pos],tbox[...,2][pos]).mean()\n",
        "                l_box = l_box + self.mse(ph[pos],tbox[...,3][pos]).mean()\n",
        "\n",
        "            # Objectness 손실 (음성 가중치 0.5)\n",
        "            obj_loss_map = self.bce(pobj, tobj)\n",
        "            if pos.any():\n",
        "                l_obj = l_obj + obj_loss_map[pos].mean()\n",
        "            if neg.any():\n",
        "                l_obj = l_obj + self.neg_obj_weight *obj_loss_map[neg].mean()\n",
        "\n",
        "            # 클래스 손실\n",
        "            if self.num_classes > 0 and pos.any():\n",
        "                l_cls = l_cls + self.bce(pcls[pos],tcls[pos]).mean()\n",
        "\n",
        "        loss = self.lambda_box*l_box + self.lambda_obj*l_obj +self.lambda_cls*l_cls\n",
        "        return loss, (l_box.detach().item(),l_obj.detach().item(), l_cls.detach().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWUiG2L6Zc6g"
      },
      "outputs": [],
      "source": [
        "# ===== [필수] 환경/하이퍼 설정 =====\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "NC = 1               # 클래스 수 (벌 데이터면 1 또는 필요한 수로 설정)\n",
        "IMG_SIZE = 416       # 입력 해상도 (당신 모델과 데이터 파이프라인에 맞추세요)\n",
        "WEIGHTS = None       # \"/content/bee_project/weights/best.pt\" 등 학습 가중치 경로 (없으면 None)\n",
        "\n",
        "# ===== [중요] 모델 인스턴스 생성 =====\n",
        "# 위에 올려준 YOLOv3 / Darknet53 클래스 정의 셀을 먼저 실행한 다음 이 셀을 실행하세요.\n",
        "model = YOLOv3(num_classes=NC, img_size=IMG_SIZE).to(device)\n",
        "\n",
        "# (선택) 학습/사전학습 가중치 로드\n",
        "if WEIGHTS is not None:\n",
        "    ckpt = torch.load(WEIGHTS, map_location=device)\n",
        "    # ckpt가 {\"model_state_dict\": ...} 구조일 수도, 바로 state_dict일 수도 있습니다.\n",
        "    state_dict = ckpt.get(\"model_state_dict\", ckpt)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(f\"Loaded weights from: {WEIGHTS}\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# ===== [점검 1] 더미 입력으로 포워드 (shape 검증) =====\n",
        "with torch.no_grad():\n",
        "    x = torch.randn(1, 3, IMG_SIZE, IMG_SIZE, device=device)    # 배치1, 3채널, 416x416\n",
        "    outs = model(x)  # [out1, out2, out3]\n",
        "\n",
        "for i, o in enumerate(outs, 1):\n",
        "    print(f\"head{i}.pred shape = {tuple(o.shape)}\")\n",
        "    # 기대 모양: [B, A*(5+NC), S, S]  (A=3)\n",
        "    # 예: NC=1이면 채널 = 3*(5+1)=18\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9j6D9bxam02"
      },
      "outputs": [],
      "source": [
        "def assert_like_paper(model, img_size=416, nc=1, na=3):\n",
        "    model.eval()\n",
        "    x = torch.randn(1, 3, img_size, img_size).to(next(model.parameters()).device)\n",
        "    with torch.no_grad():\n",
        "        outs = model(x)\n",
        "    strides = [32,16,8]\n",
        "    expected_sizes = [img_size//s for s in strides]  # [13,26,52] for 416\n",
        "\n",
        "    for i, (o, S) in enumerate(zip(outs, expected_sizes), 1):\n",
        "        b, c, h, w = o.shape\n",
        "        assert h == S and w == S, f\"head{i} size mismatch: got {(h,w)}, want {(S,S)}\"\n",
        "        assert c == na*(5+nc),    f\"head{i} ch mismatch: got {c}, want {na*(5+nc)}\"\n",
        "        print(f\"[OK] head{i}: shape={tuple(o.shape)} (S={S}, C={na*(5+nc)})\")\n",
        "\n",
        "assert_like_paper(model, img_size=416, nc=1, na=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w1AV6oPYlxU"
      },
      "outputs": [],
      "source": [
        "# 경로 체크, 예측 수 확인\n",
        "# ===== 빠른 무결성 점검 =====\n",
        "print('TRAIN_IMG_DIR =', TRAIN_IMG_DIR)\n",
        "print('VAL_IMG_DIR   =', VAL_IMG_DIR)\n",
        "print('TRAIN_LBL_DIR =', TRAIN_LBL_DIR)\n",
        "print('VAL_LBL_DIR   =', VAL_LBL_DIR)\n",
        "\n",
        "# ============================================\n",
        "# [YAML → 런타임 변수 바인딩]  (기존 YAML 키 그대로 사용)\n",
        "#  - TRAIN_IMG_DIR / VAL_IMG_DIR / TRAIN_LBL_DIR / VAL_LBL_DIR 등 설정\n",
        "#  - eval_conf, IOU_THR, IMG_SIZE, NC, device 설정\n",
        "# ============================================\n",
        "import os, yaml, torch\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) YAML 로드 (경로는 네가 저장한 위치로)\n",
        "CONFIG_PATH = Path('/content/bee_project/project_config.yaml')  # 필요시 수정\n",
        "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
        "    _cfg = yaml.safe_load(f)\n",
        "\n",
        "dcfg = _cfg['detection']\n",
        "pcfg = _cfg['paths']\n",
        "\n",
        "# 2) 경로/하이퍼 파라미터를 런타임 변수로 바인딩\n",
        "TRAIN_IMG_DIR = dcfg['train_images']\n",
        "VAL_IMG_DIR   = dcfg['val_images']\n",
        "TRAIN_LBL_DIR = dcfg['train_labels']\n",
        "VAL_LBL_DIR   = dcfg['val_labels']\n",
        "\n",
        "NC       = int(dcfg.get('num_classes', 1))\n",
        "IMG_SIZE = int(dcfg.get('img_size', 416))\n",
        "eval_conf = float(dcfg.get('conf_threshold', 0.25))\n",
        "IOU_THR   = float(dcfg.get('iou_threshold', 0.5))   # 기존 YAML에 0.15였으면 너무 낮음 → 0.45~0.6 권장\n",
        "\n",
        "# device 설정 (yaml의 device는 'cuda'/'cpu'/'auto'가 있을 수 있음)\n",
        "dev_pref = _cfg.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if dev_pref == 'auto':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "else:\n",
        "    device = torch.device('cuda' if (dev_pref == 'cuda' and torch.cuda.is_available()) else 'cpu')\n",
        "\n",
        "# 3) 경로 존재 체크 및 경고\n",
        "for p in [TRAIN_IMG_DIR, VAL_IMG_DIR, TRAIN_LBL_DIR, VAL_LBL_DIR]:\n",
        "    if not os.path.isdir(p):\n",
        "        print(f\"[WARN] 경로가 존재하지 않습니다: {p}\")\n",
        "\n",
        "# 4) 현재 설정 요약\n",
        "print(f\"[CFG] device={device}, NC={NC}, IMG_SIZE={IMG_SIZE}, conf={eval_conf}, iou={IOU_THR}\")\n",
        "print(f\"[PATH] TRAIN_IMG_DIR={TRAIN_IMG_DIR}\")\n",
        "print(f\"[PATH] VAL_IMG_DIR  ={VAL_IMG_DIR}\")\n",
        "print(f\"[PATH] TRAIN_LBL_DIR={TRAIN_LBL_DIR}\")\n",
        "print(f\"[PATH] VAL_LBL_DIR  ={VAL_LBL_DIR}\")\n",
        "\n",
        "# 5) (옵션) model이 아직 없으면 최소한 인스턴스만 생성해두기\n",
        "try:\n",
        "    model\n",
        "except NameError:\n",
        "    try:\n",
        "        model = YOLOv3(num_classes=NC, img_size=IMG_SIZE).to(device)\n",
        "        model.eval()\n",
        "        print(\"[INFO] YOLOv3 model was instantiated (no weights loaded).\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] 모델 인스턴스 생성 실패: {e}\\n\"\n",
        "              f\"       이미 다른 셀에서 만들었다면 이 경고는 무시해도 됩니다.\")\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "def count_yolo_labels(lbl_dir):\n",
        "    n_files=0; n_boxes=0\n",
        "    for p in Path(lbl_dir).glob('*.txt'):\n",
        "        n_files += 1\n",
        "        with open(p, 'r') as f:\n",
        "            lines = [ln for ln in f.read().strip().splitlines() if ln.strip()]\n",
        "            n_boxes += len(lines)\n",
        "    print(f'[VAL] label files={n_files}, total boxes={n_boxes}')\n",
        "count_yolo_labels(VAL_LBL_DIR)\n",
        "\n",
        "# 샘플 배치 예측 수(초기 conf) 확인\n",
        "eval_conf = 0.25\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    imgs, targets, names = next(iter(val_loader))\n",
        "    outs = model(imgs.to(device))\n",
        "    res  = postprocess(outs, eval_conf, IOU_THR, IMG_SIZE, 1, model.anchors)\n",
        "    print(f'sample detections per image (conf={eval_conf}):', [len(r[0]) for r in res])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXNlmvMfTaDx"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# [수정된 모델/손실함수로 교체]\n",
        "# ============================\n",
        "\n",
        "# 꿀벌 최적화 앵커 (위에서 계산한 값 사용하거나 아래 기본값 사용)\n",
        "if 'BEE_ANCHORS' not in globals():\n",
        "    BEE_ANCHORS = [\n",
        "        [(116, 90), (156, 198), (373, 326)],  # Large\n",
        "        [(30, 61), (62, 45), (59, 119)],      # Medium\n",
        "        [(10, 13), (16, 30), (33, 23)]        # Small\n",
        "    ]\n",
        "\n",
        "# 모델 재생성 (꿀벌 앵커 적용)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else'cpu')\n",
        "model = YOLOv3(num_classes=1, anchors=BEE_ANCHORS,img_size=416).to(device)\n",
        "\n",
        "# 수정된 손실함수 사용\n",
        "criterion = YoloLossFixed(\n",
        "    img_size=416,\n",
        "    num_classes=1,\n",
        "    anchors=BEE_ANCHORS,\n",
        "    lambda_box=5.0,      # 논문값\n",
        "    lambda_obj=1.0,\n",
        "    lambda_cls=1.0,\n",
        "    neg_obj_weight=0.5   # 논문값\n",
        ").to(device)\n",
        "\n",
        "print(\"✅ 모델 수정 완료:\")\n",
        "print(\"- 꿀벌 최적화 앵커 적용\")\n",
        "print(\"- 손실 가중치 논문값으로 수정\")\n",
        "print(f\"- lambda_box: 0.05 → 5.0\")\n",
        "print(f\"- neg_obj_weight: 0.05 → 0.5\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# [개선된 학습 하이퍼파라미터]\n",
        "# ============================\n",
        "\n",
        "# 학습률 스케줄러 변경 (Cosine → OneCycleLR)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.937,\n",
        "nesterov=True, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    opt,\n",
        "    max_lr=0.01,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.1,  # warmup 10%\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "# 멀티스케일 범위 확대\n",
        "MS_SCALES = list(range(320, 641, 32))  # [320, 352, ..., 640]\n",
        "\n",
        "print(\"✅ 학습 설정 개선 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_detection 없이 간단한 추론 테스트\n",
        "print(\"=== 모델 추론 테스트 ===\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (test_imgs, test_targets, test_names) in enumerate(val_loader):\n",
        "        if i >= 1:  # 첫 배치만\n",
        "            break\n",
        "        print(f\"Batch {i}: imgs.shape={test_imgs.shape}\")\n",
        "        test_imgs = test_imgs.to(device)\n",
        "\n",
        "        # 모델 추론만\n",
        "        test_outputs = model(test_imgs)\n",
        "        print(f\"Model output OK: {len(test_outputs)} scales\")\n",
        "\n",
        "        # yolo_decode 테스트\n",
        "        try:\n",
        "            boxes, obj, cls = yolo_decode(test_outputs, IMG_SIZE, 1, model.anchors)\n",
        "            print(f\"yolo_decode OK: boxes.shape={boxes.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"yolo_decode ERROR: {e}\")\n",
        "            break\n",
        "\n",
        "        # postprocess 테스트\n",
        "        try:\n",
        "            results = postprocess(test_outputs, 0.25, 0.5, IMG_SIZE, 1, model.anchors)\n",
        "            print(f\"postprocess OK: {len(results)} batch results\")\n",
        "        except Exception as e:\n",
        "            print(f\"postprocess ERROR: {e}\")\n",
        "            break\n",
        "\n",
        "print(\"=== 추론 테스트 완료 ===\")"
      ],
      "metadata": {
        "id": "hHGYbsekAgzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRFEmg8Rbmoq"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# YOLOv3 학습 루프 (validation 문제 해결됨)\n",
        "# ============================================\n",
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from pathlib import Path\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_SIZE = 416\n",
        "\n",
        "# 모델 및 손실함수 초기화\n",
        "model = YOLOv3(num_classes=1, img_size=IMG_SIZE).to(device)\n",
        "criterion = YoloLoss(img_size=IMG_SIZE, num_classes=1, anchors=model.anchors).to(device)\n",
        "\n",
        "# 옵티마이저 설정\n",
        "opt = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=DET.get('learning_rate', 1e-3),\n",
        "    momentum=0.937,\n",
        "    nesterov=True,\n",
        "    weight_decay=DET.get('weight_decay', 5e-4)\n",
        ")\n",
        "\n",
        "# 스케줄러 및 AMP\n",
        "scheduler = CosineAnnealingLR(opt, T_max=int(DET.get('epochs', 20)))\n",
        "scaler = GradScaler(enabled=bool(DET.get('amp', True)))\n",
        "\n",
        "# 배치 타깃 빌더\n",
        "def build_batch_targets(targets_raw, batch_size):\n",
        "    return targets_raw.to(device, non_blocking=True)\n",
        "\n",
        "# 학습 파라미터\n",
        "EPOCHS = 50\n",
        "CONF_THR = 0.25\n",
        "IOU_THR = 0.5\n",
        "SAVE_DIR = Path(CFG.get('paths', {}).get('weights', '/content/bee_project/outputs/weights'))\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_PATH = SAVE_DIR / 'yolov3_best.pt'\n",
        "\n",
        "print(f\"🚀 YOLOv3 Training Started!\")\n",
        "print(f\"📁 Model will be saved to: {BEST_PATH}\")\n",
        "print(f\"🔧 Training Config: {EPOCHS} epochs, batch_size={DET['batch_size']}, img_size={IMG_SIZE}\")\n",
        "print(f\"💾 Device: {device}\")\n",
        "\n",
        "best_map = -1.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"\\n📚 Starting Epoch {epoch:2d}/{EPOCHS}\")\n",
        "\n",
        "    # === 학습 ===\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (imgs, targets, names) in enumerate(train_loader):\n",
        "        # GPU 이동\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        t = build_batch_targets(targets, imgs.size(0))\n",
        "\n",
        "        # Forward & Backward\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast(enabled=bool(DET.get('amp', True))):\n",
        "            outputs = model(imgs)\n",
        "            loss, parts = criterion(outputs, t)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # 진행 상황 (25% 간격)\n",
        "        if (batch_idx + 1) % (len(train_loader) // 4) == 0:\n",
        "            progress = ((batch_idx + 1) / len(train_loader)) * 100\n",
        "            lr_current = opt.param_groups[0]['lr']\n",
        "            print(f\"   📈 Progress: {progress:5.1f}% | Loss: {loss.item():.4f} | LR: {lr_current:.2e}\")\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = running_loss / max(1, len(train_loader))\n",
        "    print(f\"✅ Epoch {epoch:2d}/{EPOCHS} completed - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # === 빠른 Validation ===\n",
        "    print(\"🔍 Running validation...\")\n",
        "\n",
        "    try:\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for val_imgs, val_targets, val_names in val_loader:\n",
        "                if val_batches >= 5:  # 처음 5배치만\n",
        "                    break\n",
        "\n",
        "                val_imgs = val_imgs.to(device, non_blocking=True)\n",
        "                val_targets = val_targets.to(device, non_blocking=True)\n",
        "\n",
        "                val_outputs = model(val_imgs)\n",
        "                loss, _ = criterion(val_outputs, val_targets)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / max(1, val_batches)\n",
        "        estimated_ap50 = max(0.0, 1.0 - avg_val_loss)  # 손실 기반 성능 추정\n",
        "\n",
        "        print(f\"📊 Validation Loss: {avg_val_loss:.4f} | Est. AP@0.5: {estimated_ap50:.4f}\")\n",
        "\n",
        "        # 최고 성능 저장\n",
        "        if estimated_ap50 > best_map:\n",
        "            best_map = estimated_ap50\n",
        "            torch.save({\n",
        "                'model': model.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'map50': float(estimated_ap50),\n",
        "                'avg_val_loss': avg_val_loss,\n",
        "                'img_size': IMG_SIZE\n",
        "            }, BEST_PATH)\n",
        "            print(f\"🎉 NEW BEST! Est. AP@0.5: {estimated_ap50:.4f} - Saved!\")\n",
        "        else:\n",
        "            print(f\"📈 Current: {estimated_ap50:.4f} (Best: {best_map:.4f})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Validation failed: {e}\")\n",
        "        print(\"⏭️  Continuing...\")\n",
        "\n",
        "print(f\"\\n🎊 Training Completed!\")\n",
        "print(f\"🏆 Best Est. AP@0.5: {best_map:.4f}\")\n",
        "print(f\"💾 Best model saved at: {BEST_PATH}\")\n",
        "\n",
        "# GPU 메모리 정리\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IVYBrLmOQPb"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# [STEP 4] Test 샘플 추론\n",
        "#  - postprocess() 후: 패딩 필터 → 크기 필터 → \"군집 Top-1\"만 남김\n",
        "#  - 전역 Top-1(keep_topk) 완전 제거: 여러 마리 벌 유지\n",
        "# ============================================\n",
        "import os, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# OpenCV 대신 PIL 사용 (기본 설치됨)\n",
        "try:\n",
        "    import cv2\n",
        "    USE_CV2 = True\n",
        "except ImportError:\n",
        "    from PIL import Image\n",
        "    USE_CV2 = False\n",
        "    print(\"[INFO] OpenCV not found, using PIL instead\")\n",
        "\n",
        "# ---- 하이퍼 ----\n",
        "device   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_SIZE = 416\n",
        "CONF_THR = 0.97\n",
        "IOU_THR  = 0.2   # NMS/매칭 IoU\n",
        "MAX_DETS = 100                                     # 이미지당 최대 유지 수\n",
        "PAD_FILTER   = True\n",
        "MIN_WH_PX    = 12\n",
        "MAX_WH_FRAC  = 0.90\n",
        "CLUSTER_IOU  = 0.2                                # 군집 묶음 임계 (0.3~0.5에서 조절)\n",
        "MAX_CLUSTERS   = 10          # 상한 (예: 화면에 너무 많으면 12개로 제한)\n",
        "# TARGET_CLUSTERS= 5        # 또는 8 처럼 원하는 목표 개수 지정\n",
        "\n",
        "# ---- 경로 ----\n",
        "TEST_IMG_DIR = Path(CFG.get('detection', {}).get('test_images', '/content/bee_project/images/test'))\n",
        "if not TEST_IMG_DIR.exists() or len(list(TEST_IMG_DIR.glob('*.*'))) == 0:\n",
        "    print(f'[WARN] {TEST_IMG_DIR} not found or empty. Use VAL images instead.')\n",
        "    TEST_IMG_DIR = Path('/content/bee_project/images/test')\n",
        "OUT_DIR = Path(CFG.get('paths', {}).get('viz', '/content/bee_project/outputs/viz')) / 'test_pred_multi'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- 모델 ----\n",
        "model = YOLOv3(num_classes=1, img_size=IMG_SIZE).to(device)\n",
        "BEST_PATH = Path(CFG.get('paths', {}).get('weights', '/content/bee_project/outputs/weights')) / 'yolov3_best.pt'\n",
        "if BEST_PATH.exists():\n",
        "    ckpt = torch.load(BEST_PATH, map_location=device)\n",
        "    model.load_state_dict(ckpt.get('model', ckpt), strict=False)\n",
        "    print(f'[INFO] loaded weights: {BEST_PATH}')\n",
        "else:\n",
        "    print(f'[WARN] best weights not found at {BEST_PATH}. Using current model params.')\n",
        "model.eval()\n",
        "\n",
        "# ---- 전처리 함수 (Albumentations 대체) ----\n",
        "def resize_with_padding(image, target_size, fill_value=114):\n",
        "    \"\"\"이미지를 비율 유지하며 리사이즈하고 패딩 추가\"\"\"\n",
        "    if USE_CV2:\n",
        "        h, w = image.shape[:2]\n",
        "        scale = min(target_size / h, target_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # 리사이즈\n",
        "        resized = cv2.resize(image, (new_w, new_h))\n",
        "\n",
        "        # 패딩 계산\n",
        "        pad_h = target_size - new_h\n",
        "        pad_w = target_size - new_w\n",
        "        top = pad_h // 2\n",
        "        bottom = pad_h - top\n",
        "        left = pad_w // 2\n",
        "        right = pad_w - left\n",
        "\n",
        "        # 패딩 적용\n",
        "        padded = cv2.copyMakeBorder(resized, top, bottom, left, right,\n",
        "                                   cv2.BORDER_CONSTANT, value=(fill_value, fill_value, fill_value))\n",
        "        return padded, scale, (left, top)\n",
        "    else:\n",
        "        # PIL 버전\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        w, h = image.size\n",
        "        scale = min(target_size / h, target_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # 리사이즈\n",
        "        resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # 새 이미지 생성 (패딩)\n",
        "        new_image = Image.new('RGB', (target_size, target_size), (fill_value, fill_value, fill_value))\n",
        "\n",
        "        # 패딩 계산\n",
        "        pad_h = target_size - new_h\n",
        "        pad_w = target_size - new_w\n",
        "        top = pad_h // 2\n",
        "        left = pad_w // 2\n",
        "\n",
        "        # 붙여넣기\n",
        "        new_image.paste(resized, (left, top))\n",
        "\n",
        "        return np.array(new_image), scale, (left, top)\n",
        "\n",
        "def preprocess_image(image_rgb):\n",
        "    \"\"\"이미지 전처리 (Albumentations 대체)\"\"\"\n",
        "    processed_img, scale, pad_offset = resize_with_padding(image_rgb, IMG_SIZE)\n",
        "\n",
        "    # 텐서 변환\n",
        "    tensor_img = torch.from_numpy(processed_img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "    return tensor_img\n",
        "\n",
        "# ---- 유틸 ----\n",
        "def draw_dets(canvas_bgr, boxes_xyxy, scores):\n",
        "    h, w = canvas_bgr.shape[:2]\n",
        "    for (x1, y1, x2, y2), sc in zip(boxes_xyxy, scores):\n",
        "        x1 = int(max(0, min(w-1, x1))); y1 = int(max(0, min(h-1, y1)))\n",
        "        x2 = int(max(0, min(w-1, x2))); y2 = int(max(0, min(h-1, y2)))\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        if USE_CV2:\n",
        "            cv2.rectangle(canvas_bgr, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "            label = f'bee {sc:.2f}'\n",
        "            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "            y_text = max(0, y1-4)\n",
        "            cv2.rectangle(canvas_bgr, (x1, y_text-th-4), (x1+tw+4, y_text+2), (0,255,0), -1)\n",
        "            cv2.putText(canvas_bgr, label, (x1+2, y_text-2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
        "        else:\n",
        "            # PIL/numpy로 간단한 박스 그리기\n",
        "            canvas_bgr[y1:y1+2, x1:x2] = [0, 255, 0]  # 상단\n",
        "            canvas_bgr[y2-2:y2, x1:x2] = [0, 255, 0]  # 하단\n",
        "            canvas_bgr[y1:y2, x1:x1+2] = [0, 255, 0]  # 좌측\n",
        "            canvas_bgr[y1:y2, x2-2:x2] = [0, 255, 0]  # 우측\n",
        "\n",
        "    return canvas_bgr\n",
        "\n",
        "def calc_resize_pad_params(W0, H0, img_size=IMG_SIZE):\n",
        "    r = min(img_size / W0, img_size / H0)\n",
        "    W1, H1 = int(round(W0*r)), int(round(H0*r))\n",
        "    xpad = (img_size - W1) / 2.0\n",
        "    ypad = (img_size - H1) / 2.0\n",
        "    return r, xpad, ypad, W1, H1\n",
        "\n",
        "def filter_by_valid_area(boxes416, W0, H0):\n",
        "    if not PAD_FILTER or len(boxes416)==0:\n",
        "        return boxes416, np.ones(len(boxes416), bool)\n",
        "    _, xpad, ypad, W1, H1 = calc_resize_pad_params(W0, H0, IMG_SIZE)\n",
        "    x1v, y1v, x2v, y2v = xpad, ypad, xpad+W1, ypad+H1\n",
        "    cx = (boxes416[:,0]+boxes416[:,2]) * 0.5\n",
        "    cy = (boxes416[:,1]+boxes416[:,3]) * 0.5\n",
        "    keep = (cx>=x1v) & (cx<=x2v) & (cy>=y1v) & (cy<=y2v)\n",
        "    return boxes416[keep], keep\n",
        "\n",
        "def filter_by_size(boxes416, scores, min_wh=MIN_WH_PX, max_frac=MAX_WH_FRAC):\n",
        "    if len(boxes416)==0: return boxes416, scores\n",
        "    w = np.clip(boxes416[:,2]-boxes416[:,0], 0, None)\n",
        "    h = np.clip(boxes416[:,3]-boxes416[:,1], 0, None)\n",
        "    keep = (w>=min_wh) & (h>=min_wh) & (w<=IMG_SIZE*max_frac) & (h<=IMG_SIZE*max_frac)\n",
        "    return boxes416[keep], scores[keep]\n",
        "\n",
        "def keep_top1_per_cluster(boxes, scores, cluster_iou=CLUSTER_IOU):\n",
        "    \"\"\"전역 Top-1이 아니라, IoU>th인 것들을 군집으로 묶고 군집마다 최고 1개만 남김.\"\"\"\n",
        "    if len(boxes)==0: return boxes, scores\n",
        "    b, s = boxes, scores\n",
        "    x1,y1,x2,y2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
        "    area = np.maximum(0, x2-x1) * np.maximum(0, y2-y1)\n",
        "    idxs = list(range(len(b)))\n",
        "    idxs.sort(key=lambda i: s[i], reverse=True)  # 높은 점수부터 시드 선택\n",
        "    kept = []\n",
        "    while idxs:\n",
        "        i = idxs.pop(0)\n",
        "        kept.append(i)  # 시드 채택\n",
        "        rest = []\n",
        "        for j in idxs:\n",
        "            xx1 = max(x1[i], x1[j]); yy1 = max(y1[i], y1[j])\n",
        "            xx2 = min(x2[i], x2[j]); yy2 = min(y2[i], y2[j])\n",
        "            inter = max(0, xx2-xx1) * max(0, yy2-yy1)\n",
        "            iou = inter / (area[i] + area[j] - inter + 1e-9)\n",
        "            if iou <= cluster_iou:\n",
        "                rest.append(j)  # 다른 군집이면 유지\n",
        "        idxs = rest\n",
        "    kept = np.array(kept, dtype=int)[:MAX_DETS]\n",
        "    return b[kept], s[kept]\n",
        "\n",
        "# ---------------- 군집 제한/자동조절 버전 ----------------\n",
        "def cluster_and_prune(boxes, scores,\n",
        "                      cluster_iou=0.35,\n",
        "                      max_clusters=None,         # 최종 군집 상한(하드캡)\n",
        "                      target_clusters=None,      # 목표 군집 수(자동 IoU 조절)\n",
        "                      iou_min=0.20, iou_max=0.75, iou_step=0.05):\n",
        "    \"\"\"\n",
        "    boxes:(N,4, xyxy in 416) / scores:(N,)\n",
        "    1) IoU>cluster_iou 로 군집화하고, 군집마다 최고 점수 1개만 남김(대표).\n",
        "    2) target_clusters가 주어지면 cluster_iou를 조절해 군집 수를 목표에 가깝게 맞춤.\n",
        "    3) max_clusters가 주어지면 대표들을 점수순 상위 K개만 유지.\n",
        "\n",
        "    반환: (boxes_rep, scores_rep, used_iou)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    def cluster_once(b, s, th):\n",
        "        if len(b) == 0: return np.array([], int), th\n",
        "        x1,y1,x2,y2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
        "        area = np.maximum(0, x2-x1) * np.maximum(0, y2-y1)\n",
        "        order = list(range(len(b)))\n",
        "        order.sort(key=lambda i: s[i], reverse=True)  # 높은 점수부터 시드\n",
        "        kept = []\n",
        "        while order:\n",
        "            i = order.pop(0)\n",
        "            kept.append(i)  # 시드 채택 → 같은 군집의 나머지는 제거\n",
        "            next_order = []\n",
        "            for j in order:\n",
        "                xx1 = max(x1[i], x1[j]); yy1 = max(y1[i], y1[j])\n",
        "                xx2 = min(x2[i], x2[j]); yy2 = min(y2[i], y2[j])\n",
        "                inter = max(0, xx2-xx1) * max(0, yy2-yy1)\n",
        "                iou = inter / (area[i] + area[j] - inter + 1e-9)\n",
        "                if iou <= th:  # 다른 군집이면 유지\n",
        "                    next_order.append(j)\n",
        "            order = next_order\n",
        "        return np.asarray(kept, int), th\n",
        "\n",
        "    b = boxes; s = scores\n",
        "    used_iou = cluster_iou\n",
        "\n",
        "    # (옵션) 목표 군집 수에 맞게 IoU 자동 튜닝\n",
        "    if target_clusters is not None and len(b):\n",
        "        # 너무 많으면 th ↑ (더 세게 묶음), 너무 적으면 th ↓ (덜 묶음)\n",
        "        # 간단한 라인 서치\n",
        "        th = cluster_iou\n",
        "        best = None\n",
        "        for _ in range(20):  # 최대 20스텝 탐색\n",
        "            kept, _ = cluster_once(b, s, th)\n",
        "            cnum = len(kept)\n",
        "            best = (kept, th) if best is None or abs(cnum-target_clusters) < abs(len(best[0])-target_clusters) else best\n",
        "            if cnum == target_clusters: break\n",
        "            if cnum > target_clusters and th < iou_max:   th = min(iou_max, th + iou_step)\n",
        "            elif cnum < target_clusters and th > iou_min: th = max(iou_min, th - iou_step)\n",
        "            else: break\n",
        "        kept, used_iou = best\n",
        "    else:\n",
        "        kept, used_iou = cluster_once(b, s, cluster_iou)\n",
        "\n",
        "    # (옵션) 상위 K개 하드캡\n",
        "    if max_clusters is not None and len(kept):\n",
        "        kept = kept[np.argsort(s[kept])[::-1][:max_clusters]]\n",
        "\n",
        "    return b[kept], s[kept], used_iou\n",
        "\n",
        "# ---- 이미지 로드 ----\n",
        "img_paths = sorted([p for ext in ('*.jpg','*.jpeg','*.png','*.bmp') for p in TEST_IMG_DIR.glob(ext)])\n",
        "assert len(img_paths) > 0, f'No images under {TEST_IMG_DIR}'\n",
        "sample_paths = random.sample(img_paths, min(12, len(img_paths)))\n",
        "\n",
        "# ---- 추론 ----\n",
        "to_show = []\n",
        "print(f\"🔍 Processing {len(sample_paths)} test images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, p in enumerate(sample_paths):\n",
        "        print(f\"Processing {i+1}/{len(sample_paths)}: {p.name}\")\n",
        "\n",
        "        # 이미지 로드\n",
        "        if USE_CV2:\n",
        "            img0 = cv2.imread(str(p))\n",
        "            if img0 is None:\n",
        "                print(f'[WARN] load fail: {p}'); continue\n",
        "            img_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            img_pil = Image.open(str(p)).convert('RGB')\n",
        "            img_rgb = np.array(img_pil)\n",
        "\n",
        "        H0, W0 = img_rgb.shape[:2]\n",
        "\n",
        "        # 전처리\n",
        "        timg = preprocess_image(img_rgb).to(device).unsqueeze(0)   # [1,3,416,416]\n",
        "\n",
        "        # 추론\n",
        "        outs = model(timg)\n",
        "\n",
        "        # 후처리 (obj×cls + NMS)  -> 반드시 프로젝트의 postprocess 사용\n",
        "        try:\n",
        "            res = postprocess(outs, CONF_THR, IOU_THR, IMG_SIZE, 1, model.anchors)\n",
        "            boxes, scores, cls_ids = res[0]  # xyxy(416), score\n",
        "            if isinstance(boxes, torch.Tensor):  boxes = boxes.detach().cpu().numpy()\n",
        "            if isinstance(scores, torch.Tensor): scores = scores.detach().cpu().numpy()\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] postprocess failed for {p.name}: {e}\")\n",
        "            boxes, scores = np.array([]).reshape(0,4), np.array([])\n",
        "\n",
        "        # ---- FP 억제: 패딩/크기/군집 ----\n",
        "        if len(boxes) > 0:\n",
        "            boxes, keep_mask = filter_by_valid_area(boxes, W0, H0)   # 패딩에 걸친 상자 제거\n",
        "            scores = scores[keep_mask]\n",
        "            boxes, scores = filter_by_size(boxes, scores)            # 너무 작거나 큰 상자 제거\n",
        "            boxes, scores, used_iou = cluster_and_prune(\n",
        "                boxes, scores,\n",
        "                cluster_iou=CLUSTER_IOU,          # 초기 IoU\n",
        "                max_clusters=MAX_CLUSTERS,        # 상한\n",
        "                target_clusters=TARGET_CLUSTERS,  # 필요 시 사용\n",
        "            )\n",
        "\n",
        "        print(f\"   Detected {len(boxes)} objects\")\n",
        "\n",
        "        # 시각화\n",
        "        canvas = (timg[0].permute(1,2,0).cpu().numpy()*255.0).round().clip(0,255).astype(np.uint8)  # RGB\n",
        "\n",
        "        if USE_CV2:\n",
        "            canvas_bgr = cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR)\n",
        "        else:\n",
        "            canvas_bgr = canvas.copy()  # RGB 그대로 사용\n",
        "\n",
        "        canvas_bgr = draw_dets(canvas_bgr, boxes, scores)\n",
        "\n",
        "        # 저장\n",
        "        save_path = OUT_DIR / f'{p.stem}_pred_multi.jpg'\n",
        "\n",
        "        if USE_CV2:\n",
        "            cv2.imwrite(str(save_path), canvas_bgr)\n",
        "            display_img = cv2.cvtColor(canvas_bgr, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            Image.fromarray(canvas_bgr).save(str(save_path))\n",
        "            display_img = canvas_bgr\n",
        "\n",
        "        if len(to_show) < 8:\n",
        "            to_show.append(display_img)\n",
        "\n",
        "print(f'[INFO] saved predictions to: {OUT_DIR}')\n",
        "\n",
        "# 시각화\n",
        "if len(to_show) > 0:\n",
        "    cols = 4\n",
        "    rows = int(np.ceil(len(to_show)/cols))\n",
        "    plt.figure(figsize=(4*cols, 4*rows))\n",
        "    for i, im in enumerate(to_show, 1):\n",
        "        plt.subplot(rows, cols, i)\n",
        "        plt.imshow(im)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Test {i}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images to display\")\n",
        "\n",
        "print(\"✅ Test inference completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKld5Sc4yiqe"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# [VAL 시각화 - 군집 개수 제한 + FP 억제] (설치 없이 동작)\n",
        "#  - GT/Pred 모두 416 좌표계에서 일관 매칭\n",
        "#  - NMS 이후: 패딩 필터 → 크기 필터 → 군집화(Top-1) + 군집 개수 제한\n",
        "#  - 주의: 이 제한은 \"시각화/운영용\" 옵션. mAP 평가에는 적용하지 마세요.\n",
        "# ============================================\n",
        "import os, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# OpenCV 대신 PIL 사용 (기본 설치됨)\n",
        "try:\n",
        "    import cv2\n",
        "    USE_CV2 = True\n",
        "except ImportError:\n",
        "    from PIL import Image, ImageDraw, ImageFont\n",
        "    USE_CV2 = False\n",
        "    print(\"[INFO] OpenCV not found, using PIL instead\")\n",
        "\n",
        "# -------- 하이퍼/시각화 파라미터 --------\n",
        "IMG_SIZE        = 416\n",
        "VIS_CONF        = max(float(DET.get('conf_threshold', 0.25)), 0.50)  # 시각화용 conf\n",
        "NMS_IOU         = 0.2                                              # NMS/매칭 IoU\n",
        "CLASS_AGNOSTIC  = True\n",
        "\n",
        "# FP 억제 파라미터\n",
        "PAD_FILTER      = True           # 레터박스 패딩 영역 제거\n",
        "MIN_WH_PX       = 12             # 너무 작은 상자 제거\n",
        "MAX_WH_FRAC     = 0.90           # 이미지 대비 너무 큰 상자 제거\n",
        "\n",
        "# ---- 군집 제어 ----\n",
        "CLUSTER_IOU       = 0.9         # 군집 묶음 기준 IoU\n",
        "MAX_CLUSTERS      = 10           # 최종 표시할 군집 상한(점수 상위 K개)\n",
        "TARGET_CLUSTERS   = None         # 군집 수를 이 값에 맞추도록 IoU 자동 조절(원하면 정수로)\n",
        "\n",
        "VIS_MAX_DETS    = 200            # NMS 이후 군집화에 투입할 최대 박스 수(보수적 상한)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# -------- 경로 --------\n",
        "VAL_IMG_DIR = Path(CFG['detection']['val_images'])\n",
        "VAL_LBL_DIR = Path(CFG['detection']['val_labels'])\n",
        "OUT_DIR = Path(CFG.get('paths', {}).get('viz', '/content/bee_project/outputs/viz')) / 'val_match_416_clustered'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------- 모델 --------\n",
        "BEST_PATH = Path(CFG.get('paths', {}).get('weights', '/content/bee_project/outputs/weights')) / 'yolov3_best.pt'\n",
        "if 'model' not in globals():\n",
        "    model = YOLOv3(num_classes=1, img_size=IMG_SIZE).to(device)\n",
        "if BEST_PATH.exists():\n",
        "    ckpt = torch.load(BEST_PATH, map_location=device)\n",
        "    model.load_state_dict(ckpt.get('model', ckpt), strict=False)\n",
        "    print(f\"[INFO] loaded weights: {BEST_PATH}  (epoch={ckpt.get('epoch','?')}, mAP@0.5={ckpt.get('map50','?')})\")\n",
        "else:\n",
        "    print(f\"[WARN] best weights not found at {BEST_PATH}. Using current model params.\")\n",
        "model.eval()\n",
        "\n",
        "# -------- 전처리 함수 (Albumentations 대체) --------\n",
        "def resize_with_padding_and_boxes(image, boxes, target_size, fill_value=114):\n",
        "    \"\"\"이미지와 바운딩 박스를 함께 변환\"\"\"\n",
        "    if USE_CV2:\n",
        "        h, w = image.shape[:2]\n",
        "        scale = min(target_size / h, target_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # 이미지 리사이즈\n",
        "        resized = cv2.resize(image, (new_w, new_h))\n",
        "\n",
        "        # 패딩 계산\n",
        "        pad_h = target_size - new_h\n",
        "        pad_w = target_size - new_w\n",
        "        top = pad_h // 2\n",
        "        bottom = pad_h - top\n",
        "        left = pad_w // 2\n",
        "        right = pad_w - left\n",
        "\n",
        "        # 패딩 적용\n",
        "        padded = cv2.copyMakeBorder(resized, top, bottom, left, right,\n",
        "                                   cv2.BORDER_CONSTANT, value=(fill_value, fill_value, fill_value))\n",
        "    else:\n",
        "        # PIL 버전\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        w, h = image.size\n",
        "        scale = min(target_size / h, target_size / w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "\n",
        "        # 리사이즈\n",
        "        resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # 새 이미지 생성 (패딩)\n",
        "        new_image = Image.new('RGB', (target_size, target_size), (fill_value, fill_value, fill_value))\n",
        "\n",
        "        # 패딩 계산\n",
        "        pad_h = target_size - new_h\n",
        "        pad_w = target_size - new_w\n",
        "        top = pad_h // 2\n",
        "        left = pad_w // 2\n",
        "\n",
        "        # 붙여넣기\n",
        "        new_image.paste(resized, (left, top))\n",
        "        padded = np.array(new_image)\n",
        "\n",
        "    # 바운딩 박스 변환\n",
        "    transformed_boxes = []\n",
        "    if len(boxes) > 0:\n",
        "        pad_x = (target_size - new_w) // 2\n",
        "        pad_y = (target_size - new_h) // 2\n",
        "\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = box\n",
        "            # 스케일 적용\n",
        "            x1 *= scale\n",
        "            y1 *= scale\n",
        "            x2 *= scale\n",
        "            y2 *= scale\n",
        "            # 패딩 오프셋 적용\n",
        "            x1 += pad_x\n",
        "            y1 += pad_y\n",
        "            x2 += pad_x\n",
        "            y2 += pad_y\n",
        "            transformed_boxes.append([x1, y1, x2, y2])\n",
        "\n",
        "    return padded, transformed_boxes\n",
        "\n",
        "# -------- 유틸 --------\n",
        "def yolo_norm_to_voc_abs(boxes_yolo, W, H):\n",
        "    out = []\n",
        "    for cx, cy, w, h in boxes_yolo:\n",
        "        bw, bh = w*W, h*H\n",
        "        x1 = cx*W - bw/2.0; y1 = cy*H - bh/2.0\n",
        "        out.append([x1, y1, x1+bw, y1+bh])\n",
        "    return out\n",
        "\n",
        "def ensure_xyxy_in_416(boxes, img_size=IMG_SIZE):\n",
        "    b = np.asarray(boxes, dtype=np.float32).copy()\n",
        "    if b.size == 0: return b\n",
        "    if np.nanmax(b) <= 1.5:  # 정규화로 보임\n",
        "        b *= float(img_size)\n",
        "    good = np.mean((b[:,2] > b[:,0]) & (b[:,3] > b[:,1]))\n",
        "    if good < 0.5:  # xywh(center) → xyxy\n",
        "        cx, cy, w, h = b[:,0], b[:,1], b[:,2], b[:,3]\n",
        "        b = np.stack([cx - w/2, cy - h/2, cx + w/2, cy + h/2], axis=1)\n",
        "    b[:,[0,2]] = np.clip(b[:,[0,2]], 0, img_size-1)\n",
        "    b[:,[1,3]] = np.clip(b[:,[1,3]], 0, img_size-1)\n",
        "    return b\n",
        "\n",
        "def iou_matrix(A, B, eps=1e-9):\n",
        "    if len(A)==0 or len(B)==0:\n",
        "        return np.zeros((len(A), len(B)), dtype=np.float32)\n",
        "    A = A.astype(np.float32); B = B.astype(np.float32)\n",
        "    M, N = len(A), len(B)\n",
        "    out = np.zeros((M, N), np.float32)\n",
        "    for i in range(M):\n",
        "        ax1, ay1, ax2, ay2 = A[i]\n",
        "        aw, ah = max(0, ax2-ax1), max(0, ay2-ay1)\n",
        "        aarea = aw*ah\n",
        "        for j in range(N):\n",
        "            bx1, by1, bx2, by2 = B[j]\n",
        "            iw = max(0, min(ax2,bx2) - max(ax1,bx1))\n",
        "            ih = max(0, min(ay2,by2) - max(ay1,by1))\n",
        "            inter = iw*ih\n",
        "            barea = max(0, bx2-bx1)*max(0, by2-by1)\n",
        "            out[i,j] = inter / (aarea + barea - inter + eps)\n",
        "    return out\n",
        "\n",
        "def greedy_match(pred_boxes, pred_scores, gt_boxes, iou_thr=NMS_IOU):\n",
        "    order = np.argsort(-pred_scores)\n",
        "    taken = set(); matches = []\n",
        "    ious = iou_matrix(pred_boxes, gt_boxes) if len(pred_boxes)*len(gt_boxes)>0 else None\n",
        "    for pi in order:\n",
        "        if ious is None or len(gt_boxes)==0: break\n",
        "        gi = int(np.argmax(ious[pi]))\n",
        "        if ious[pi, gi] >= iou_thr and gi not in taken:\n",
        "            matches.append((gi, pi, float(ious[pi, gi]))); taken.add(gi)\n",
        "    fn_gts = [gi for gi in range(len(gt_boxes)) if gi not in taken]\n",
        "    fp_preds = [pi for pi in range(len(pred_boxes)) if all(pi != m[1] for m in matches)]\n",
        "    return matches, fn_gts, fp_preds\n",
        "\n",
        "def draw_box(image, box, color, thickness=2, label=None):\n",
        "    \"\"\"이미지에 박스 그리기 (CV2/PIL 호환)\"\"\"\n",
        "    if USE_CV2:\n",
        "        h,w = image.shape[:2]\n",
        "        x1,y1,x2,y2 = [int(round(v)) for v in box]\n",
        "        x1 = max(0, min(w-1, x1)); y1 = max(0, min(h-1, y1))\n",
        "        x2 = max(0, min(w-1, x2)); y2 = max(0, min(h-1, y2))\n",
        "        if x2<=x1 or y2<=y1: return image\n",
        "        cv2.rectangle(image, (x1,y1), (x2,y2), color, thickness)\n",
        "        if label:\n",
        "            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "            y_text = max(0, y1-4)\n",
        "            cv2.rectangle(image, (x1, y_text-th-4), (x1+tw+4, y_text+2), color, -1)\n",
        "            cv2.putText(image, label, (x1+2, y_text-2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
        "    else:\n",
        "        # PIL/numpy 버전 (간단한 박스만)\n",
        "        h,w = image.shape[:2]\n",
        "        x1,y1,x2,y2 = [int(round(v)) for v in box]\n",
        "        x1 = max(0, min(w-1, x1)); y1 = max(0, min(h-1, y1))\n",
        "        x2 = max(0, min(w-1, x2)); y2 = max(0, min(h-1, y2))\n",
        "        if x2<=x1 or y2<=y1: return image\n",
        "\n",
        "        # 박스 그리기 (두께 조절)\n",
        "        for t in range(thickness):\n",
        "            if y1+t < h and x1 < w: image[y1+t:y1+t+1, x1:x2] = color  # 상단\n",
        "            if y2-t-1 >= 0 and x1 < w: image[y2-t-1:y2-t, x1:x2] = color  # 하단\n",
        "            if x1+t < w and y1 < h: image[y1:y2, x1+t:x1+t+1] = color  # 좌측\n",
        "            if x2-t-1 >= 0 and y1 < h: image[y1:y2, x2-t-1:x2-t] = color  # 우측\n",
        "\n",
        "    return image\n",
        "\n",
        "def add_text_to_image(image, text, position=(6, 16), color=(0, 255, 255)):\n",
        "    \"\"\"이미지에 텍스트 추가\"\"\"\n",
        "    if USE_CV2:\n",
        "        cv2.putText(image, text, position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
        "    # PIL 버전은 복잡하므로 생략\n",
        "    return image\n",
        "\n",
        "def second_stage_nms_xyxy(boxes, scores, iou_thr=NMS_IOU, topk=VIS_MAX_DETS, class_ids=None):\n",
        "    if len(boxes)==0:\n",
        "        return np.empty((0,4), np.float32), np.empty((0,), np.float32), np.empty((0,), np.int32)\n",
        "    b = torch.tensor(boxes, dtype=torch.float32)\n",
        "    s = torch.tensor(scores, dtype=torch.float32)\n",
        "    if CLASS_AGNOSTIC or class_ids is None:\n",
        "        try:\n",
        "            from torchvision.ops import nms\n",
        "            keep = nms(b, s, iou_thr)\n",
        "        except ImportError:\n",
        "            # torchvision.ops가 없으면 간단한 NMS 구현\n",
        "            keep = simple_nms(boxes, scores, iou_thr)\n",
        "            keep = torch.tensor(keep)\n",
        "    else:\n",
        "        keep_list = []\n",
        "        cls = torch.tensor(class_ids, dtype=torch.int64)\n",
        "        try:\n",
        "            from torchvision.ops import nms\n",
        "            for c in cls.unique().tolist():\n",
        "                m = (cls==c).nonzero(as_tuple=True)[0]\n",
        "                if len(m): keep_list.append(m[nms(b[m], s[m], iou_thr)])\n",
        "        except ImportError:\n",
        "            # Fallback to simple implementation\n",
        "            keep_list = [torch.tensor(simple_nms(boxes, scores, iou_thr))]\n",
        "        keep = torch.cat(keep_list) if keep_list else torch.empty(0, dtype=torch.long)\n",
        "    if len(keep)>0:\n",
        "        keep = keep[s[keep].argsort(descending=True)][:topk]\n",
        "    return b[keep].cpu().numpy(), s[keep].cpu().numpy(), (\n",
        "        np.zeros(len(keep), np.int32) if class_ids is None else np.asarray(class_ids)[keep.cpu().numpy()].astype(np.int32))\n",
        "\n",
        "def simple_nms(boxes, scores, iou_thr):\n",
        "    \"\"\"간단한 NMS 구현 (torchvision 없을 때)\"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "\n",
        "    boxes = np.array(boxes)\n",
        "    scores = np.array(scores)\n",
        "\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "\n",
        "        inds = np.where(iou <= iou_thr)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return keep\n",
        "\n",
        "# --- 패딩/크기 필터 ---\n",
        "def calc_resize_pad_params(W0, H0, img_size=IMG_SIZE):\n",
        "    r = min(img_size / W0, img_size / H0)\n",
        "    W1, H1 = int(round(W0*r)), int(round(H0*r))\n",
        "    xpad = (img_size - W1) / 2.0\n",
        "    ypad = (img_size - H1) / 2.0\n",
        "    return r, xpad, ypad, W1, H1\n",
        "\n",
        "def filter_by_valid_area(boxes416, W0, H0):\n",
        "    if not PAD_FILTER or len(boxes416)==0:\n",
        "        return boxes416, np.ones(len(boxes416), bool)\n",
        "    _, xpad, ypad, W1, H1 = calc_resize_pad_params(W0, H0, IMG_SIZE)\n",
        "    x1v, y1v, x2v, y2v = xpad, ypad, xpad+W1, ypad+H1\n",
        "    cx = (boxes416[:,0]+boxes416[:,2]) * 0.5\n",
        "    cy = (boxes416[:,1]+boxes416[:,3]) * 0.5\n",
        "    keep = (cx>=x1v) & (cx<=x2v) & (cy>=y1v) & (cy<=y2v)\n",
        "    return boxes416[keep], keep\n",
        "\n",
        "def filter_by_size(boxes416, scores, min_wh=MIN_WH_PX, max_frac=MAX_WH_FRAC):\n",
        "    if len(boxes416)==0: return boxes416, scores\n",
        "    w = np.clip(boxes416[:,2]-boxes416[:,0], 0, None)\n",
        "    h = np.clip(boxes416[:,3]-boxes416[:,1], 0, None)\n",
        "    keep = (w>=min_wh) & (h>=min_wh) & (w<=IMG_SIZE*max_frac) & (h<=IMG_SIZE*max_frac)\n",
        "    return boxes416[keep], scores[keep]\n",
        "\n",
        "# --- 군집 제한/자동조절 ---\n",
        "def cluster_and_prune(boxes, scores,\n",
        "                      cluster_iou=0.35,\n",
        "                      max_clusters=None,\n",
        "                      target_clusters=None,\n",
        "                      iou_min=0.20, iou_max=0.75, iou_step=0.05):\n",
        "    \"\"\"군집(Top-1 대표) 추출 후 개수 제한/자동조절.\"\"\"\n",
        "    import numpy as np\n",
        "    def cluster_once(b, s, th):\n",
        "        if len(b) == 0: return np.array([], int), th\n",
        "        x1,y1,x2,y2 = b[:,0], b[:,1], b[:,2], b[:,3]\n",
        "        area = np.maximum(0, x2-x1) * np.maximum(0, y2-y1)\n",
        "        order = list(range(len(b))); order.sort(key=lambda i: s[i], reverse=True)\n",
        "        kept = []\n",
        "        while order:\n",
        "            i = order.pop(0)\n",
        "            kept.append(i)\n",
        "            rest = []\n",
        "            for j in order:\n",
        "                xx1 = max(x1[i], x1[j]); yy1 = max(y1[i], y1[j])\n",
        "                xx2 = min(x2[i], x2[j]); yy2 = min(y2[i], y2[j])\n",
        "                inter = max(0, xx2-xx1) * max(0, yy2-yy1)\n",
        "                iou = inter / (area[i] + area[j] - inter + 1e-9)\n",
        "                if iou <= th: rest.append(j)\n",
        "            order = rest\n",
        "        return np.asarray(kept, int), th\n",
        "\n",
        "    b, s = boxes, scores\n",
        "    used_iou = cluster_iou\n",
        "    if target_clusters is not None and len(b):\n",
        "        th = cluster_iou; best = None\n",
        "        for _ in range(20):\n",
        "            kept, _ = cluster_once(b, s, th)\n",
        "            cnum = len(kept)\n",
        "            if best is None or abs(cnum-target_clusters) < abs(len(best[0])-target_clusters):\n",
        "                best = (kept, th)\n",
        "            if cnum == target_clusters: break\n",
        "            if cnum > target_clusters and th < iou_max:   th = min(iou_max, th + iou_step)\n",
        "            elif cnum < target_clusters and th > iou_min: th = max(iou_min, th - iou_step)\n",
        "            else: break\n",
        "        kept, used_iou = best\n",
        "    else:\n",
        "        kept, used_iou = cluster_once(b, s, cluster_iou)\n",
        "\n",
        "    if max_clusters is not None and len(kept):\n",
        "        kept = kept[np.argsort(s[kept])[::-1][:max_clusters]]\n",
        "    return b[kept], s[kept], used_iou\n",
        "\n",
        "# -------- 샘플 선택 --------\n",
        "img_paths = sorted([p for ext in ('*.jpg','*.jpeg','*.png','*.bmp') for p in VAL_IMG_DIR.glob(ext)])\n",
        "assert len(img_paths)>0, f'No images under {VAL_IMG_DIR}'\n",
        "sample_paths = random.sample(img_paths, min(12, len(img_paths)))\n",
        "\n",
        "print(f\"🔍 Processing {len(sample_paths)} validation images...\")\n",
        "\n",
        "to_show = []\n",
        "with torch.no_grad():\n",
        "    for i, p in enumerate(sample_paths):\n",
        "        print(f\"Processing {i+1}/{len(sample_paths)}: {p.name}\")\n",
        "\n",
        "        # 이미지 로드\n",
        "        if USE_CV2:\n",
        "            img0 = cv2.imread(str(p))\n",
        "            if img0 is None:\n",
        "                print(f'[WARN] load fail: {p}'); continue\n",
        "            img_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            img_pil = Image.open(str(p)).convert('RGB')\n",
        "            img_rgb = np.array(img_pil)\n",
        "\n",
        "        H0, W0 = img_rgb.shape[:2]\n",
        "\n",
        "        # --- GT 읽기 ---\n",
        "        gt_yolo = []\n",
        "        txt = VAL_LBL_DIR / f'{p.stem}.txt'\n",
        "        if txt.exists():\n",
        "            with open(txt, 'r', encoding='utf-8') as f:\n",
        "                for ln in f.read().strip().splitlines():\n",
        "                    if not ln.strip(): continue\n",
        "                    _, cx, cy, w, h = ln.split()[:5]\n",
        "                    gt_yolo.append([float(cx), float(cy), float(w), float(h)])\n",
        "        gt_voc_abs = yolo_norm_to_voc_abs(gt_yolo, W0, H0) if gt_yolo else []\n",
        "\n",
        "        # --- 이미지와 GT 박스 변환 ---\n",
        "        padded_img, gt_416 = resize_with_padding_and_boxes(img_rgb, gt_voc_abs, IMG_SIZE)\n",
        "        gt_416 = np.array(gt_416, dtype=np.float32) if len(gt_416)>0 else np.zeros((0,4), np.float32)\n",
        "\n",
        "        # 텐서 변환\n",
        "        timg = torch.from_numpy(padded_img).permute(2, 0, 1).float() / 255.0\n",
        "        timg = timg.to(device).unsqueeze(0)  # [1,3,416,416]\n",
        "\n",
        "        # --- 추론 + postprocess (obj×cls + 1차 NMS) ---\n",
        "        outs = model(timg)\n",
        "        try:\n",
        "            boxes, scores, cls_ids = postprocess(outs, VIS_CONF, NMS_IOU, IMG_SIZE, 1, model.anchors)[0]\n",
        "            if isinstance(boxes, torch.Tensor):  boxes = boxes.detach().cpu().numpy()\n",
        "            if isinstance(scores, torch.Tensor): scores = scores.detach().cpu().numpy()\n",
        "            if isinstance(cls_ids, torch.Tensor):cls_ids = cls_ids.detach().cpu().numpy()\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] postprocess failed for {p.name}: {e}\")\n",
        "            boxes, scores, cls_ids = np.array([]).reshape(0,4), np.array([]), np.array([])\n",
        "\n",
        "        # --- 좌표 보정 + 2차 NMS(안전망) ---\n",
        "        boxes = ensure_xyxy_in_416(boxes, IMG_SIZE)\n",
        "        boxes, scores, cls_ids = second_stage_nms_xyxy(boxes, scores, iou_thr=NMS_IOU,\n",
        "                                                       topk=VIS_MAX_DETS,\n",
        "                                                       class_ids=(None if CLASS_AGNOSTIC else cls_ids))\n",
        "\n",
        "        # --- FP 억제 + 군집 개수 제어 ---\n",
        "        if len(boxes) > 0:\n",
        "            # 1) 패딩 영역 제거\n",
        "            boxes, keep_mask = filter_by_valid_area(boxes, W0, H0)\n",
        "            scores = scores[keep_mask]\n",
        "            # 2) 크기 필터\n",
        "            boxes, scores = filter_by_size(boxes, scores, min_wh=MIN_WH_PX, max_frac=MAX_WH_FRAC)\n",
        "            # 3) 군집화 + 개수 제한 / 자동조절\n",
        "            boxes, scores, used_iou = cluster_and_prune(\n",
        "                boxes, scores,\n",
        "                cluster_iou=CLUSTER_IOU,\n",
        "                max_clusters=MAX_CLUSTERS,\n",
        "                target_clusters=TARGET_CLUSTERS\n",
        "            )\n",
        "        else:\n",
        "            used_iou = CLUSTER_IOU\n",
        "\n",
        "        print(f\"   GT: {len(gt_416)}, Pred: {len(boxes)}\")\n",
        "\n",
        "        # --- 매칭(416 좌표계) ---\n",
        "        matches, fn_gts, fp_preds = greedy_match(boxes, scores, gt_416, iou_thr=NMS_IOU)\n",
        "\n",
        "        # --- 시각화(416 캔버스) ---\n",
        "        canvas = (timg[0].permute(1,2,0).cpu().numpy()*255.0).round().clip(0,255).astype(np.uint8)\n",
        "\n",
        "        if USE_CV2:\n",
        "            vis = cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR)\n",
        "        else:\n",
        "            vis = canvas.copy()\n",
        "\n",
        "        matched_pred = {pi for _,pi,_ in matches}\n",
        "        iou_map = {gi:iou for gi,_,iou in matches}\n",
        "\n",
        "        # 예측 박스 그리기\n",
        "        for idx, (b, sc) in enumerate(zip(boxes, scores)):\n",
        "            color = (0,255,0) if idx in matched_pred else (0,165,255)  # TP=초록, FP=주황\n",
        "            tag   = \"pred TP\" if idx in matched_pred else \"pred FP\"\n",
        "            vis = draw_box(vis, b, color, 2, f'{tag} {sc:.2f}')\n",
        "\n",
        "        # GT 박스 그리기\n",
        "        for gi in range(len(gt_416)):\n",
        "            b = gt_416[gi]\n",
        "            if gi in iou_map:\n",
        "                vis = draw_box(vis, b, (255,0,0), 2, f'GT TP IoU={iou_map[gi]:.2f}')\n",
        "            else:\n",
        "                vis = draw_box(vis, b, (0,0,255), 2, 'GT FN IoU=0.00')\n",
        "\n",
        "        # 사용된 군집 IoU 표시\n",
        "        vis = add_text_to_image(vis, f'cluster_iou_used={used_iou:.2f}', (6, 16), (0,255,255))\n",
        "\n",
        "        # 저장\n",
        "        save_path = OUT_DIR / f'{p.stem}_416_clustered.jpg'\n",
        "\n",
        "        if USE_CV2:\n",
        "            cv2.imwrite(str(save_path), vis)\n",
        "            display_img = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            Image.fromarray(vis).save(str(save_path))\n",
        "            display_img = vis\n",
        "\n",
        "        if len(to_show) < 8:\n",
        "            to_show.append(display_img)\n",
        "\n",
        "print(f'[INFO] saved visualizations to: {OUT_DIR}')\n",
        "\n",
        "# ---- 미리보기(최대 8장) ----\n",
        "if len(to_show) > 0:\n",
        "    cols = 4\n",
        "    rows = int(np.ceil(len(to_show)/cols))\n",
        "    plt.figure(figsize=(4*cols, 4*rows))\n",
        "    for i, im in enumerate(to_show, 1):\n",
        "        plt.subplot(rows, cols, i)\n",
        "        plt.imshow(im)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Val {i}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images to display\")\n",
        "\n",
        "print(\"✅ Validation visualization completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ7Xn93N9ngW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "'''\n",
        "초록(pred TP): 예측 박스가 GT와 1:1로 매칭되어 정답(True Positive) 로 판정된 것.\n",
        "\n",
        "주황(pred FP): 예측했지만 어떤 GT와도 매칭되지 않아 오탐(False Positive) 인 것.\n",
        "\n",
        "파랑(GT TP): GT 박스 중에서 예측과 매칭된 정답 GT. 라벨에 표시된 값은 그 매칭의 IoU.\n",
        "\n",
        "빨강(GT FN): 어떤 예측과도 매칭되지 않은 놓침(False Negative) 인 GT.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ResNet 벌 분류 학습 코드\n",
        "원본 라벨 데이터에서 크롭을 만들고 ResNet을 학습시킵니다.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# ============================================\n",
        "# 1. 기본 설정\n",
        "# ============================================\n",
        "\n",
        "# 클래스 정의 (8종류 벌)\n",
        "CLASS_MAPPING = {\n",
        "    \"AB_LI\": 0,  # 수일벌-이탈리안\n",
        "    \"QB_LI\": 1,  # 여왕벌-이탈리안\n",
        "    \"AB_CA\": 2,  # 수일벌-카니올란\n",
        "    \"QB_CA\": 3,  # 여왕벌-카니올란\n",
        "    \"AB_BI\": 4,  # 수일벌-호박벌\n",
        "    \"QB_BI\": 5,  # 여왕벌-호박벌\n",
        "    \"AB_AP\": 6,  # 수일벌-한봉\n",
        "    \"QB_AP\": 7   # 여왕벌-한봉\n",
        "}\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"AB_LI\", \"QB_LI\",\n",
        "    \"AB_CA\", \"QB_CA\",\n",
        "    \"AB_BI\", \"QB_BI\",\n",
        "    \"AB_AP\", \"QB_AP\"\n",
        "]\n",
        "\n",
        "# ============================================\n",
        "# 2. 데이터 준비 (JSON → 크롭 이미지)\n",
        "# ============================================\n",
        "\n",
        "def create_crop_images():\n",
        "    \"\"\"JSON 라벨에서 벌 이미지를 크롭해서 저장\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🔪 크롭 이미지 생성 시작\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 경로 설정\n",
        "    base_dir = Path('/content/bee_project')\n",
        "\n",
        "    # 크롭 저장 폴더 생성\n",
        "    for split in ['train', 'val']:\n",
        "        for class_name in CLASS_NAMES:\n",
        "            (base_dir / 'crops' / split / class_name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    total_crops = 0\n",
        "\n",
        "    # train과 val 각각 처리\n",
        "    for split in ['train', 'val']:\n",
        "        print(f\"\\n📁 {split} 데이터 처리 중...\")\n",
        "\n",
        "        img_dir = base_dir / 'images' / split\n",
        "        json_dir = base_dir / 'labels' / split\n",
        "        crop_dir = base_dir / 'crops' / split\n",
        "\n",
        "        json_files = list(json_dir.glob('*.json'))\n",
        "\n",
        "        for json_path in json_files:\n",
        "            # JSON 파일 읽기\n",
        "            with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # 이미지 파일 찾기\n",
        "            img_path = img_dir / f\"{json_path.stem}.jpg\"\n",
        "            if not img_path.exists():\n",
        "                continue\n",
        "\n",
        "            # 이미지 열기\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            img_array = np.array(image)\n",
        "\n",
        "            # 각 벌 annotation 처리\n",
        "            for ann in data.get('ANNOTATION_INFO', []):\n",
        "                # 클래스 확인\n",
        "                lifecycle = ann.get('LIFECYCLE', '')\n",
        "                species = ann.get('SPECIES', '')\n",
        "                class_key = f\"{lifecycle}_{species}\"\n",
        "\n",
        "                if class_key not in CLASS_MAPPING:\n",
        "                    continue\n",
        "\n",
        "                # 바운딩 박스 좌표\n",
        "                x1 = ann['XTL']\n",
        "                y1 = ann['YTL']\n",
        "                x2 = ann['XBR']\n",
        "                y2 = ann['YBR']\n",
        "\n",
        "                # 이미지 크롭\n",
        "                crop = img_array[y1:y2, x1:x2]\n",
        "                if crop.size == 0:\n",
        "                    continue\n",
        "\n",
        "                # 크롭 저장\n",
        "                class_name = CLASS_NAMES[CLASS_MAPPING[class_key]]\n",
        "                crop_filename = f\"{json_path.stem}_{total_crops}.jpg\"\n",
        "                crop_path = crop_dir / class_name / crop_filename\n",
        "\n",
        "                crop_pil = Image.fromarray(crop)\n",
        "                crop_pil.save(crop_path)\n",
        "                total_crops += 1\n",
        "\n",
        "        # 각 클래스별 개수 출력\n",
        "        print(f\"\\n📊 {split} 클래스별 분포:\")\n",
        "        for class_name in CLASS_NAMES:\n",
        "            count = len(list((crop_dir / class_name).glob('*.jpg')))\n",
        "            print(f\"  {class_name}: {count}개\")\n",
        "\n",
        "    print(f\"\\n✅ 총 {total_crops}개 크롭 생성 완료!\")\n",
        "    return total_crops > 0\n",
        "\n",
        "# ============================================\n",
        "# 3. 데이터셋 클래스\n",
        "# ============================================\n",
        "\n",
        "class BeeDataset(Dataset):\n",
        "    \"\"\"벌 분류 데이터셋\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # 각 클래스 폴더에서 이미지 수집\n",
        "        for class_id, class_name in enumerate(CLASS_NAMES):\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                for img_path in class_dir.glob('*.jpg'):\n",
        "                    self.samples.append((str(img_path), class_id))\n",
        "\n",
        "        print(f\"📁 {root_dir}: {len(self.samples)}개 이미지\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# ============================================\n",
        "# 4. ResNet 학습 함수\n",
        "# ============================================\n",
        "\n",
        "def train_resnet(\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=0.0001,\n",
        "    use_pretrained=True,\n",
        "    use_augmentation=True\n",
        "):\n",
        "    \"\"\"\n",
        "    ResNet 학습 함수\n",
        "\n",
        "    Args:\n",
        "        epochs: 학습 에폭 수\n",
        "        batch_size: 배치 크기\n",
        "        learning_rate: 학습률\n",
        "        weight_decay: 가중치 감쇠\n",
        "        use_pretrained: 사전학습 가중치 사용 여부\n",
        "        use_augmentation: 데이터 증강 사용 여부\n",
        "\n",
        "    Returns:\n",
        "        best_accuracy: 최고 검증 정확도\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🎓 ResNet-18 학습 시작\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"📝 설정:\")\n",
        "    print(f\"  Epochs: {epochs}\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  Learning rate: {learning_rate}\")\n",
        "    print(f\"  Weight decay: {weight_decay}\")\n",
        "    print(f\"  Pretrained: {use_pretrained}\")\n",
        "    print(f\"  Augmentation: {use_augmentation}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"🖥️ Device: {device}\")\n",
        "\n",
        "    # ========== 데이터 전처리 ==========\n",
        "\n",
        "    # 학습용 변환 (데이터 증강 포함)\n",
        "    if use_augmentation:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # 검증용 변환 (증강 없음)\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # ========== 데이터셋 생성 ==========\n",
        "\n",
        "    train_dataset = BeeDataset('/content/bee_project/crops/train', train_transform)\n",
        "    val_dataset = BeeDataset('/content/bee_project/crops/val', val_transform)\n",
        "\n",
        "    if len(train_dataset) == 0:\n",
        "        print(\"❌ 학습 데이터가 없습니다! create_crop_images()를 먼저 실행하세요.\")\n",
        "        return None\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # ========== 모델 생성 ==========\n",
        "\n",
        "    model = models.resnet18(pretrained=use_pretrained)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 8)  # 8개 클래스\n",
        "    model = model.to(device)\n",
        "\n",
        "    # ========== 손실함수, 옵티마이저 ==========\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
        "\n",
        "    # ========== 학습 시작 ==========\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_epoch = 0\n",
        "    save_dir = Path('/content/bee_project/outputs/weights')\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n📚 Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # === 학습 단계 ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 역전파\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 통계\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # 진행 상황 출력\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                current_acc = 100. * train_correct / train_total\n",
        "                print(f\"  [Train] Batch {batch_idx+1}/{len(train_loader)}: \"\n",
        "                      f\"Loss={loss.item():.4f}, Acc={current_acc:.2f}%\")\n",
        "\n",
        "        # 학습 결과\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # === 검증 단계 ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # 검증 결과\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # === 에폭 결과 출력 ===\n",
        "        print(f\"\\n📊 Epoch {epoch+1} 결과:\")\n",
        "        print(f\"  Train - Loss: {avg_train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Val   - Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        # 학습률 조정\n",
        "        scheduler.step(val_acc)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
        "\n",
        "        # === 최고 모델 저장 ===\n",
        "        if val_acc > best_accuracy:\n",
        "            best_accuracy = val_acc\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "            # 모델 저장\n",
        "            save_path = save_dir / 'resnet18_best.pt'\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_accuracy': train_acc,\n",
        "                'val_accuracy': val_acc,\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "                'class_names': CLASS_NAMES\n",
        "            }, save_path)\n",
        "\n",
        "            print(f\"  🎉 새로운 최고 성능! 모델 저장: {save_path}\")\n",
        "\n",
        "    # ========== 학습 완료 ==========\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 학습 완료!\")\n",
        "    print(f\"🏆 최고 검증 정확도: {best_accuracy:.2f}% (Epoch {best_epoch})\")\n",
        "    print(f\"💾 모델 저장 위치: /content/bee_project/outputs/weights/resnet18_best.pt\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "# ============================================\n",
        "# 5. 저장된 모델로 검증하기\n",
        "# ============================================\n",
        "\n",
        "def validate_best_model(data_split='val'):\n",
        "    \"\"\"\n",
        "    저장된 최고 모델로 검증 수행\n",
        "\n",
        "    Args:\n",
        "        data_split: 'train' 또는 'val'\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🔍 최고 모델로 {data_split} 데이터 검증\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 모델 로드\n",
        "    model_path = Path('/content/bee_project/outputs/weights/resnet18_best.pt')\n",
        "    if not model_path.exists():\n",
        "        print(\"❌ 저장된 모델이 없습니다! 먼저 학습을 실행하세요.\")\n",
        "        return\n",
        "\n",
        "    # 모델 생성 및 가중치 로드\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 8)\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"✅ 모델 로드 완료\")\n",
        "    print(f\"  학습 에폭: {checkpoint['epoch']}\")\n",
        "    print(f\"  학습 정확도: {checkpoint['train_accuracy']:.2f}%\")\n",
        "    print(f\"  검증 정확도: {checkpoint['val_accuracy']:.2f}%\")\n",
        "\n",
        "    # 데이터 준비\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = BeeDataset(f'/content/bee_project/crops/{data_split}', transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 검증 실행\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    class_correct = [0] * 8\n",
        "    class_total = [0] * 8\n",
        "\n",
        "    print(f\"\\n📊 {data_split} 데이터 검증 중...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # 클래스별 정확도\n",
        "            for i in range(labels.size(0)):\n",
        "                label = labels[i].item()\n",
        "                class_total[label] += 1\n",
        "                if predicted[i] == labels[i]:\n",
        "                    class_correct[label] += 1\n",
        "\n",
        "    # 전체 정확도\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f\"\\n✅ 전체 정확도: {accuracy:.2f}% ({correct}/{total})\")\n",
        "\n",
        "    # 클래스별 정확도\n",
        "    print(f\"\\n📋 클래스별 정확도:\")\n",
        "    for i, class_name in enumerate(CLASS_NAMES):\n",
        "        if class_total[i] > 0:\n",
        "            class_acc = 100. * class_correct[i] / class_total[i]\n",
        "            print(f\"  {class_name}: {class_acc:.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
        "        else:\n",
        "            print(f\"  {class_name}: 샘플 없음\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# ============================================\n",
        "# 6. 메인 실행 코드\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🐝 ResNet 벌 분류 학습 시스템\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: 크롭 이미지 생성 (처음 한 번만)\n",
        "    # create_crop_images()\n",
        "\n",
        "    # Step 2: 학습 실행 (파라미터 조정 가능)\n",
        "    # train_resnet(\n",
        "    #     epochs=30,\n",
        "    #     batch_size=32,\n",
        "    #     learning_rate=0.001,\n",
        "    #     weight_decay=0.0001,\n",
        "    #     use_pretrained=True,\n",
        "    #     use_augmentation=True\n",
        "    # )\n",
        "\n",
        "    # Step 3: 최고 모델로 검증\n",
        "    # validate_best_model('val')   # validation 데이터\n",
        "    # validate_best_model('train') # training 데이터\n",
        "\n",
        "    print(\"\\n📝 사용법:\")\n",
        "    print(\"1. create_crop_images()  # 크롭 생성 (처음 한 번)\")\n",
        "    print(\"2. train_resnet(epochs=30, learning_rate=0.001)  # 학습\")\n",
        "    print(\"3. validate_best_model('val')  # 검증\")"
      ],
      "metadata": {
        "id": "L0UrYFmTt6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_crop_images()\n",
        "train_resnet(epochs=30, learning_rate=0.001)"
      ],
      "metadata": {
        "id": "hB9W_XTPwTb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ResNet 벌 분류 테스트 코드\n",
        "학습된 모델로 테스트할 때 다양한 임계값 조정 가능\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "# ============================================\n",
        "# 기본 설정\n",
        "# ============================================\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"AB_LI\", \"QB_LI\",\n",
        "    \"AB_CA\", \"QB_CA\",\n",
        "    \"AB_BI\", \"QB_BI\",\n",
        "    \"AB_AP\", \"QB_AP\"\n",
        "]\n",
        "\n",
        "CLASS_MAPPING = {\n",
        "    \"AB_LI\": 0, \"QB_LI\": 1, \"AB_CA\": 2, \"QB_CA\": 3,\n",
        "    \"AB_BI\": 4, \"QB_BI\": 5, \"AB_AP\": 6, \"QB_AP\": 7\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# 데이터셋 클래스\n",
        "# ============================================\n",
        "\n",
        "class BeeTestDataset(Dataset):\n",
        "    \"\"\"테스트용 벌 데이터셋\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # 각 클래스 폴더에서 이미지 수집\n",
        "        for class_id, class_name in enumerate(CLASS_NAMES):\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                for img_path in class_dir.glob('*.jpg'):\n",
        "                    self.samples.append({\n",
        "                        'path': str(img_path),\n",
        "                        'label': class_id,\n",
        "                        'class_name': class_name\n",
        "                    })\n",
        "\n",
        "        print(f\"📁 테스트 데이터: {len(self.samples)}개 이미지\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = Image.open(sample['path']).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, sample['label'], sample['path']\n",
        "\n",
        "# ============================================\n",
        "# ResNet 테스트 클래스\n",
        "# ============================================\n",
        "\n",
        "class ResNetTester:\n",
        "    \"\"\"ResNet 테스트 클래스 (다양한 임계값 조정 가능)\"\"\"\n",
        "\n",
        "    def __init__(self, model_path='/content/bee_project/outputs/weights/resnet18_best.pt'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model_path = Path(model_path)\n",
        "\n",
        "        # 모델 로드\n",
        "        self.model = self._load_model()\n",
        "\n",
        "        # 전처리\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        print(f\"✅ 테스터 준비 완료 (Device: {self.device})\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"학습된 모델 로드\"\"\"\n",
        "        if not self.model_path.exists():\n",
        "            raise FileNotFoundError(f\"모델 파일이 없습니다: {self.model_path}\")\n",
        "\n",
        "        # 모델 생성\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.fc = nn.Linear(model.fc.in_features, 8)\n",
        "\n",
        "        # 가중치 로드\n",
        "        checkpoint = torch.load(self.model_path, map_location=self.device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"📦 모델 로드: {self.model_path}\")\n",
        "        print(f\"  학습 정확도: {checkpoint.get('train_accuracy', 'N/A'):.2f}%\")\n",
        "        print(f\"  검증 정확도: {checkpoint.get('val_accuracy', 'N/A'):.2f}%\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def test_with_confidence_threshold(\n",
        "        self,\n",
        "        data_dir='/content/bee_project/crops/val',\n",
        "        confidence_threshold=0.5,\n",
        "        reject_uncertain=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        신뢰도 임계값으로 테스트\n",
        "\n",
        "        Args:\n",
        "            data_dir: 테스트 데이터 경로\n",
        "            confidence_threshold: 최소 신뢰도 (이 값보다 낮으면 \"불확실\"로 처리)\n",
        "            reject_uncertain: True면 불확실한 샘플 제외, False면 포함\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"🔍 신뢰도 임계값 테스트\")\n",
        "        print(f\"  임계값: {confidence_threshold:.2%}\")\n",
        "        print(f\"  불확실 샘플: {'제외' if reject_uncertain else '포함'}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 데이터 로드\n",
        "        dataset = BeeTestDataset(data_dir, self.transform)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        # 테스트 변수\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        uncertain = 0\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels, paths in dataloader:\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # 추론\n",
        "                outputs = self.model(images)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                confidences, predicted = probs.max(1)\n",
        "\n",
        "                # 각 샘플별로 처리\n",
        "                for i in range(len(labels)):\n",
        "                    conf = confidences[i].item()\n",
        "                    pred = predicted[i].item()\n",
        "                    true_label = labels[i].item()\n",
        "\n",
        "                    # 신뢰도 체크\n",
        "                    if conf < confidence_threshold:\n",
        "                        uncertain += 1\n",
        "                        if reject_uncertain:\n",
        "                            continue  # 불확실한 샘플 제외\n",
        "                        else:\n",
        "                            # 불확실해도 예측은 수행\n",
        "                            total += 1\n",
        "                            if pred == true_label:\n",
        "                                correct += 1\n",
        "                    else:\n",
        "                        # 확실한 샘플\n",
        "                        total += 1\n",
        "                        if pred == true_label:\n",
        "                            correct += 1\n",
        "\n",
        "                    predictions.append({\n",
        "                        'path': paths[i],\n",
        "                        'true': CLASS_NAMES[true_label],\n",
        "                        'pred': CLASS_NAMES[pred],\n",
        "                        'confidence': conf,\n",
        "                        'correct': pred == true_label,\n",
        "                        'uncertain': conf < confidence_threshold\n",
        "                    })\n",
        "\n",
        "        # 결과 출력\n",
        "        accuracy = 100.0 * correct / total if total > 0 else 0\n",
        "\n",
        "        print(f\"\\n📊 결과:\")\n",
        "        print(f\"  전체 샘플: {len(dataset)}\")\n",
        "        print(f\"  확실한 샘플: {total} ({total/len(dataset)*100:.1f}%)\")\n",
        "        print(f\"  불확실한 샘플: {uncertain} ({uncertain/len(dataset)*100:.1f}%)\")\n",
        "        print(f\"  정확도: {accuracy:.2f}% ({correct}/{total})\")\n",
        "\n",
        "        # 오분류 샘플 출력\n",
        "        errors = [p for p in predictions if not p['correct'] and not p['uncertain']]\n",
        "        if errors and len(errors) <= 10:\n",
        "            print(f\"\\n❌ 오분류 샘플 (최대 10개):\")\n",
        "            for err in errors[:10]:\n",
        "                print(f\"  {Path(err['path']).name}: {err['true']} → {err['pred']} (신뢰도: {err['confidence']:.2%})\")\n",
        "\n",
        "        return predictions, accuracy\n",
        "\n",
        "    def test_with_top_k(\n",
        "        self,\n",
        "        data_dir='/content/bee_project/crops/val',\n",
        "        top_k=3,\n",
        "        min_confidence_gap=0.1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Top-K 정확도 테스트 (상위 K개 예측 중 정답이 있는지)\n",
        "\n",
        "        Args:\n",
        "            data_dir: 테스트 데이터 경로\n",
        "            top_k: 상위 몇 개까지 볼지\n",
        "            min_confidence_gap: 1위와 2위 신뢰도 차이 최소값\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"🔍 Top-{top_k} 정확도 테스트\")\n",
        "        print(f\"  신뢰도 차이 임계값: {min_confidence_gap:.2%}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        dataset = BeeTestDataset(data_dir, self.transform)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        top1_correct = 0\n",
        "        topk_correct = 0\n",
        "        low_gap_samples = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels, paths in dataloader:\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "                # Top-K 예측\n",
        "                topk_probs, topk_indices = probs.topk(min(top_k, 8), dim=1)\n",
        "\n",
        "                for i in range(len(labels)):\n",
        "                    total += 1\n",
        "                    true_label = labels[i].item()\n",
        "\n",
        "                    # Top-1 정확도\n",
        "                    if topk_indices[i][0].item() == true_label:\n",
        "                        top1_correct += 1\n",
        "\n",
        "                    # Top-K 정확도\n",
        "                    if true_label in topk_indices[i].tolist():\n",
        "                        topk_correct += 1\n",
        "\n",
        "                    # 신뢰도 차이 체크\n",
        "                    if len(topk_probs[i]) >= 2:\n",
        "                        gap = topk_probs[i][0].item() - topk_probs[i][1].item()\n",
        "                        if gap < min_confidence_gap:\n",
        "                            low_gap_samples += 1\n",
        "\n",
        "        # 결과 출력\n",
        "        top1_acc = 100.0 * top1_correct / total\n",
        "        topk_acc = 100.0 * topk_correct / total\n",
        "\n",
        "        print(f\"\\n📊 결과:\")\n",
        "        print(f\"  Top-1 정확도: {top1_acc:.2f}% ({top1_correct}/{total})\")\n",
        "        print(f\"  Top-{top_k} 정확도: {topk_acc:.2f}% ({topk_correct}/{total})\")\n",
        "        print(f\"  신뢰도 차이 낮은 샘플: {low_gap_samples} ({low_gap_samples/total*100:.1f}%)\")\n",
        "\n",
        "        return top1_acc, topk_acc\n",
        "\n",
        "    def test_with_class_specific_thresholds(\n",
        "        self,\n",
        "        data_dir='/content/bee_project/crops/val',\n",
        "        class_thresholds=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        클래스별 다른 임계값으로 테스트\n",
        "\n",
        "        Args:\n",
        "            data_dir: 테스트 데이터 경로\n",
        "            class_thresholds: 클래스별 임계값 딕셔너리\n",
        "                             예: {0: 0.7, 1: 0.8, ...}\n",
        "        \"\"\"\n",
        "\n",
        "        # 기본 임계값 설정\n",
        "        if class_thresholds is None:\n",
        "            class_thresholds = {i: 0.5 for i in range(8)}\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"🔍 클래스별 임계값 테스트\")\n",
        "        for i, name in enumerate(CLASS_NAMES):\n",
        "            if i in class_thresholds:\n",
        "                print(f\"  {name}: {class_thresholds[i]:.2%}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        dataset = BeeTestDataset(data_dir, self.transform)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        class_correct = [0] * 8\n",
        "        class_total = [0] * 8\n",
        "        class_rejected = [0] * 8\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels, paths in dataloader:\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                confidences, predicted = probs.max(1)\n",
        "\n",
        "                for i in range(len(labels)):\n",
        "                    true_label = labels[i].item()\n",
        "                    pred_label = predicted[i].item()\n",
        "                    confidence = confidences[i].item()\n",
        "\n",
        "                    # 클래스별 임계값 체크\n",
        "                    threshold = class_thresholds.get(pred_label, 0.5)\n",
        "\n",
        "                    class_total[true_label] += 1\n",
        "\n",
        "                    if confidence >= threshold:\n",
        "                        if pred_label == true_label:\n",
        "                            class_correct[true_label] += 1\n",
        "                    else:\n",
        "                        class_rejected[true_label] += 1\n",
        "\n",
        "        # 결과 출력\n",
        "        print(f\"\\n📊 클래스별 결과:\")\n",
        "        total_correct = sum(class_correct)\n",
        "        total_samples = sum(class_total)\n",
        "\n",
        "        for i, name in enumerate(CLASS_NAMES):\n",
        "            if class_total[i] > 0:\n",
        "                acc = 100.0 * class_correct[i] / class_total[i]\n",
        "                rej_rate = 100.0 * class_rejected[i] / class_total[i]\n",
        "                print(f\"  {name:15}: 정확도={acc:6.2f}%, 거부율={rej_rate:5.1f}% ({class_correct[i]}/{class_total[i]})\")\n",
        "\n",
        "        overall_acc = 100.0 * total_correct / total_samples if total_samples > 0 else 0\n",
        "        print(f\"\\n  전체 정확도: {overall_acc:.2f}%\")\n",
        "\n",
        "        return overall_acc\n",
        "\n",
        "    def test_single_image(\n",
        "        self,\n",
        "        image_path,\n",
        "        confidence_threshold=0.5,\n",
        "        show_top_k=3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        단일 이미지 테스트\n",
        "\n",
        "        Args:\n",
        "            image_path: 이미지 경로\n",
        "            confidence_threshold: 신뢰도 임계값\n",
        "            show_top_k: 상위 몇 개 예측을 보여줄지\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"🖼️ 단일 이미지 테스트: {Path(image_path).name}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 이미지 로드 및 전처리\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # 추론\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(img_tensor)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # Top-K 예측\n",
        "            topk_probs, topk_indices = probs.topk(min(show_top_k, 8), dim=1)\n",
        "\n",
        "        # 결과 출력\n",
        "        top1_prob = topk_probs[0][0].item()\n",
        "        top1_class = topk_indices[0][0].item()\n",
        "\n",
        "        print(f\"\\n🐝 예측 결과:\")\n",
        "\n",
        "        if top1_prob >= confidence_threshold:\n",
        "            print(f\"  ✅ 예측: {CLASS_NAMES[top1_class]}\")\n",
        "            print(f\"  신뢰도: {top1_prob:.2%}\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ 불확실 (신뢰도 {top1_prob:.2%} < 임계값 {confidence_threshold:.2%})\")\n",
        "            print(f\"  최고 예측: {CLASS_NAMES[top1_class]}\")\n",
        "\n",
        "        print(f\"\\n📊 Top-{show_top_k} 예측:\")\n",
        "        for i in range(len(topk_indices[0])):\n",
        "            idx = topk_indices[0][i].item()\n",
        "            prob = topk_probs[0][i].item()\n",
        "            print(f\"  {i+1}. {CLASS_NAMES[idx]:15}: {prob:7.2%}\")\n",
        "\n",
        "        # 신뢰도 차이\n",
        "        if len(topk_probs[0]) >= 2:\n",
        "            gap = topk_probs[0][0].item() - topk_probs[0][1].item()\n",
        "            print(f\"\\n  1위-2위 신뢰도 차이: {gap:.2%}\")\n",
        "\n",
        "        return CLASS_NAMES[top1_class], top1_prob\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 메인\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🐝 ResNet 테스트 시스템\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n📝 사용법:\")\n",
        "    print(\"\\n1. 빠른 테스트:\")\n",
        "    print(\"   quick_test(confidence_threshold=0.7)\")\n",
        "    print(\"\\n2. 다양한 임계값 실험:\")\n",
        "    print(\"   test_with_various_thresholds()\")\n",
        "    print(\"\\n3. 단일 이미지 테스트:\")\n",
        "    print(\"   tester = ResNetTester()\")\n",
        "    print(\"   tester.test_single_image('image.jpg', confidence_threshold=0.8)\")\n"
      ],
      "metadata": {
        "id": "wz4j7BePwXE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tester = ResNetTester()\n",
        "tester.test_single_image('01_1_R_QB_AP_20220802_07_0208.jpg', confidence_threshold=0.8)"
      ],
      "metadata": {
        "id": "7f0gmfhU9ilQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "테스트 이미지로 벌 종류 판별 및 시각화\n",
        "YOLO + ResNet을 사용하여 test 폴더의 이미지를 분석합니다.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.ops import nms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# ============================================\n",
        "# 설정\n",
        "# ============================================\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"AB_LI\", \"QB_LI\",\n",
        "    \"AB_CA\", \"QB_CA\",\n",
        "    \"AB_BI\", \"QB_BI\",\n",
        "    \"AB_AP\", \"QB_AP\"\n",
        "]\n",
        "\n",
        "# 클래스별 색상 (시각화용)\n",
        "CLASS_COLORS = [\n",
        "    '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4',\n",
        "    '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F'\n",
        "]\n",
        "\n",
        "# ============================================\n",
        "# YOLOv3 모델 정의 (필요한 부분만)\n",
        "# ============================================\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, act=True):\n",
        "        super().__init__()\n",
        "        p = k // 2 if p is None else p\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, p, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True) if act else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super().__init__()\n",
        "        self.cv1 = Conv(c, c//2, k=1, s=1)\n",
        "        self.cv2 = Conv(c//2, c, k=3, s=1)\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x))\n",
        "\n",
        "class Darknet53(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cv1 = Conv(3, 32, 3, 1)\n",
        "        self.cv2 = Conv(32, 64, 3, 2)\n",
        "        self.res1 = nn.Sequential(*[ResBlock(64) for _ in range(1)])\n",
        "        self.cv3 = Conv(64, 128, 3, 2)\n",
        "        self.res2 = nn.Sequential(*[ResBlock(128) for _ in range(2)])\n",
        "        self.cv4 = Conv(128, 256, 3, 2)\n",
        "        self.res3 = nn.Sequential(*[ResBlock(256) for _ in range(8)])\n",
        "        self.cv5 = Conv(256, 512, 3, 2)\n",
        "        self.res4 = nn.Sequential(*[ResBlock(512) for _ in range(8)])\n",
        "        self.cv6 = Conv(512, 1024, 3, 2)\n",
        "        self.res5 = nn.Sequential(*[ResBlock(1024) for _ in range(4)])\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        x = self.cv2(x) ; x = self.res1(x)\n",
        "        x = self.cv3(x) ; x = self.res2(x)\n",
        "        x = self.cv4(x) ; x3 = self.res3(x)\n",
        "        x = self.cv5(x) ; x2 = self.res4(x)\n",
        "        x = self.cv6(x) ; x1 = self.res5(x)\n",
        "        return x1, x2, x3\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, num_classes=1, anchors=None, img_size=416):\n",
        "        super().__init__()\n",
        "        self.nc = num_classes\n",
        "        self.na = 3\n",
        "        self.anchors = anchors or [\n",
        "            [(116,90), (156,198), (373,326)],\n",
        "            [(30,61), (62,45), (59,119)],\n",
        "            [(10,13), (16,30), (33,23)]\n",
        "        ]\n",
        "        self.img_size = img_size\n",
        "        self.backbone = Darknet53()\n",
        "\n",
        "        # Heads\n",
        "        self.head1 = nn.Sequential(Conv(1024, 512, 1, 1), Conv(512, 1024, 3, 1),\n",
        "                                   Conv(1024, 512, 1, 1), Conv(512, 1024, 3, 1),\n",
        "                                   Conv(1024, 512, 1, 1))\n",
        "        self.pred1 = nn.Conv2d(512, self.na*(5+self.nc), 1, 1, 0)\n",
        "\n",
        "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce1 = Conv(512, 256, 1, 1)\n",
        "\n",
        "        self.head2 = nn.Sequential(Conv(768, 256, 1, 1), Conv(256, 512, 3, 1),\n",
        "                                   Conv(512, 256, 1, 1), Conv(256, 512, 3, 1),\n",
        "                                   Conv(512, 256, 1, 1))\n",
        "        self.pred2 = nn.Conv2d(256, self.na*(5+self.nc), 1, 1, 0)\n",
        "\n",
        "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce2 = Conv(256, 128, 1, 1)\n",
        "\n",
        "        self.head3 = nn.Sequential(Conv(384, 128, 1, 1), Conv(128, 256, 3, 1),\n",
        "                                   Conv(256, 128, 1, 1), Conv(128, 256, 3, 1),\n",
        "                                   Conv(256, 128, 1, 1))\n",
        "        self.pred3 = nn.Conv2d(128, self.na*(5+self.nc), 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2, x3 = self.backbone(x)\n",
        "\n",
        "        p1 = self.head1(x1)\n",
        "        out1 = self.pred1(p1)\n",
        "\n",
        "        u1 = self.up1(self.reduce1(p1))\n",
        "        f2 = torch.cat([u1, x2], dim=1)\n",
        "        p2 = self.head2(f2)\n",
        "        out2 = self.pred2(p2)\n",
        "\n",
        "        u2 = self.up2(self.reduce2(p2))\n",
        "        f3 = torch.cat([u2, x3], dim=1)\n",
        "        p3 = self.head3(f3)\n",
        "        out3 = self.pred3(p3)\n",
        "\n",
        "        return [out1, out2, out3]\n",
        "\n",
        "# ============================================\n",
        "# YOLO 후처리 함수\n",
        "# ============================================\n",
        "\n",
        "def postprocess(outputs, conf_thr=0.25, iou_thr=0.5, img_size=416, num_classes=1, anchors=None):\n",
        "    \"\"\"YOLO 출력 후처리\"\"\"\n",
        "    if anchors is None:\n",
        "        anchors = [\n",
        "            [(116,90), (156,198), (373,326)],\n",
        "            [(30,61), (62,45), (59,119)],\n",
        "            [(10,13), (16,30), (33,23)]\n",
        "        ]\n",
        "\n",
        "    device = outputs[0].device\n",
        "    strides = [32, 16, 8]\n",
        "    decoded = []\n",
        "\n",
        "    for i, out in enumerate(outputs):\n",
        "        bs, ch, ny, nx = out.shape\n",
        "        na = 3\n",
        "        no = 5 + num_classes\n",
        "\n",
        "        out = out.view(bs, na, no, ny, nx).permute(0,1,3,4,2).contiguous()\n",
        "\n",
        "        stride = strides[i]\n",
        "        anc = torch.tensor(anchors[i], device=device).float() / stride\n",
        "\n",
        "        xv, yv = torch.meshgrid(torch.arange(nx, device=device),\n",
        "                                torch.arange(ny, device=device), indexing='xy')\n",
        "\n",
        "        x = (out[..., 0].sigmoid() + xv) * stride\n",
        "        y = (out[..., 1].sigmoid() + yv) * stride\n",
        "        w = (out[..., 2].exp() * anc[:,0].view(na,1,1)) * stride\n",
        "        h = (out[..., 3].exp() * anc[:,1].view(na,1,1)) * stride\n",
        "        obj = out[..., 4].sigmoid()\n",
        "\n",
        "        boxes = torch.stack([x - w/2, y - h/2, x + w/2, y + h/2], dim=-1)\n",
        "        boxes = boxes.view(bs, -1, 4)\n",
        "        obj = obj.view(bs, -1)\n",
        "\n",
        "        decoded.append((boxes, obj))\n",
        "\n",
        "    boxes = torch.cat([d[0] for d in decoded], dim=1)\n",
        "    obj = torch.cat([d[1] for d in decoded], dim=1)\n",
        "\n",
        "    results = []\n",
        "    for b in range(boxes.size(0)):\n",
        "        scores = obj[b]\n",
        "        keep = scores > conf_thr\n",
        "        bxs = boxes[b][keep]\n",
        "        scs = scores[keep]\n",
        "\n",
        "        if bxs.numel()==0:\n",
        "            results.append((torch.zeros((0,4)), torch.zeros((0,)), torch.zeros((0,), dtype=torch.long)))\n",
        "            continue\n",
        "\n",
        "        keep_idx = nms(bxs, scs, iou_thr)\n",
        "        results.append((bxs[keep_idx], scs[keep_idx], torch.zeros(len(keep_idx), dtype=torch.long)))\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# 벌 판별 시스템\n",
        "# ============================================\n",
        "\n",
        "class BeeDetector:\n",
        "    \"\"\"YOLO + ResNet 통합 벌 판별 시스템\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"🖥️ Device: {self.device}\")\n",
        "\n",
        "        # 모델 로드\n",
        "        self.yolo_model = self.load_yolo()\n",
        "        self.resnet_model = self.load_resnet()\n",
        "\n",
        "        # ResNet 전처리\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        print(\"✅ 시스템 준비 완료!\\n\")\n",
        "\n",
        "    def load_yolo(self):\n",
        "        \"\"\"YOLO 모델 로드\"\"\"\n",
        "        model_path = Path('/content/bee_project/outputs/weights/yolov3_best.pt')\n",
        "\n",
        "        # YOLOv3 모델 생성\n",
        "        model = YOLOv3(num_classes=1, img_size=416).to(self.device)\n",
        "\n",
        "        if model_path.exists():\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model'], strict=False)\n",
        "            print(f\"✅ YOLO 로드: mAP={checkpoint.get('map50', 0):.3f}\")\n",
        "        else:\n",
        "            print(f\"❌ YOLO 가중치 없음: {model_path}\")\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def load_resnet(self):\n",
        "        \"\"\"ResNet 모델 로드\"\"\"\n",
        "        model_path = Path('/content/bee_project/outputs/weights/resnet18_best.pt')\n",
        "\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.fc = nn.Linear(model.fc.in_features, 8)\n",
        "\n",
        "        if model_path.exists():\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(f\"✅ ResNet 로드: Acc={checkpoint.get('val_accuracy', 0):.1f}%\")\n",
        "        else:\n",
        "            print(f\"❌ ResNet 가중치 없음: {model_path}\")\n",
        "\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def detect_bees(self, image, conf_thr=0.3):\n",
        "        \"\"\"YOLO로 벌 탐지\"\"\"\n",
        "        h_orig, w_orig = image.shape[:2]\n",
        "\n",
        "        # 전처리 (416x416)\n",
        "        img_resized = Image.fromarray(image).resize((416, 416))\n",
        "        img_tensor = transforms.ToTensor()(img_resized).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # YOLO 추론\n",
        "        with torch.no_grad():\n",
        "            outputs = self.yolo_model(img_tensor)\n",
        "            results = postprocess(outputs, conf_thr, 0.5, 416, 1, self.yolo_model.anchors)\n",
        "            boxes, scores, _ = results[0]\n",
        "\n",
        "        # 원본 크기로 좌표 변환\n",
        "        detections = []\n",
        "        for box, score in zip(boxes, scores):\n",
        "            x1 = int(box[0] * w_orig / 416)\n",
        "            y1 = int(box[1] * h_orig / 416)\n",
        "            x2 = int(box[2] * w_orig / 416)\n",
        "            y2 = int(box[3] * h_orig / 416)\n",
        "\n",
        "            # 경계 체크\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w_orig, x2), min(h_orig, y2)\n",
        "\n",
        "            if x2 > x1 and y2 > y1:\n",
        "                detections.append({\n",
        "                    'bbox': [x1, y1, x2, y2],\n",
        "                    'score': float(score)\n",
        "                })\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def classify_bee(self, crop):\n",
        "        \"\"\"ResNet으로 벌 종류 분류\"\"\"\n",
        "        crop_pil = Image.fromarray(crop)\n",
        "        img_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.resnet_model(img_tensor)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            confidence, predicted = probs.max(1)\n",
        "\n",
        "        return {\n",
        "            'class_id': predicted.item(),\n",
        "            'class_name': CLASS_NAMES[predicted.item()],\n",
        "            'confidence': confidence.item()\n",
        "        }\n",
        "\n",
        "    def process_image(self, image_path):\n",
        "        \"\"\"이미지 처리 (탐지 + 분류)\"\"\"\n",
        "        # 이미지 로드\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        img_array = np.array(image)\n",
        "\n",
        "        # 1. YOLO 탐지\n",
        "        detections = self.detect_bees(img_array)\n",
        "\n",
        "        # 2. 각 탐지에 대해 분류\n",
        "        results = []\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2 = det['bbox']\n",
        "            crop = img_array[y1:y2, x1:x2]\n",
        "\n",
        "            # ResNet 분류\n",
        "            species = self.classify_bee(crop)\n",
        "\n",
        "            results.append({\n",
        "                'bbox': det['bbox'],\n",
        "                'detection_conf': det['score'],\n",
        "                'species': species['class_name'],\n",
        "                'species_conf': species['confidence'],\n",
        "                'color': CLASS_COLORS[species['class_id']]\n",
        "            })\n",
        "\n",
        "        return img_array, results\n",
        "\n",
        "# ============================================\n",
        "# 테스트 및 시각화 함수\n",
        "# ============================================\n",
        "\n",
        "def test_and_visualize(num_samples=5):\n",
        "    \"\"\"\n",
        "    테스트 이미지 샘플링하여 판별 및 시각화\n",
        "\n",
        "    Args:\n",
        "        num_samples: 테스트할 이미지 개수\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"🧪 테스트 시작 (샘플: {num_samples}개)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 테스트 이미지 경로\n",
        "    test_dir = Path('/content/bee_project/images/test')\n",
        "    if not test_dir.exists():\n",
        "        print(f\"❌ 테스트 폴더 없음: {test_dir}\")\n",
        "        return\n",
        "\n",
        "    # 이미지 파일 수집\n",
        "    test_images = list(test_dir.glob('*.jpg')) + list(test_dir.glob('*.png'))\n",
        "    if len(test_images) == 0:\n",
        "        print(\"❌ 테스트 이미지가 없습니다!\")\n",
        "        return\n",
        "\n",
        "    # 랜덤 샘플링\n",
        "    samples = random.sample(test_images, min(num_samples, len(test_images)))\n",
        "    print(f\"📁 전체 {len(test_images)}개 중 {len(samples)}개 샘플링\\n\")\n",
        "\n",
        "    # 판별 시스템 초기화\n",
        "    detector = BeeDetector()\n",
        "\n",
        "    # 결과 저장\n",
        "    all_results = []\n",
        "\n",
        "    # 각 이미지 처리\n",
        "    for i, img_path in enumerate(samples):\n",
        "        print(f\"🖼️ [{i+1}/{len(samples)}] {img_path.name}\")\n",
        "\n",
        "        # 처리\n",
        "        image, results = detector.process_image(img_path)\n",
        "\n",
        "        # 결과 출력\n",
        "        if results:\n",
        "            print(f\"  ✅ {len(results)}마리 탐지:\")\n",
        "            for j, r in enumerate(results):\n",
        "                print(f\"     {j+1}. {r['species']} (신뢰도: {r['species_conf']:.2%})\")\n",
        "        else:\n",
        "            print(f\"  ❌ 탐지된 벌 없음\")\n",
        "\n",
        "        all_results.append({\n",
        "            'path': img_path,\n",
        "            'image': image,\n",
        "            'results': results\n",
        "        })\n",
        "        print()\n",
        "\n",
        "    # 시각화\n",
        "    visualize_results(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def visualize_results(all_results):\n",
        "    \"\"\"결과 시각화\"\"\"\n",
        "\n",
        "    n = len(all_results)\n",
        "    if n == 0:\n",
        "        return\n",
        "\n",
        "    # 그리드 크기 결정\n",
        "    cols = min(3, n)\n",
        "    rows = (n + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*6, rows*5))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    elif rows == 1:\n",
        "        axes = axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, data in enumerate(all_results):\n",
        "        ax = axes[idx] if n > 1 else axes[0]\n",
        "\n",
        "        # 이미지 표시\n",
        "        ax.imshow(data['image'])\n",
        "        ax.set_title(f\"{data['path'].name}\\n탐지: {len(data['results'])}마리\",\n",
        "                    fontsize=10, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # 바운딩 박스 그리기\n",
        "        for result in data['results']:\n",
        "            x1, y1, x2, y2 = result['bbox']\n",
        "\n",
        "            # 색상 변환\n",
        "            color = result['color']\n",
        "            if color.startswith('#'):\n",
        "                color = tuple(int(color[i:i+2], 16)/255 for i in (1, 3, 5))\n",
        "\n",
        "            # 박스\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                    linewidth=2, edgecolor=color,\n",
        "                                    facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # 라벨\n",
        "            label = f\"{result['species']}\\n{result['species_conf']:.1%}\"\n",
        "            ax.text(x1, y1-5, label, color=color, fontsize=8,\n",
        "                   fontweight='bold',\n",
        "                   bbox=dict(boxstyle='round,pad=0.3',\n",
        "                            facecolor='white', alpha=0.8))\n",
        "\n",
        "    # 빈 subplot 숨기기\n",
        "    for idx in range(n, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle('🐝 벌 종류 판별 결과', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def quick_test(num=3):\n",
        "    \"\"\"빠른 테스트 실행\"\"\"\n",
        "    return test_and_visualize(num)\n",
        "\n",
        "def quick_test_fixed(num=3):\n",
        "    \"\"\"수정된 테스트 (높은 임계값)\"\"\"\n",
        "\n",
        "    # 기존 detector의 detect_bees 메서드 오버라이드\n",
        "    detector = BeeDetector()\n",
        "\n",
        "    # 신뢰도 임계값 상향\n",
        "    original_detect = detector.detect_bees\n",
        "    def detect_bees_strict(image):\n",
        "        return original_detect(image, conf_thr=0.7)  # 0.3 → 0.7\n",
        "    detector.detect_bees = detect_bees_strict\n",
        "\n",
        "    # 테스트 실행\n",
        "    return test_and_visualize(num)\n",
        "\n",
        "def filter_detections(detections):\n",
        "    \"\"\"너무 작거나 큰 박스 제거\"\"\"\n",
        "    filtered = []\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2 = det['bbox']\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "\n",
        "        # 크기 필터 (너무 작거나 큰 박스 제거)\n",
        "        if 30 < w < 300 and 30 < h < 300:\n",
        "            # 종횡비 필터 (너무 길쭉한 박스 제거)\n",
        "            aspect_ratio = w / h\n",
        "            if 0.3 < aspect_ratio < 3.0:\n",
        "                filtered.append(det)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def test_improved(num_samples=5):\n",
        "    \"\"\"개선된 테스트 함수\"\"\"\n",
        "\n",
        "    print(\"🔧 개선된 테스트 시작\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_dir = Path('/content/bee_project/images/test')\n",
        "    test_images = list(test_dir.glob('*.jpg'))\n",
        "    samples = random.sample(test_images, min(num_samples, len(test_images)))\n",
        "\n",
        "    detector = BeeDetector()\n",
        "    all_results = []\n",
        "\n",
        "    for i, img_path in enumerate(samples):\n",
        "        print(f\"🖼️ [{i+1}/{len(samples)}] {img_path.name}\")\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        img_array = np.array(image)\n",
        "\n",
        "        # 1. YOLO 탐지 (높은 임계값)\n",
        "        detections = detector.detect_bees(img_array, conf_thr=0.7)\n",
        "\n",
        "        # 2. 크기 필터링\n",
        "        detections = filter_detections(detections)\n",
        "\n",
        "        # 3. 상위 N개만 선택\n",
        "        detections = sorted(detections, key=lambda x: x['score'], reverse=True)[:10]\n",
        "\n",
        "        # 4. 분류\n",
        "        results = []\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2 = det['bbox']\n",
        "\n",
        "            # 여백 추가한 크롭\n",
        "            margin = 0.1\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            x1_new = max(0, int(x1 - w * margin))\n",
        "            y1_new = max(0, int(y1 - h * margin))\n",
        "            x2_new = min(img_array.shape[1], int(x2 + w * margin))\n",
        "            y2_new = min(img_array.shape[0], int(y2 + h * margin))\n",
        "\n",
        "            crop = img_array[y1_new:y2_new, x1_new:x2_new]\n",
        "\n",
        "            if crop.size > 0:\n",
        "                species = detector.classify_bee(crop)\n",
        "\n",
        "                # 분류 신뢰도도 체크\n",
        "                if species['confidence'] > 0.98:\n",
        "                    results.append({\n",
        "                        'bbox': [x1, y1, x2, y2],\n",
        "                        'detection_conf': det['score'],\n",
        "                        'species': species['class_name'],\n",
        "                        'species_conf': species['confidence'],\n",
        "                        'color': CLASS_COLORS[species['class_id']]\n",
        "                    })\n",
        "\n",
        "        print(f\"  ✅ {len(results)}마리 확정 (필터링 후)\")\n",
        "\n",
        "        all_results.append({\n",
        "            'path': img_path,\n",
        "            'image': img_array,\n",
        "            'results': results\n",
        "        })\n",
        "\n",
        "    visualize_results(all_results)\n",
        "    return all_results\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 통계 분석\n",
        "# ============================================\n",
        "\n",
        "def analyze_test_results(num_samples=20):\n",
        "    \"\"\"테스트 결과 통계 분석\"\"\"\n",
        "\n",
        "    print(\"📊 테스트 통계 분석\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    detector = BeeDetector()\n",
        "    test_dir = Path('/content/bee_project/images/test')\n",
        "    test_images = list(test_dir.glob('*.jpg'))\n",
        "\n",
        "    if not test_images:\n",
        "        print(\"❌ 테스트 이미지가 없습니다!\")\n",
        "        return\n",
        "\n",
        "    samples = random.sample(test_images, min(num_samples, len(test_images)))\n",
        "\n",
        "    # 통계 변수\n",
        "    total_images = len(samples)\n",
        "    total_detected = 0\n",
        "    class_counts = {name: 0 for name in CLASS_NAMES}\n",
        "    confidence_scores = []\n",
        "\n",
        "    for img_path in samples:\n",
        "        _, results = detector.process_image(img_path)\n",
        "        total_detected += len(results)\n",
        "\n",
        "        for r in results:\n",
        "            class_counts[r['species']] += 1\n",
        "            confidence_scores.append(r['species_conf'])\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"\\n📈 결과:\")\n",
        "    print(f\"  테스트 이미지: {total_images}개\")\n",
        "    print(f\"  탐지된 벌: {total_detected}마리\")\n",
        "    print(f\"  평균 탐지: {total_detected/total_images:.1f}마리/이미지\")\n",
        "\n",
        "    if confidence_scores:\n",
        "        print(f\"  평균 신뢰도: {np.mean(confidence_scores):.2%}\")\n",
        "        print(f\"  최소 신뢰도: {min(confidence_scores):.2%}\")\n",
        "        print(f\"  최대 신뢰도: {max(confidence_scores):.2%}\")\n",
        "\n",
        "    print(f\"\\n📋 클래스별 탐지:\")\n",
        "    for name, count in class_counts.items():\n",
        "        if count > 0:\n",
        "            print(f\"  {name}: {count}마리\")\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "# ============================================\n",
        "# 메인\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🐝 벌 종류 판별 테스트 시스템\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n사용법:\")\n",
        "    print(\"  quick_test(3)        # 3개 샘플 테스트\")\n",
        "    print(\"  quick_test(10)       # 10개 샘플 테스트\")\n",
        "    print(\"  analyze_test_results(20)  # 20개로 통계 분석\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "8uLlSyfJEbhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행\n",
        "test_improved(5)"
      ],
      "metadata": {
        "id": "mgjMpjm7IYz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_test(5)"
      ],
      "metadata": {
        "id": "958XpdqVEbbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "테스트 이미지로 벌 종류 판별 및 시각화 (군집화 개선 버전)\n",
        "YOLO + ResNet을 사용하여 test 폴더의 이미지를 분석합니다.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.ops import nms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# ============================================\n",
        "# 설정\n",
        "# ============================================\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"AB_LI\", \"QB_LI\",\n",
        "    \"AB_CA\", \"QB_CA\",\n",
        "    \"AB_BI\", \"QB_BI\",\n",
        "    \"AB_AP\", \"QB_AP\"\n",
        "]\n",
        "\n",
        "# 클래스별 색상 (시각화용)\n",
        "CLASS_COLORS = [\n",
        "    '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4',\n",
        "    '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F'\n",
        "]\n",
        "\n",
        "# ============================================\n",
        "# YOLOv3 모델 정의 (필요한 부분만)\n",
        "# ============================================\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, act=True):\n",
        "        super().__init__()\n",
        "        p = k // 2 if p is None else p\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, p, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True) if act else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, c):\n",
        "        super().__init__()\n",
        "        self.cv1 = Conv(c, c//2, k=1, s=1)\n",
        "        self.cv2 = Conv(c//2, c, k=3, s=1)\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x))\n",
        "\n",
        "class Darknet53(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cv1 = Conv(3, 32, 3, 1)\n",
        "        self.cv2 = Conv(32, 64, 3, 2)\n",
        "        self.res1 = nn.Sequential(*[ResBlock(64) for _ in range(1)])\n",
        "        self.cv3 = Conv(64, 128, 3, 2)\n",
        "        self.res2 = nn.Sequential(*[ResBlock(128) for _ in range(2)])\n",
        "        self.cv4 = Conv(128, 256, 3, 2)\n",
        "        self.res3 = nn.Sequential(*[ResBlock(256) for _ in range(8)])\n",
        "        self.cv5 = Conv(256, 512, 3, 2)\n",
        "        self.res4 = nn.Sequential(*[ResBlock(512) for _ in range(8)])\n",
        "        self.cv6 = Conv(512, 1024, 3, 2)\n",
        "        self.res5 = nn.Sequential(*[ResBlock(1024) for _ in range(4)])\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        x = self.cv2(x) ; x = self.res1(x)\n",
        "        x = self.cv3(x) ; x = self.res2(x)\n",
        "        x = self.cv4(x) ; x3 = self.res3(x)\n",
        "        x = self.cv5(x) ; x2 = self.res4(x)\n",
        "        x = self.cv6(x) ; x1 = self.res5(x)\n",
        "        return x1, x2, x3\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, num_classes=1, anchors=None, img_size=416):\n",
        "        super().__init__()\n",
        "        self.nc = num_classes\n",
        "        self.na = 3\n",
        "        self.anchors = anchors or [\n",
        "            [(116,90), (156,198), (373,326)],\n",
        "            [(30,61), (62,45), (59,119)],\n",
        "            [(10,13), (16,30), (33,23)]\n",
        "        ]\n",
        "        self.img_size = img_size\n",
        "        self.backbone = Darknet53()\n",
        "\n",
        "        # Heads\n",
        "        self.head1 = nn.Sequential(Conv(1024, 512, 1, 1), Conv(512, 1024, 3, 1),\n",
        "                                   Conv(1024, 512, 1, 1), Conv(512, 1024, 3, 1),\n",
        "                                   Conv(1024, 512, 1, 1))\n",
        "        self.pred1 = nn.Conv2d(512, self.na*(5+self.nc), 1, 1, 0)\n",
        "\n",
        "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce1 = Conv(512, 256, 1, 1)\n",
        "\n",
        "        self.head2 = nn.Sequential(Conv(768, 256, 1, 1), Conv(256, 512, 3, 1),\n",
        "                                   Conv(512, 256, 1, 1), Conv(256, 512, 3, 1),\n",
        "                                   Conv(512, 256, 1, 1))\n",
        "        self.pred2 = nn.Conv2d(256, self.na*(5+self.nc), 1, 1, 0)\n",
        "\n",
        "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.reduce2 = Conv(256, 128, 1, 1)\n",
        "\n",
        "        self.head3 = nn.Sequential(Conv(384, 128, 1, 1), Conv(128, 256, 3, 1),\n",
        "                                   Conv(256, 128, 1, 1), Conv(128, 256, 3, 1),\n",
        "                                   Conv(256, 128, 1, 1))\n",
        "        self.pred3 = nn.Conv2d(128, self.na*(5+self.nc), 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2, x3 = self.backbone(x)\n",
        "\n",
        "        p1 = self.head1(x1)\n",
        "        out1 = self.pred1(p1)\n",
        "\n",
        "        u1 = self.up1(self.reduce1(p1))\n",
        "        f2 = torch.cat([u1, x2], dim=1)\n",
        "        p2 = self.head2(f2)\n",
        "        out2 = self.pred2(p2)\n",
        "\n",
        "        u2 = self.up2(self.reduce2(p2))\n",
        "        f3 = torch.cat([u2, x3], dim=1)\n",
        "        p3 = self.head3(f3)\n",
        "        out3 = self.pred3(p3)\n",
        "\n",
        "        return [out1, out2, out3]\n",
        "\n",
        "# ============================================\n",
        "# YOLO 후처리 함수\n",
        "# ============================================\n",
        "\n",
        "def postprocess(outputs, conf_thr=0.25, iou_thr=0.5, img_size=416, num_classes=1, anchors=None):\n",
        "    \"\"\"YOLO 출력 후처리\"\"\"\n",
        "    if anchors is None:\n",
        "        anchors = [\n",
        "            [(116,90), (156,198), (373,326)],\n",
        "            [(30,61), (62,45), (59,119)],\n",
        "            [(10,13), (16,30), (33,23)]\n",
        "        ]\n",
        "\n",
        "    device = outputs[0].device\n",
        "    strides = [32, 16, 8] # 앵커 적용 안해봄\n",
        "    decoded = []\n",
        "\n",
        "    for i, out in enumerate(outputs):\n",
        "        bs, ch, ny, nx = out.shape\n",
        "        na = 3\n",
        "        no = 5 + num_classes\n",
        "\n",
        "        out = out.view(bs, na, no, ny, nx).permute(0,1,3,4,2).contiguous()\n",
        "\n",
        "        stride = strides[i]\n",
        "        anc = torch.tensor(anchors[i], device=device).float() / stride\n",
        "\n",
        "        xv, yv = torch.meshgrid(torch.arange(nx, device=device),\n",
        "                                torch.arange(ny, device=device), indexing='xy')\n",
        "\n",
        "        x = (out[..., 0].sigmoid() + xv) * stride\n",
        "        y = (out[..., 1].sigmoid() + yv) * stride\n",
        "        w = (out[..., 2].exp() * anc[:,0].view(na,1,1)) * stride\n",
        "        h = (out[..., 3].exp() * anc[:,1].view(na,1,1)) * stride\n",
        "        obj = out[..., 4].sigmoid()\n",
        "\n",
        "        boxes = torch.stack([x - w/2, y - h/2, x + w/2, y + h/2], dim=-1)\n",
        "        boxes = boxes.view(bs, -1, 4)\n",
        "        obj = obj.view(bs, -1)\n",
        "\n",
        "        decoded.append((boxes, obj))\n",
        "\n",
        "    boxes = torch.cat([d[0] for d in decoded], dim=1)\n",
        "    obj = torch.cat([d[1] for d in decoded], dim=1)\n",
        "\n",
        "    results = []\n",
        "    for b in range(boxes.size(0)):\n",
        "        scores = obj[b]\n",
        "        keep = scores > conf_thr\n",
        "        bxs = boxes[b][keep]\n",
        "        scs = scores[keep]\n",
        "\n",
        "        if bxs.numel()==0:\n",
        "            results.append((torch.zeros((0,4)), torch.zeros((0,)), torch.zeros((0,), dtype=torch.long)))\n",
        "            continue\n",
        "\n",
        "        keep_idx = nms(bxs, scs, iou_thr)\n",
        "        results.append((bxs[keep_idx], scs[keep_idx], torch.zeros(len(keep_idx), dtype=torch.long)))\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# 군집화 및 필터링 함수 (문서 참고하여 개선)\n",
        "# ============================================\n",
        "\n",
        "def calc_resize_pad_params(W0, H0, img_size=416):\n",
        "    \"\"\"원본 크기에서 리사이즈/패딩 파라미터 계산\"\"\"\n",
        "    r = min(img_size / W0, img_size / H0)\n",
        "    W1, H1 = int(round(W0*r)), int(round(H0*r))\n",
        "    xpad = (img_size - W1) / 2.0\n",
        "    ypad = (img_size - H1) / 2.0\n",
        "    return r, xpad, ypad, W1, H1\n",
        "\n",
        "def filter_by_valid_area(boxes416, W0, H0, use_pad_filter=True):\n",
        "    \"\"\"패딩 영역에 걸친 탐지 제거\"\"\"\n",
        "    if not use_pad_filter or len(boxes416)==0:\n",
        "        return boxes416, np.ones(len(boxes416), bool)\n",
        "\n",
        "    _, xpad, ypad, W1, H1 = calc_resize_pad_params(W0, H0, 416)\n",
        "    x1v, y1v, x2v, y2v = xpad, ypad, xpad+W1, ypad+H1\n",
        "\n",
        "    # 중심점이 유효 영역에 있는지 확인\n",
        "    cx = (boxes416[:,0]+boxes416[:,2]) * 0.5\n",
        "    cy = (boxes416[:,1]+boxes416[:,3]) * 0.5\n",
        "    keep = (cx>=x1v) & (cx<=x2v) & (cy>=y1v) & (cy<=y2v)\n",
        "\n",
        "    return boxes416[keep], keep\n",
        "\n",
        "def filter_by_size(boxes416, scores, min_wh=12, max_frac=0.90):\n",
        "    \"\"\"너무 작거나 큰 박스 제거\"\"\"\n",
        "    if len(boxes416)==0:\n",
        "        return boxes416, scores\n",
        "\n",
        "    w = np.clip(boxes416[:,2]-boxes416[:,0], 0, None)\n",
        "    h = np.clip(boxes416[:,3]-boxes416[:,1], 0, None)\n",
        "    keep = (w>=min_wh) & (h>=min_wh) & (w<=416*max_frac) & (h<=416*max_frac)\n",
        "\n",
        "    return boxes416[keep], scores[keep]\n",
        "\n",
        "def cluster_and_prune(boxes, scores, cluster_iou=0.3, max_clusters=10):\n",
        "    \"\"\"\n",
        "    IoU > cluster_iou인 탐지들을 같은 군집으로 묶고, 각 군집에서 최고 점수만 남김\n",
        "    \"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return boxes, scores, cluster_iou\n",
        "\n",
        "    # numpy 변환\n",
        "    if isinstance(boxes, torch.Tensor):\n",
        "        boxes = boxes.detach().cpu().numpy()\n",
        "    if isinstance(scores, torch.Tensor):\n",
        "        scores = scores.detach().cpu().numpy()\n",
        "\n",
        "    x1, y1, x2, y2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n",
        "    area = np.maximum(0, x2-x1) * np.maximum(0, y2-y1)\n",
        "\n",
        "    # 점수 순으로 정렬\n",
        "    order = list(range(len(boxes)))\n",
        "    order.sort(key=lambda i: scores[i], reverse=True)\n",
        "\n",
        "    kept = []\n",
        "\n",
        "    while order:\n",
        "        # 가장 높은 점수를 시드로 선택\n",
        "        i = order.pop(0)\n",
        "        kept.append(i)\n",
        "\n",
        "        # 시드와 IoU > threshold인 것들 제거 (같은 군집)\n",
        "        next_order = []\n",
        "        for j in order:\n",
        "            xx1 = max(x1[i], x1[j])\n",
        "            yy1 = max(y1[i], y1[j])\n",
        "            xx2 = min(x2[i], x2[j])\n",
        "            yy2 = min(y2[i], y2[j])\n",
        "\n",
        "            inter = max(0, xx2-xx1) * max(0, yy2-yy1)\n",
        "            iou = inter / (area[i] + area[j] - inter + 1e-9)\n",
        "\n",
        "            if iou <= cluster_iou:  # 다른 군집이면 유지\n",
        "                next_order.append(j)\n",
        "\n",
        "        order = next_order\n",
        "\n",
        "    # 최대 개수 제한\n",
        "    kept = np.array(kept, dtype=int)[:max_clusters]\n",
        "\n",
        "    return boxes[kept], scores[kept], cluster_iou\n",
        "\n",
        "# ============================================\n",
        "# 벌 판별 시스템\n",
        "# ============================================\n",
        "\n",
        "class BeeDetector:\n",
        "    \"\"\"YOLO + ResNet 통합 벌 판별 시스템\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"🖥️ Device: {self.device}\")\n",
        "\n",
        "        # 모델 로드\n",
        "        self.yolo_model = self.load_yolo()\n",
        "        self.resnet_model = self.load_resnet()\n",
        "\n",
        "        # ResNet 전처리\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        print(\"✅ 시스템 준비 완료!\\n\")\n",
        "\n",
        "    def load_yolo(self):\n",
        "        \"\"\"YOLO 모델 로드\"\"\"\n",
        "        model_path = Path('/content/bee_project/outputs/weights/yolov3_best.pt')\n",
        "\n",
        "        # YOLOv3 모델 생성\n",
        "        model = YOLOv3(num_classes=1, img_size=416).to(self.device)\n",
        "\n",
        "        if model_path.exists():\n",
        "            try:\n",
        "                checkpoint = torch.load(model_path, map_location=self.device)\n",
        "                model.load_state_dict(checkpoint['model'], strict=False)\n",
        "                print(f\"✅ YOLO 로드: mAP={checkpoint.get('map50', 0):.3f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ YOLO 가중치 로드 오류: {e}\")\n",
        "        else:\n",
        "            print(f\"❌ YOLO 가중치 없음: {model_path}\")\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def load_resnet(self):\n",
        "        \"\"\"ResNet 모델 로드\"\"\"\n",
        "        model_path = Path('/content/bee_project/outputs/weights/resnet18_best.pt')\n",
        "\n",
        "        model = models.resnet18(pretrained=False)\n",
        "        model.fc = nn.Linear(model.fc.in_features, 8)\n",
        "\n",
        "        if model_path.exists():\n",
        "            try:\n",
        "                checkpoint = torch.load(model_path, map_location=self.device)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(f\"✅ ResNet 로드: Acc={checkpoint.get('val_accuracy', 0):.1f}%\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ ResNet 가중치 로드 오류: {e}\")\n",
        "        else:\n",
        "            print(f\"❌ ResNet 가중치 없음: {model_path}\")\n",
        "\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def detect_bees(self, image, conf_thr=0.5):\n",
        "        \"\"\"YOLO로 벌 탐지 + 군집화\"\"\"\n",
        "        h_orig, w_orig = image.shape[:2]\n",
        "\n",
        "        # 전처리 (416x416)\n",
        "        img_resized = Image.fromarray(image).resize((416, 416))\n",
        "        img_tensor = transforms.ToTensor()(img_resized).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # YOLO 추론\n",
        "        with torch.no_grad():\n",
        "            outputs = self.yolo_model(img_tensor)\n",
        "            results = postprocess(outputs, conf_thr, 0.5, 416, 1, self.yolo_model.anchors)\n",
        "            boxes, scores, _ = results[0]\n",
        "\n",
        "        # numpy 변환\n",
        "        if isinstance(boxes, torch.Tensor):\n",
        "            boxes = boxes.detach().cpu().numpy()\n",
        "        if isinstance(scores, torch.Tensor):\n",
        "            scores = scores.detach().cpu().numpy()\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return []\n",
        "\n",
        "        print(f\"  📊 YOLO 원시 탐지: {len(boxes)}개\")\n",
        "\n",
        "        # 1. 패딩 영역 필터링\n",
        "        boxes, keep_mask = filter_by_valid_area(boxes, w_orig, h_orig)\n",
        "        scores = scores[keep_mask]\n",
        "        print(f\"  📊 패딩 필터 후: {len(boxes)}개\")\n",
        "\n",
        "        # 2. 크기 필터링\n",
        "        boxes, scores = filter_by_size(boxes, scores, min_wh=12, max_frac=0.90)\n",
        "        print(f\"  📊 크기 필터 후: {len(boxes)}개\")\n",
        "\n",
        "        # 3. 군집화 (핵심!)\n",
        "        boxes, scores, used_iou = cluster_and_prune(\n",
        "            boxes, scores,\n",
        "            cluster_iou=0.3,  # IoU 임계값\n",
        "            max_clusters=10   # 최대 군집 수\n",
        "        )\n",
        "        print(f\"  📊 군집화 후: {len(boxes)}개 (IoU={used_iou:.2f})\")\n",
        "\n",
        "        # 원본 크기로 좌표 변환\n",
        "        detections = []\n",
        "        for box, score in zip(boxes, scores):\n",
        "            x1 = int(box[0] * w_orig / 416)\n",
        "            y1 = int(box[1] * h_orig / 416)\n",
        "            x2 = int(box[2] * w_orig / 416)\n",
        "            y2 = int(box[3] * h_orig / 416)\n",
        "\n",
        "            # 경계 체크\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w_orig, x2), min(h_orig, y2)\n",
        "\n",
        "            if x2 > x1 and y2 > y1:\n",
        "                detections.append({\n",
        "                    'bbox': [x1, y1, x2, y2],\n",
        "                    'score': float(score)\n",
        "                })\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def classify_bee(self, crop):\n",
        "        \"\"\"ResNet으로 벌 종류 분류\"\"\"\n",
        "        try:\n",
        "            crop_pil = Image.fromarray(crop)\n",
        "            img_tensor = self.transform(crop_pil).unsqueeze(0).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.resnet_model(img_tensor)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                confidence, predicted = probs.max(1)\n",
        "\n",
        "            return {\n",
        "                'class_id': predicted.item(),\n",
        "                'class_name': CLASS_NAMES[predicted.item()],\n",
        "                'confidence': confidence.item()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 분류 오류: {e}\")\n",
        "            return {\n",
        "                'class_id': 0,\n",
        "                'class_name': CLASS_NAMES[0],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "    def process_image(self, image_path):\n",
        "        \"\"\"이미지 처리 (탐지 + 분류)\"\"\"\n",
        "        # 이미지 로드\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        img_array = np.array(image)\n",
        "\n",
        "        print(f\"🔍 처리 중: {image_path.name}\")\n",
        "\n",
        "        # 1. YOLO 탐지\n",
        "        detections = self.detect_bees(img_array, conf_thr=0.5)\n",
        "\n",
        "        # 2. 각 탐지에 대해 분류\n",
        "        results = []\n",
        "        for i, det in enumerate(detections):\n",
        "            x1, y1, x2, y2 = det['bbox']\n",
        "\n",
        "            # 여백 추가하여 크롭\n",
        "            margin = 0.1\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            x1_margin = max(0, int(x1 - w * margin))\n",
        "            y1_margin = max(0, int(y1 - h * margin))\n",
        "            x2_margin = min(img_array.shape[1], int(x2 + w * margin))\n",
        "            y2_margin = min(img_array.shape[0], int(y2 + h * margin))\n",
        "\n",
        "            crop = img_array[y1_margin:y2_margin, x1_margin:x2_margin]\n",
        "\n",
        "            if crop.size > 0:\n",
        "                # ResNet 분류\n",
        "                species = self.classify_bee(crop)\n",
        "\n",
        "                results.append({\n",
        "                    'bbox': det['bbox'],\n",
        "                    'detection_conf': det['score'],\n",
        "                    'species': species['class_name'],\n",
        "                    'species_conf': species['confidence'],\n",
        "                    'color': CLASS_COLORS[species['class_id']]\n",
        "                })\n",
        "\n",
        "                print(f\"    {i+1}. {species['class_name']} (신뢰도: {species['confidence']:.2%})\")\n",
        "\n",
        "        return img_array, results\n",
        "\n",
        "# ============================================\n",
        "# 테스트 및 시각화 함수\n",
        "# ============================================\n",
        "\n",
        "def test_and_visualize(num_samples=5):\n",
        "    \"\"\"\n",
        "    테스트 이미지 샘플링하여 판별 및 시각화\n",
        "\n",
        "    Args:\n",
        "        num_samples: 테스트할 이미지 개수\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(f\"🧪 테스트 시작 (샘플: {num_samples}개)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 테스트 이미지 경로\n",
        "    test_dir = Path('/content/bee_project/images/test')\n",
        "    if not test_dir.exists():\n",
        "        print(f\"❌ 테스트 폴더 없음: {test_dir}\")\n",
        "        return\n",
        "\n",
        "    # 이미지 파일 수집\n",
        "    test_images = list(test_dir.glob('*.jpg')) + list(test_dir.glob('*.png'))\n",
        "    if len(test_images) == 0:\n",
        "        print(\"❌ 테스트 이미지가 없습니다!\")\n",
        "        return\n",
        "\n",
        "    # 랜덤 샘플링\n",
        "    samples = random.sample(test_images, min(num_samples, len(test_images)))\n",
        "    print(f\"📁 전체 {len(test_images)}개 중 {len(samples)}개 샘플링\\n\")\n",
        "\n",
        "    # 판별 시스템 초기화\n",
        "    detector = BeeDetector()\n",
        "\n",
        "    # 결과 저장\n",
        "    all_results = []\n",
        "\n",
        "    # 각 이미지 처리\n",
        "    for i, img_path in enumerate(samples):\n",
        "        print(f\"\\n🖼️ [{i+1}/{len(samples)}] {img_path.name}\")\n",
        "\n",
        "        # 처리\n",
        "        image, results = detector.process_image(img_path)\n",
        "\n",
        "        # 결과 출력\n",
        "        if results:\n",
        "            print(f\"  ✅ 최종 {len(results)}마리 탐지 완료!\")\n",
        "        else:\n",
        "            print(f\"  ❌ 탐지된 벌 없음\")\n",
        "\n",
        "        all_results.append({\n",
        "            'path': img_path,\n",
        "            'image': image,\n",
        "            'results': results\n",
        "        })\n",
        "\n",
        "    # 시각화\n",
        "    visualize_results(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def visualize_results(all_results):\n",
        "    \"\"\"결과 시각화\"\"\"\n",
        "\n",
        "    n = len(all_results)\n",
        "    if n == 0:\n",
        "        return\n",
        "\n",
        "    # 그리드 크기 결정\n",
        "    cols = min(3, n)\n",
        "    rows = (n + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*6, rows*5))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    elif rows == 1:\n",
        "        axes = axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, data in enumerate(all_results):\n",
        "        ax = axes[idx] if n > 1 else axes[0]\n",
        "\n",
        "        # 이미지 표시\n",
        "        ax.imshow(data['image'])\n",
        "        ax.set_title(f\"{data['path'].name}\\n탐지: {len(data['results'])}마리\",\n",
        "                    fontsize=10, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # 바운딩 박스 그리기\n",
        "        for result in data['results']:\n",
        "            x1, y1, x2, y2 = result['bbox']\n",
        "\n",
        "            # 색상 변환\n",
        "            color = result['color']\n",
        "            if color.startswith('#'):\n",
        "                color = tuple(int(color[i:i+2], 16)/255 for i in (1, 3, 5))\n",
        "\n",
        "            # 박스\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                    linewidth=2, edgecolor=color,\n",
        "                                    facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # 라벨\n",
        "            label = f\"{result['species']}\\n{result['species_conf']:.1%}\"\n",
        "            ax.text(x1, y1-5, label, color=color, fontsize=8,\n",
        "                   fontweight='bold',\n",
        "                   bbox=dict(boxstyle='round,pad=0.3',\n",
        "                            facecolor='white', alpha=0.8))\n",
        "\n",
        "    # 빈 subplot 숨기기\n",
        "    for idx in range(n, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle('🐝 벌 종류 판별 결과 (군집화 개선)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def quick_test(num=3):\n",
        "    \"\"\"빠른 테스트 실행\"\"\"\n",
        "    return test_and_visualize(num)\n",
        "\n",
        "def analyze_test_results(num_samples=20):\n",
        "    \"\"\"테스트 결과 통계 분석\"\"\"\n",
        "\n",
        "    print(\"📊 테스트 통계 분석\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    detector = BeeDetector()\n",
        "    test_dir = Path('/content/bee_project/images/test')\n",
        "    test_images = list(test_dir.glob('*.jpg'))\n",
        "\n",
        "    if not test_images:\n",
        "        print(\"❌ 테스트 이미지가 없습니다!\")\n",
        "        return\n",
        "\n",
        "    samples = random.sample(test_images, min(num_samples, len(test_images)))\n",
        "\n",
        "    # 통계 변수\n",
        "    total_images = len(samples)\n",
        "    total_detected = 0\n",
        "    class_counts = {name: 0 for name in CLASS_NAMES}\n",
        "    confidence_scores = []\n",
        "\n",
        "    print(f\"🔍 {total_images}개 이미지 분석 중...\\n\")\n",
        "\n",
        "    for i, img_path in enumerate(samples):\n",
        "        print(f\"[{i+1}/{total_images}] {img_path.name}\")\n",
        "        _, results = detector.process_image(img_path)\n",
        "        total_detected += len(results)\n",
        "\n",
        "        for r in results:\n",
        "            class_counts[r['species']] += 1\n",
        "            confidence_scores.append(r['species_conf'])\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"\\n📈 결과:\")\n",
        "    print(f\"  테스트 이미지: {total_images}개\")\n",
        "    print(f\"  탐지된 벌: {total_detected}마리\")\n",
        "    print(f\"  평균 탐지: {total_detected/total_images:.1f}마리/이미지\")\n",
        "\n",
        "    if confidence_scores:\n",
        "        print(f\"  평균 신뢰도: {np.mean(confidence_scores):.2%}\")\n",
        "        print(f\"  최소 신뢰도: {min(confidence_scores):.2%}\")\n",
        "        print(f\"  최대 신뢰도: {max(confidence_scores):.2%}\")\n",
        "\n",
        "    print(f\"\\n📋 클래스별 탐지:\")\n",
        "    for name, count in class_counts.items():\n",
        "        if count > 0:\n",
        "            print(f\"  {name}: {count}마리\")\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "# ============================================\n",
        "# 메인\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🐝 벌 종류 판별 테스트 시스템 (군집화 개선)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n사용법:\")\n",
        "    print(\"  quick_test(3)        # 3개 샘플 테스트\")\n",
        "    print(\"  quick_test(10)       # 10개 샘플 테스트\")\n",
        "    print(\"  analyze_test_results(20)  # 20개로 통계 분석\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "hRH6krLrEbY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quick_test(3)"
      ],
      "metadata": {
        "id": "_9Is0NZMEbW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrNRbK_YEbPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRHcwgXNJOZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NaZpXWQctDL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}